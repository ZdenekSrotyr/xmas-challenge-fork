name: keboola-core
version: 1.0.0
description: Keboola platform knowledge for Gemini
metadata:
  generated_at: '2025-12-21T21:03:36.375763'
  source_path: docs/keboola
  generator: gemini_generator.py v1.0
  poc_notice: This is a POC. Not production-ready.
knowledge_base:
- source: 01-core-concepts.md
  content: "# Core Concepts\n\n## Overview\n\nKeboola is a cloud-based data platform\
    \ that enables you to extract, transform, and load data from various sources.\n\
    \n## Key Concepts\n\n### Project\nA project is the top-level container in Keboola.\
    \ All your configurations, data, and orchestrations belong to a project.\n\n###\
    \ Storage\nKeboola Storage is where your data lives. It consists of:\n- **Buckets**:\
    \ Logical containers for tables\n- **Tables**: The actual data\n- **Files**: Temporary\
    \ file storage\n\n### Components\nComponents are the building blocks:\n- **Extractors**:\
    \ Pull data from external sources\n- **Transformations**: Process and modify data\n\
    - **Writers**: Send data to external destinations\n\n## Authentication\n\nUse\
    \ Storage API tokens for authentication:\n\n```python\nimport os\nimport requests\n\
    \nSTORAGE_TOKEN = os.environ[\"KEBOOLA_TOKEN\"]\nSTACK_URL = os.environ.get(\"\
    KEBOOLA_STACK_URL\", \"connection.keboola.com\")\n\nheaders = {\n    \"X-StorageApi-Token\"\
    : STORAGE_TOKEN,\n    \"Content-Type\": \"application/json\"\n}\n\nresponse =\
    \ requests.get(\n    f\"https://{STACK_URL}/v2/storage/tables\",\n    headers=headers\n\
    )\n```\n\n### Token Scopes and Permissions\n\nStorage API tokens have different\
    \ permission scopes that control what operations they can perform:\n\n#### Available\
    \ Scopes\n\n- **Read-only access** (`storage:read`):\n  - List buckets and tables\n\
    \  - Export table data\n  - View table metadata\n  - View configurations (read-only)\n\
    \  - Cannot create, modify, or delete anything\n\n- **Full access** (`storage:write`,\
    \ `storage:read`):\n  - All read operations\n  - Create/delete buckets and tables\n\
    \  - Import/write data\n  - Manage table structure\n  - Create and modify configurations\n\
    \n- **Configuration management** (`configurations:read`, `configurations:write`):\n\
    \  - Read component configurations\n  - Create/modify component configurations\n\
    \  - Manage orchestrations\n\n#### Creating Tokens with Specific Scopes\n\n**Via\
    \ Keboola UI**:\n1. Go to **Users & Settings** → **API Tokens**\n2. Click **New\
    \ Token**\n3. Enter token description\n4. Select permissions:\n   - **Read-only**:\
    \ Check only \"Read\" boxes\n   - **Full access**: Check all permission boxes\n\
    \   - **Custom**: Select specific scopes needed\n5. Set expiration date (optional\
    \ but recommended)\n6. Click **Create**\n\n**Via API** (requires admin token):\n\
    \n```python\n# Create read-only token\nresponse = requests.post(\n    f\"https://{STACK_URL}/v2/storage/tokens\"\
    ,\n    headers={\"X-StorageApi-Token\": admin_token},\n    json={\n        \"\
    description\": \"Read-only token for reporting\",\n        \"expiresIn\": 2592000,\
    \  # 30 days in seconds\n        \"canManageBuckets\": False,\n        \"canReadAllFileUploads\"\
    : True,\n        \"bucketPermissions\": {\n            \"in.c-main\": \"read\"\
    \n        }\n    }\n)\nread_only_token = response.json()[\"token\"]\n\n# Create\
    \ full access token\nresponse = requests.post(\n    f\"https://{STACK_URL}/v2/storage/tokens\"\
    ,\n    headers={\"X-StorageApi-Token\": admin_token},\n    json={\n        \"\
    description\": \"Full access token for ETL pipeline\",\n        \"expiresIn\"\
    : 7776000,  # 90 days\n        \"canManageBuckets\": True,\n        \"canReadAllFileUploads\"\
    : True,\n        \"bucketPermissions\": {}  # Empty = all buckets\n    }\n)\n\
    full_access_token = response.json()[\"token\"]\n```\n\n#### Bucket-Level Permissions\n\
    \nYou can grant granular access to specific buckets:\n\n```python\nresponse =\
    \ requests.post(\n    f\"https://{STACK_URL}/v2/storage/tokens\",\n    headers={\"\
    X-StorageApi-Token\": admin_token},\n    json={\n        \"description\": \"Limited\
    \ access token\",\n        \"canManageBuckets\": False,\n        \"bucketPermissions\"\
    : {\n            \"in.c-main\": \"read\",      # Read-only access\n          \
    \  \"out.c-reports\": \"write\"   # Read and write access\n        }\n    }\n\
    )\n```\n\n**Permission levels**:\n- `\"read\"`: Can list and export tables\n-\
    \ `\"write\"`: Can read + create/modify/delete tables\n\n#### Security Best Practices\n\
    \n**DO**:\n\n- Use **read-only tokens** for dashboards and reporting\n- Use **full\
    \ access tokens** only for ETL/data pipelines\n- Set **expiration dates** on all\
    \ tokens (30-90 days recommended)\n- Create **separate tokens** for each application/service\n\
    - Use **bucket-specific permissions** when possible\n- Store tokens in **environment\
    \ variables**, never in code\n- Rotate tokens regularly (every 90 days minimum)\n\
    - Use **descriptive names** to track token usage\n- Revoke tokens immediately\
    \ when no longer needed\n\n**DON'T**:\n\n- Share tokens between applications\n\
    - Commit tokens to version control\n- Use master/admin tokens in production code\n\
    - Grant full access when read-only is sufficient\n- Create tokens without expiration\
    \ dates\n- Reuse tokens across environments (dev/staging/prod)\n\n#### Checking\
    \ Token Permissions\n\nVerify what your token can do:\n\n```python\nresponse =\
    \ requests.get(\n    f\"https://{STACK_URL}/v2/storage/tokens/verify\",\n    headers={\"\
    X-StorageApi-Token\": token}\n)\ntoken_info = response.json()\n\nprint(f\"Token\
    \ description: {token_info['description']}\")\nprint(f\"Can manage buckets: {token_info['canManageBuckets']}\"\
    )\nprint(f\"Bucket permissions: {token_info['bucketPermissions']}\")\nprint(f\"\
    Expires: {token_info.get('expires', 'Never')}\")\n```\n\n#### Common Permission\
    \ Errors\n\n```python\n# Error: 403 Forbidden - Insufficient permissions\ntry:\n\
    \    response = requests.post(\n        f\"https://{STACK_URL}/v2/storage/buckets\"\
    ,\n        headers={\"X-StorageApi-Token\": read_only_token},\n        json={\"\
    name\": \"new-bucket\", \"stage\": \"in\"}\n    )\n    response.raise_for_status()\n\
    except requests.exceptions.HTTPError as e:\n    if e.response.status_code == 403:\n\
    \        print(\"Error: Token does not have write permissions\")\n        print(\"\
    Solution: Use a token with canManageBuckets=True\")\n    raise\n```\n\n#### Use\
    \ Case Examples\n\n**Read-only dashboard token**:\n```python\n# For Streamlit\
    \ apps, Data Apps, reporting tools\ntoken_config = {\n    \"description\": \"\
    Dashboard read-only access\",\n    \"expiresIn\": 2592000,  # 30 days\n    \"\
    canManageBuckets\": False,\n    \"bucketPermissions\": {\n        \"in.c-analytics\"\
    : \"read\",\n        \"in.c-sales\": \"read\"\n    }\n}\n```\n\n**ETL pipeline\
    \ token**:\n```python\n# For extractors, transformations, data loading\ntoken_config\
    \ = {\n    \"description\": \"ETL pipeline full access\",\n    \"expiresIn\":\
    \ 7776000,  # 90 days\n    \"canManageBuckets\": True,\n    \"bucketPermissions\"\
    : {}  # All buckets\n}\n```\n\n**Component development token**:\n```python\n#\
    \ For local development and testing\ntoken_config = {\n    \"description\": \"\
    Dev environment token\",\n    \"expiresIn\": 2592000,  # 30 days\n    \"canManageBuckets\"\
    : True,\n    \"bucketPermissions\": {\n        \"in.c-dev\": \"write\",\n    \
    \    \"out.c-dev\": \"write\"\n    }\n}\n```\n\n## Regional Stacks\n\nKeboola\
    \ operates multiple regional stacks:\n- **US**: connection.keboola.com\n- **EU**:\
    \ connection.eu-central-1.keboola.com\n- **Azure**: connection.north-europe.azure.keboola.com\n\
    \nAlways use your project's stack URL, not a hardcoded one.\n\n### Workspaces\n\
    \nWorkspaces are temporary database environments (Snowflake, Redshift, or BigQuery)\
    \ created for:\n- **Data Apps**: Direct database access for analytics\n- **Transformations**:\
    \ SQL/Python data processing\n- **Sandboxes**: Ad-hoc data exploration\n\n**Key\
    \ Concepts**:\n\n- **Workspace ID**: Identifies a specific workspace instance\
    \ (e.g., `12345`)\n- **Project ID**: Identifies your Keboola project (e.g., `6789`)\n\
    - **Context**: Determines which API/connection to use\n\n**Workspace vs Storage**:\n\
    \n| Aspect | Workspace | Storage |\n|--------|-----------|--------|\n| **Technology**\
    \ | Snowflake/Redshift/BigQuery | Keboola Storage API |\n| **Access Method** |\
    \ Database connection (SQL) | REST API (HTTP) |\n| **Use Case** | SQL queries,\
    \ Data Apps | Data management, orchestration |\n| **Persistence** | Temporary\
    \ (auto-deleted) | Permanent |\n| **Table Names** | `database.schema.table` |\
    \ `bucket.table` |\n\n**When to Use What**:\n\n```python\n# Use WORKSPACE when:\n\
    # - Running inside Data App (production)\n# - Running transformation\n# - Direct\
    \ SQL queries needed\nif 'KBC_PROJECT_ID' in os.environ:\n    conn = st.connection('snowflake',\
    \ type='snowflake')\n    query = f'SELECT * FROM \"{os.environ[\"KBC_PROJECT_ID\"\
    ]}\".\"in.c-main\".\"customers\"'\n    df = conn.query(query)\n\n# Use STORAGE\
    \ API when:\n# - Running outside Keboola (local development)\n# - Managing tables/buckets\n\
    # - Orchestrating data flows\nelse:\n    import requests\n    response = requests.post(\n\
    \        f\"https://{stack_url}/v2/storage/tables/in.c-main.customers/export-async\"\
    ,\n        headers={\"X-StorageApi-Token\": token}\n    )\n```\n"
  format: markdown
- source: 02-storage-api.md
  content: "# Storage API\n\n## Reading Tables\n\n### List All Tables\n\n```python\n\
    import requests\nimport os\n\nstack_url = os.environ.get(\"KEBOOLA_STACK_URL\"\
    , \"connection.keboola.com\")\ntoken = os.environ[\"KEBOOLA_TOKEN\"]\n\nresponse\
    \ = requests.get(\n    f\"https://{stack_url}/v2/storage/tables\",\n    headers={\"\
    X-StorageApi-Token\": token}\n)\n\ntables = response.json()\nfor table in tables:\n\
    \    print(f\"{table['id']}: {table['rowsCount']} rows\")\n```\n\n### Export Table\
    \ Data\n\n```python\nimport time\n\n# Start async export job (NOTE: POST method\
    \ required)\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n    headers={\"X-StorageApi-Token\": token}\n)\nresponse.raise_for_status()\n\
    \njob_id = response.json()[\"id\"]\n\n# Poll for completion with timeout\ntimeout\
    \ = 300  # 5 minutes\nstart_time = time.time()\n\nwhile time.time() - start_time\
    \ < timeout:\n    job_response = requests.get(\n        f\"https://{stack_url}/v2/storage/jobs/{job_id}\"\
    ,\n        headers={\"X-StorageApi-Token\": token}\n    )\n    job_response.raise_for_status()\n\
    \n    job = job_response.json()\n    \n    if job[\"status\"] == \"success\":\n\
    \        # Download and save data to file\n        file_url = job[\"results\"\
    ][\"file\"][\"url\"]\n        data_response = requests.get(file_url)\n       \
    \ \n        with open(\"table_data.csv\", \"wb\") as f:\n            f.write(data_response.content)\n\
    \        \n        print(f\"Table exported to table_data.csv\")\n        break\n\
    \    \n    elif job[\"status\"] in [\"error\", \"cancelled\", \"terminated\"]:\n\
    \        error_msg = job.get(\"error\", {}).get(\"message\", \"Unknown error\"\
    )\n        raise Exception(f\"Export job failed with status {job['status']}: {error_msg}\"\
    )\n    \n    time.sleep(2)\nelse:\n    raise TimeoutError(f\"Export job {job_id}\
    \ did not complete within {timeout} seconds\")\n\n# Optional: Load data into memory\
    \ if needed\nimport csv\nwith open(\"table_data.csv\", \"r\") as f:\n    reader\
    \ = csv.DictReader(f)\n    data = list(reader)\n```\n\n## Writing Tables\n\n###\
    \ Create Table from CSV\n\n```python\n### Create New Table from CSV\n\n```python\n\
    # Upload CSV file to create a NEW table\ncsv_data = \"id,name,value\\n1,foo,100\\\
    n2,bar,200\"\n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/buckets/in.c-main/tables-async\"\
    ,\n    headers={\n        \"X-StorageApi-Token\": token,\n        \"Content-Type\"\
    : \"text/csv\"\n    },\n    params={\n        \"name\": \"my_table\",\n      \
    \  \"primaryKey\": \"id\",  # Optional: set primary key\n        \"dataString\"\
    : csv_data\n    }\n)\nresponse.raise_for_status()\n\njob_id = response.json()[\"\
    id\"]\n# Poll job until completion (see export example above)\n```\n\n### Import\
    \ Data to Existing Table\n\n```python\n# Import data to an EXISTING table\ntable_id\
    \ = \"in.c-main.my_table\"\ncsv_data = \"id,name,value\\n3,baz,300\\n4,qux,400\"\
    \n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\"\
    ,\n    headers={\n        \"X-StorageApi-Token\": token,\n        \"Content-Type\"\
    : \"text/csv\"\n    },\n    params={\n        \"dataString\": csv_data\n    }\n\
    )\nresponse.raise_for_status()\n\njob_id = response.json()[\"id\"]\n# Poll job\
    \ until completion\n```\n\n### Incremental Loads (Append/Update Data)\n\n**Important**:\
    \ Incremental loads require the table to have a primary key defined.\n\n```python\n\
    # Incremental load: append new rows or update existing rows by primary key\ntable_id\
    \ = \"in.c-main.my_table\"\ncsv_data = \"id,name,value\\n1,foo_updated,150\\n5,new_row,500\"\
    \n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\"\
    ,\n    headers={\n        \"X-StorageApi-Token\": token,\n        \"Content-Type\"\
    : \"text/csv\"\n    },\n    params={\n        \"incremental\": \"1\",  # Enable\
    \ incremental mode\n        \"dataString\": csv_data\n    }\n)\nresponse.raise_for_status()\n\
    \njob_id = response.json()[\"id\"]\n# Poll job until completion\n```\n\n**How\
    \ incremental mode works**:\n- If a row with the same primary key exists, it gets\
    \ UPDATED\n- If a row with a new primary key exists, it gets APPENDED\n- Existing\
    \ rows not in the import data are NOT deleted\n\n### Set or Change Primary Key\n\
    \n```python\n# Set primary key on existing table\ntable_id = \"in.c-main.my_table\"\
    \n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}\"\
    ,\n    headers={\n        \"X-StorageApi-Token\": token,\n        \"Content-Type\"\
    : \"application/json\"\n    },\n    json={\n        \"primaryKey\": [\"id\"] \
    \ # Can be multiple columns: [\"id\", \"date\"]\n    }\n)\nresponse.raise_for_status()\n\
    ```\n```\n\n## Common Patterns\n\n### Pagination\n\nKeboola Storage API supports\
    \ pagination for listing resources and previewing data. Choose the appropriate\
    \ method based on your use case.\n\n#### Overview\n\n| Use Case | Method | Max\
    \ Results | Best For |\n|----------|--------|-------------|----------|\n| List\
    \ tables/buckets | `limit`/`offset` params | Unlimited | Browsing metadata |\n\
    | Quick data preview | `data-preview` endpoint | 1,000 rows | Small samples |\n\
    | Full table export | `export-async` endpoint | Unlimited | Production data export\
    \ |\n\n#### Data Preview Pagination\n\nFor quick data preview with small result\
    \ sets, use limit/offset pagination:\n\n```python\ndef export_table_paginated(table_id,\
    \ chunk_size=10000):\n    \"\"\"Export table preview in chunks using limit/offset.\"\
    \"\"\n    offset = 0\n    all_data = []\n\n    while True:\n        response =\
    \ requests.get(\n            f\"https://{stack_url}/v2/storage/tables/{table_id}/data-preview\"\
    ,\n            headers={\"X-StorageApi-Token\": token},\n            params={\n\
    \                \"limit\": chunk_size,\n                \"offset\": offset\n\
    \            }\n        )\n        response.raise_for_status()\n\n        chunk\
    \ = response.json()\n        if not chunk:\n            break\n\n        all_data.extend(chunk)\n\
    \        offset += chunk_size\n\n    return all_data\n```\n\n**Note**: `data-preview`\
    \ endpoint is limited to 1000 rows maximum. For larger datasets, use async export.\n\
    \n#### API Response Pagination (List Operations)\n\nMany API endpoints that return\
    \ lists support pagination parameters. Use this for browsing tables, buckets,\
    \ configurations, and other metadata.\n\n**Endpoints that support pagination:**\n\
    - `/v2/storage/tables` - List all tables\n- `/v2/storage/buckets` - List all buckets\n\
    - `/v2/storage/files` - List files\n- `/v2/storage/jobs` - List jobs\n- `/v2/storage/events`\
    \ - List events\n\n**Basic pagination example:**\n\n```python\ndef list_all_tables_paginated():\n\
    \    \"\"\"List all tables with pagination support.\"\"\"\n    all_tables = []\n\
    \    offset = 0\n    limit = 100  # Request 100 records per page\n\n    while\
    \ True:\n        response = requests.get(\n            f\"https://{stack_url}/v2/storage/tables\"\
    ,\n            headers={\"X-StorageApi-Token\": token},\n            params={\n\
    \                \"limit\": limit,\n                \"offset\": offset\n     \
    \       }\n        )\n        response.raise_for_status()\n\n        tables =\
    \ response.json()\n        if not tables:\n            break\n\n        all_tables.extend(tables)\n\
    \        \n        # If fewer results than limit, we've reached the end\n    \
    \    if len(tables) < limit:\n            break\n            \n        offset\
    \ += limit\n\n    return all_tables\n```\n\n**Pagination with filtering:**\n\n\
    ```python\ndef list_tables_in_bucket_paginated(bucket_id):\n    \"\"\"List tables\
    \ in specific bucket with pagination.\"\"\"\n    all_tables = []\n    offset =\
    \ 0\n    limit = 50\n\n    while True:\n        response = requests.get(\n   \
    \         f\"https://{stack_url}/v2/storage/buckets/{bucket_id}/tables\",\n  \
    \          headers={\"X-StorageApi-Token\": token},\n            params={\n  \
    \              \"limit\": limit,\n                \"offset\": offset\n       \
    \     }\n        )\n        response.raise_for_status()\n\n        tables = response.json()\n\
    \        if not tables or len(tables) < limit:\n            all_tables.extend(tables)\n\
    \            break\n            \n        all_tables.extend(tables)\n        offset\
    \ += limit\n\n    return all_tables\n```\n\n#### Pagination Parameters\n\nCommon\
    \ pagination parameters across Keboola Storage API:\n\n- **limit**: Number of\
    \ records to return per request\n  - Default: 100 (varies by endpoint)\n  - Maximum:\
    \ 1000 for most endpoints\n  - Recommended: 100-500 for optimal performance\n\
    - **offset**: Number of records to skip from the beginning\n  - Default: 0\n \
    \ - Use case: Skip to specific page (e.g., offset=200 for page 3 with limit=100)\n\
    \n**Example parameters:**\n\n```python\n# Get first page (records 1-100)\nparams\
    \ = {\n    \"limit\": 100,\n    \"offset\": 0\n}\n\n# Get second page (records\
    \ 101-200)\nparams = {\n    \"limit\": 100,\n    \"offset\": 100\n}\n\n# Get third\
    \ page (records 201-300)\nparams = {\n    \"limit\": 100,\n    \"offset\": 200\n\
    }\n```\n\n**Calculate pagination:**\n\n```python\ndef get_page_params(page_number,\
    \ page_size=100):\n    \"\"\"Calculate offset for given page number (1-indexed).\"\
    \"\"\n    return {\n        \"limit\": page_size,\n        \"offset\": (page_number\
    \ - 1) * page_size\n    }\n\n# Usage\npage_1 = get_page_params(1, 100)  # {\"\
    limit\": 100, \"offset\": 0}\npage_2 = get_page_params(2, 100)  # {\"limit\":\
    \ 100, \"offset\": 100}\npage_5 = get_page_params(5, 50)   # {\"limit\": 50, \"\
    offset\": 200}\n```\n\n#### Full Table Export (Recommended for Large Tables)\n\
    \nFor exporting complete tables, especially large ones, use async export instead\
    \ of pagination:\n\n```python\ndef export_large_table(table_id):\n    \"\"\"Export\
    \ large table using async job (handles pagination internally).\"\"\"\n    # Start\
    \ async export\n    response = requests.post(\n        f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n        headers={\"X-StorageApi-Token\": token}\n    )\n    response.raise_for_status()\n\
    \    job_id = response.json()[\"id\"]\n    \n    # Poll for completion\n    import\
    \ time\n    timeout = 600\n    start_time = time.time()\n    \n    while time.time()\
    \ - start_time < timeout:\n        job_response = requests.get(\n            f\"\
    https://{stack_url}/v2/storage/jobs/{job_id}\",\n            headers={\"X-StorageApi-Token\"\
    : token}\n        )\n        job_response.raise_for_status()\n        job = job_response.json()\n\
    \        \n        if job[\"status\"] == \"success\":\n            # Download\
    \ complete file (pagination handled by Keboola)\n            file_url = job[\"\
    results\"][\"file\"][\"url\"]\n            data_response = requests.get(file_url)\n\
    \            \n            with open(\"table_export.csv\", \"wb\") as f:\n   \
    \             f.write(data_response.content)\n            \n            return\
    \ \"table_export.csv\"\n        \n        elif job[\"status\"] in [\"error\",\
    \ \"cancelled\", \"terminated\"]:\n            error_msg = job.get(\"error\",\
    \ {}).get(\"message\", \"Unknown error\")\n            raise Exception(f\"Export\
    \ failed: {error_msg}\")\n        \n        time.sleep(2)\n    \n    raise TimeoutError(\"\
    Export job timeout\")\n```\n\n**When to use each approach**:\n\n- **data-preview\
    \ with pagination**: Quick checks, small datasets (<1000 rows), development/debugging\n\
    - **List endpoints with pagination**: Browsing tables, buckets, configurations,\
    \ metadata operations\n- **Async export**: Production data export, large tables\
    \ (>1000 rows), complete data downloads\n\n**Performance considerations:**\n\n\
    ```python\n# For small previews (< 1000 rows): data-preview is fastest\ndef quick_preview(table_id,\
    \ rows=100):\n    response = requests.get(\n        f\"https://{stack_url}/v2/storage/tables/{table_id}/data-preview\"\
    ,\n        headers={\"X-StorageApi-Token\": token},\n        params={\"limit\"\
    : rows}\n    )\n    return response.json()\n\n# For listing metadata: pagination\
    \ is efficient\ndef list_all_tables():\n    # Uses pagination internally (see\
    \ example above)\n    return list_all_tables_paginated()\n\n# For large datasets:\
    \ async export handles pagination automatically\ndef export_full_table(table_id):\n\
    \    # Keboola handles pagination internally, returns complete file\n    return\
    \ export_large_table_correct(table_id)\n```\n\n### Reading Data Incrementally\n\
    \nUse changedSince parameter to export only recently modified data:\n\n```python\n\
    from datetime import datetime, timedelta\n\n# Get data changed in last 24 hours\n\
    yesterday = (datetime.now() - timedelta(days=1)).isoformat()\n\nresponse = requests.post(\n\
    \    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\",\n   \
    \ headers={\"X-StorageApi-Token\": token},\n    params={\"changedSince\": yesterday}\n\
    )\nresponse.raise_for_status()\n\njob_id = response.json()[\"id\"]\n# Poll job\
    \ until completion\n```\n\n### Writing Data Incrementally\n\nSee the \"Incremental\
    \ Loads (Append/Update Data)\" section under \"Writing Tables\" above for how\
    \ to write data incrementally using `incremental: \"1\"` parameter.\n\n\n### Export\
    \ Table to File (Complete Example)\n\nFor a complete, production-ready example\
    \ that saves data to a file:\n\n```python\nimport requests\nimport os\nimport\
    \ time\n\nstack_url = os.environ.get(\"KEBOOLA_STACK_URL\", \"connection.keboola.com\"\
    )\ntoken = os.environ[\"KEBOOLA_TOKEN\"]\ntable_id = \"in.c-main.customers\"\n\
    output_file = \"customers.csv\"\n\ndef export_table_to_file(table_id, output_file,\
    \ timeout=300):\n    \"\"\"Export Keboola table to local CSV file.\"\"\"\n   \
    \ \n    # Start async export\n    response = requests.post(\n        f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n        headers={\"X-StorageApi-Token\": token}\n    )\n    response.raise_for_status()\n\
    \    job_id = response.json()[\"id\"]\n    \n    print(f\"Export job started:\
    \ {job_id}\")\n    \n    # Poll for completion\n    start_time = time.time()\n\
    \    while time.time() - start_time < timeout:\n        job_response = requests.get(\n\
    \            f\"https://{stack_url}/v2/storage/jobs/{job_id}\",\n            headers={\"\
    X-StorageApi-Token\": token}\n        )\n        job_response.raise_for_status()\n\
    \        job = job_response.json()\n        \n        if job[\"status\"] == \"\
    success\":\n            # Download file\n            file_url = job[\"results\"\
    ][\"file\"][\"url\"]\n            data_response = requests.get(file_url)\n   \
    \         \n            with open(output_file, \"wb\") as f:\n               \
    \ f.write(data_response.content)\n            \n            print(f\"Table exported\
    \ to {output_file}\")\n            return output_file\n        \n        elif\
    \ job[\"status\"] in [\"error\", \"cancelled\", \"terminated\"]:\n           \
    \ error_msg = job.get(\"error\", {}).get(\"message\", \"Unknown error\")\n   \
    \         raise Exception(f\"Job {job['status']}: {error_msg}\")\n        \n \
    \       time.sleep(2)\n    \n    raise TimeoutError(f\"Export did not complete\
    \ within {timeout}s\")\n\n# Usage\nexport_table_to_file(table_id, output_file)\n\
    ```\n\n\n## Storage vs Workspace Context\n\n### Understanding the Difference\n\
    \n**Storage API** operates at the **project level**:\n- Uses project-wide Storage\
    \ API token\n- Manages permanent data storage\n- Uses table IDs like `in.c-main.customers`\n\
    - Accessed via REST API endpoints\n- Used for: data ingestion, component development,\
    \ orchestration\n\n**Workspace** operates at the **workspace level**:\n- Uses\
    \ workspace-specific database credentials\n- Provides temporary SQL access to\
    \ project data\n- Uses qualified names like `\"PROJECT_ID\".\"in.c-main\".\"customers\"\
    `\n- Accessed via native database connections (JDBC/ODBC)\n- Used for: Data Apps,\
    \ transformations, SQL analysis\n\n### When to Use Storage API (Project Context)\n\
    \n✅ **Use Storage API when**:\n- Developing custom components\n- Running scripts\
    \ outside Keboola\n- Managing buckets and tables\n- Orchestrating data pipelines\n\
    - Local development/testing\n\n```python\n# Example: Local development script\n\
    import os\nimport requests\n\ntoken = os.environ['KEBOOLA_TOKEN']\nstack_url =\
    \ os.environ.get('KEBOOLA_STACK_URL', 'connection.keboola.com')\n\n# Project-level\
    \ API call\nresponse = requests.get(\n    f\"https://{stack_url}/v2/storage/tables\"\
    ,\n    headers={\"X-StorageApi-Token\": token}\n)\n\ntables = response.json()\n\
    for table in tables:\n    print(f\"Project table: {table['id']}\")\n```\n\n###\
    \ When to Use Workspace (Workspace Context)\n\n✅ **Use Workspace when**:\n- Building\
    \ Data Apps (production runtime)\n- Writing SQL transformations\n- Running queries\
    \ in Snowflake/Redshift workspace\n- Need direct database performance\n\n```python\n\
    # Example: Data App in production\nimport os\nimport streamlit as st\n\nif 'KBC_PROJECT_ID'\
    \ in os.environ:\n    # Running in workspace - use direct connection\n    conn\
    \ = st.connection('snowflake', type='snowflake')\n    \n    # Workspace-level\
    \ SQL query with qualified names\n    project_id = os.environ['KBC_PROJECT_ID']\n\
    \    query = f'''\n        SELECT * \n        FROM \"{project_id}\".\"in.c-main\"\
    .\"customers\"\n        WHERE \"status\" = 'active'\n    '''\n    df = conn.query(query)\n\
    else:\n    # Local development - use Storage API\n    # (see Storage API examples\
    \ above)\n    pass\n```\n\n### Hybrid Pattern: Support Both Contexts\n\nData Apps\
    \ should support both contexts for local development and production:\n\n```python\n\
    # utils/data_loader.py\nimport os\nimport streamlit as st\nimport requests\n\n\
    def get_connection_mode():\n    \"\"\"Detect runtime environment.\"\"\"\n    return\
    \ 'workspace' if 'KBC_PROJECT_ID' in os.environ else 'storage_api'\n\n@st.cache_resource\n\
    def get_connection():\n    \"\"\"Get appropriate connection for environment.\"\
    \"\"\n    mode = get_connection_mode()\n    \n    if mode == 'workspace':\n  \
    \      # Production: Use workspace connection\n        return st.connection('snowflake',\
    \ type='snowflake')\n    else:\n        # Local: Return Storage API client\n \
    \       return StorageAPIClient(\n            token=os.environ['KEBOOLA_TOKEN'],\n\
    \            stack_url=os.environ.get('KEBOOLA_STACK_URL')\n        )\n\ndef get_table_reference(bucket_id,\
    \ table_name):\n    \"\"\"Get correct table reference for environment.\"\"\"\n\
    \    mode = get_connection_mode()\n    \n    if mode == 'workspace':\n       \
    \ # Workspace: Fully qualified name\n        project_id = os.environ['KBC_PROJECT_ID']\n\
    \        return f'\"{project_id}\".\"{bucket_id}\".\"{table_name}\"'\n    else:\n\
    \        # Storage API: bucket.table format\n        return f\"{bucket_id}.{table_name}\"\
    \n\n# Usage in Data App\n@st.cache_data(ttl=300)\ndef load_customers():\n    conn\
    \ = get_connection()\n    table_ref = get_table_reference('in.c-main', 'customers')\n\
    \    \n    if get_connection_mode() == 'workspace':\n        query = f'SELECT\
    \ * FROM {table_ref}'\n        return conn.query(query)\n    else:\n        #\
    \ Use Storage API export\n        return conn.export_table(table_ref)\n```\n\n\
    ### Common Pitfalls\n\n❌ **Don't mix contexts**:\n```python\n# WRONG: Using Storage\
    \ API table ID in workspace SQL\nquery = f\"SELECT * FROM in.c-main.customers\"\
    \  # Fails in workspace\n\n# CORRECT: Use qualified names in workspace\nquery\
    \ = f'SELECT * FROM \"{project_id}\".\"in.c-main\".\"customers\"'\n```\n\n❌ **Don't\
    \ use workspace credentials in Storage API**:\n```python\n# WRONG: Workspace connection\
    \ for Storage API call\nconn = st.connection('snowflake')  # This is workspace,\
    \ not Storage API\n\n# CORRECT: Use Storage API token\nimport requests\nresponse\
    \ = requests.get(\n    f\"https://{stack_url}/v2/storage/tables\",\n    headers={\"\
    X-StorageApi-Token\": storage_token}\n)\n```\n\n## Batch Operations\n\nBatch operations\
    \ allow you to work with multiple tables efficiently, reducing overhead and improving\
    \ performance for bulk data transfers.\n\n### Prerequisites\n\nBefore using batch\
    \ operations, ensure you have the required imports and variables set up:\n\n```python\n\
    import requests\nimport os\nimport time\nimport csv\nfrom typing import List,\
    \ Dict, Any\n\n# Initialize connection variables\nstack_url = os.environ.get(\"\
    KEBOOLA_STACK_URL\", \"connection.keboola.com\")\ntoken = os.environ[\"KEBOOLA_TOKEN\"\
    ]\n\n# Verify authentication\nresponse = requests.get(\n    f\"https://{stack_url}/v2/storage/tokens/verify\"\
    ,\n    headers={\"X-StorageApi-Token\": token}\n)\nresponse.raise_for_status()\n\
    print(f\"Authenticated as: {response.json()['description']}\")\n```\n\n### Helper\
    \ Functions\n\n#### Job Polling Helper\n\nReusable function for polling async\
    \ jobs with proper error handling:\n\n```python\ndef poll_job_until_complete(job_id:\
    \ str, timeout: int = 600, poll_interval: int = 3) -> Dict[str, Any]:\n    \"\"\
    \"\n    Poll Keboola job until completion.\n    \n    Args:\n        job_id: Keboola\
    \ job ID to monitor\n        timeout: Maximum wait time in seconds (default 10\
    \ minutes)\n        poll_interval: Seconds between status checks (minimum 2 seconds)\n\
    \    \n    Returns:\n        Job result dictionary on success\n    \n    Raises:\n\
    \        TimeoutError: Job didn't complete within timeout\n        Exception:\
    \ Job failed with error\n    \"\"\"\n    if poll_interval < 2:\n        raise\
    \ ValueError(\"poll_interval must be at least 2 seconds to avoid rate limits\"\
    )\n    \n    start_time = time.time()\n    attempts = 0\n    \n    while time.time()\
    \ - start_time < timeout:\n        attempts += 1\n        \n        try:\n   \
    \         response = requests.get(\n                f\"https://{stack_url}/v2/storage/jobs/{job_id}\"\
    ,\n                headers={\"X-StorageApi-Token\": token},\n                timeout=10\n\
    \            )\n            response.raise_for_status()\n        except requests.exceptions.RequestException\
    \ as e:\n            print(f\"Request failed (attempt {attempts}): {e}\")\n  \
    \          time.sleep(min(poll_interval * 2, 10))\n            continue\n    \
    \    \n        job = response.json()\n        status = job[\"status\"]\n     \
    \   \n        if status == \"success\":\n            print(f\"Job {job_id} completed\
    \ successfully\")\n            return job\n        \n        elif status in [\"\
    error\", \"cancelled\", \"terminated\"]:\n            error_msg = job.get(\"error\"\
    , {}).get(\"message\", \"Unknown error\")\n            raise Exception(f\"Job\
    \ {job_id} failed with status '{status}': {error_msg}\")\n        \n        elif\
    \ status in [\"waiting\", \"processing\"]:\n            elapsed = time.time()\
    \ - start_time\n            if attempts % 10 == 0:  # Log every 10th check\n \
    \               print(f\"Job {job_id} still {status} (elapsed: {elapsed:.1f}s)\"\
    )\n            time.sleep(poll_interval)\n        \n        else:\n          \
    \  print(f\"Warning: Unknown job status '{status}' for job {job_id}\")\n     \
    \       time.sleep(poll_interval)\n    \n    elapsed = time.time() - start_time\n\
    \    raise TimeoutError(\n        f\"Job {job_id} did not complete within {timeout}s\
    \ \"\n        f\"(checked {attempts} times over {elapsed:.1f}s)\"\n    )\n```\n\
    \n#### Export Helper with Retry\n\nWrapper for exporting single tables with built-in\
    \ retry logic:\n\n```python\ndef export_table(table_id: str, output_file: str\
    \ = None, **params) -> str:\n    \"\"\"\n    Export single table using async job.\n\
    \    \n    Args:\n        table_id: Table ID (e.g., 'in.c-main.customers')\n \
    \       output_file: Optional output filename (default: table_id.csv)\n      \
    \  **params: Additional export parameters (changedSince, whereColumn, etc.)\n\
    \    \n    Returns:\n        Path to downloaded CSV file\n    \n    Raises:\n\
    \        Exception: Export failed\n    \"\"\"\n    if output_file is None:\n \
    \       output_file = f\"{table_id.replace('.', '_')}.csv\"\n    \n    # Start\
    \ async export\n    response = requests.post(\n        f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n        headers={\"X-StorageApi-Token\": token},\n        params=params\n \
    \   )\n    response.raise_for_status()\n    job_id = response.json()[\"id\"]\n\
    \    \n    print(f\"Exporting {table_id} (job: {job_id})\")\n    \n    # Poll\
    \ for completion using helper\n    job = poll_job_until_complete(job_id, timeout=600,\
    \ poll_interval=3)\n    \n    # Download file\n    file_url = job[\"results\"\
    ][\"file\"][\"url\"]\n    data_response = requests.get(file_url)\n    data_response.raise_for_status()\n\
    \    \n    with open(output_file, \"wb\") as f:\n        f.write(data_response.content)\n\
    \    \n    print(f\"Exported {table_id} to {output_file}\")\n    return output_file\n\
    \n\ndef export_table_with_retry(table_id: str, output_file: str = None, max_retries:\
    \ int = 3, **params) -> str:\n    \"\"\"\n    Export table with automatic retry\
    \ on failure.\n    \n    Args:\n        table_id: Table ID to export\n       \
    \ output_file: Optional output filename\n        max_retries: Number of retry\
    \ attempts (default: 3)\n        **params: Additional export parameters\n    \n\
    \    Returns:\n        Path to downloaded CSV file\n    \n    Raises:\n      \
    \  Exception: Export failed after all retries\n    \"\"\"\n    for attempt in\
    \ range(max_retries):\n        try:\n            return export_table(table_id,\
    \ output_file, **params)\n        except Exception as e:\n            if attempt\
    \ < max_retries - 1:\n                wait_time = 2 ** attempt\n             \
    \   print(f\"Export attempt {attempt + 1} failed: {e}\")\n                print(f\"\
    Retrying in {wait_time}s...\")\n                time.sleep(wait_time)\n      \
    \      else:\n                raise Exception(f\"Export failed after {max_retries}\
    \ attempts: {e}\")\n```\n\n### Batch Export\n\nExport multiple tables in parallel\
    \ or sequence.\n\n#### Sequential Export\n\nExport tables one at a time (safer\
    \ for many tables):\n\n```python\ndef batch_export_tables_sequential(\n    table_ids:\
    \ List[str],\n    output_dir: str = \"exports\",\n    **export_params\n) -> Dict[str,\
    \ str]:\n    \"\"\"\n    Export multiple tables sequentially.\n    \n    Args:\n\
    \        table_ids: List of table IDs to export\n        output_dir: Directory\
    \ to save exported files (created if needed)\n        **export_params: Additional\
    \ parameters passed to each export\n                        (changedSince, whereColumn,\
    \ whereValues, whereOperator)\n    \n    Returns:\n        Dictionary mapping\
    \ table_id to output file path\n    \n    Example:\n        results = batch_export_tables_sequential(\n\
    \            ['in.c-main.customers', 'in.c-main.orders'],\n            output_dir='data',\n\
    \            changedSince='2024-01-01T00:00:00Z'\n        )\n    \"\"\"\n    import\
    \ os\n    os.makedirs(output_dir, exist_ok=True)\n    \n    results = {}\n   \
    \ failed = {}\n    \n    print(f\"Starting batch export of {len(table_ids)} tables...\"\
    )\n    \n    for i, table_id in enumerate(table_ids, 1):\n        print(f\"\\\
    n[{i}/{len(table_ids)}] Processing {table_id}\")\n        \n        output_file\
    \ = os.path.join(\n            output_dir,\n            f\"{table_id.replace('.',\
    \ '_')}.csv\"\n        )\n        \n        try:\n            export_table_with_retry(table_id,\
    \ output_file, **export_params)\n            results[table_id] = output_file\n\
    \        except Exception as e:\n            print(f\"Failed to export {table_id}:\
    \ {e}\")\n            failed[table_id] = str(e)\n    \n    print(f\"\\n=== Batch\
    \ Export Complete ===\")\n    print(f\"Success: {len(results)}/{len(table_ids)}\
    \ tables\")\n    if failed:\n        print(f\"Failed: {len(failed)} tables\")\n\
    \        for table_id, error in failed.items():\n            print(f\"  - {table_id}:\
    \ {error}\")\n    \n    if failed:\n        raise Exception(f\"Batch export completed\
    \ with {len(failed)} failures\")\n    \n    return results\n```\n\n#### Parallel\
    \ Export (Advanced)\n\nExport tables concurrently for faster processing:\n\n```python\n\
    from concurrent.futures import ThreadPoolExecutor, as_completed\n\ndef batch_export_tables_parallel(\n\
    \    table_ids: List[str],\n    output_dir: str = \"exports\",\n    max_workers:\
    \ int = 5,\n    **export_params\n) -> Dict[str, str]:\n    \"\"\"\n    Export\
    \ multiple tables in parallel using thread pool.\n    \n    Args:\n        table_ids:\
    \ List of table IDs to export\n        output_dir: Directory to save exported\
    \ files\n        max_workers: Maximum concurrent exports (default: 5, max recommended:\
    \ 10)\n        **export_params: Additional export parameters\n    \n    Returns:\n\
    \        Dictionary mapping table_id to output file path\n    \n    Warning:\n\
    \        - Higher concurrency may trigger rate limits\n        - Recommended max_workers:\
    \ 5-10\n        - Monitor for 429 (Too Many Requests) errors\n    \n    Example:\n\
    \        results = batch_export_tables_parallel(\n            table_ids=['in.c-main.table1',\
    \ 'in.c-main.table2'],\n            max_workers=5\n        )\n    \"\"\"\n   \
    \ import os\n    os.makedirs(output_dir, exist_ok=True)\n    \n    results = {}\n\
    \    failed = {}\n    \n    print(f\"Starting parallel export of {len(table_ids)}\
    \ tables (workers: {max_workers})...\")\n    \n    def export_one_table(table_id):\n\
    \        \"\"\"Export single table (thread worker function).\"\"\"\n        output_file\
    \ = os.path.join(\n            output_dir,\n            f\"{table_id.replace('.',\
    \ '_')}.csv\"\n        )\n        export_table_with_retry(table_id, output_file,\
    \ **export_params)\n        return table_id, output_file\n    \n    with ThreadPoolExecutor(max_workers=max_workers)\
    \ as executor:\n        futures = {executor.submit(export_one_table, tid): tid\
    \ for tid in table_ids}\n        \n        for future in as_completed(futures):\n\
    \            table_id = futures[future]\n            try:\n                tid,\
    \ output_file = future.result()\n                results[tid] = output_file\n\
    \                print(f\"✓ Completed: {tid}\")\n            except Exception\
    \ as e:\n                failed[table_id] = str(e)\n                print(f\"\
    ✗ Failed: {table_id} - {e}\")\n    \n    print(f\"\\n=== Batch Export Complete\
    \ ===\")\n    print(f\"Success: {len(results)}/{len(table_ids)} tables\")\n  \
    \  if failed:\n        print(f\"Failed: {len(failed)} tables\")\n    \n    if\
    \ failed:\n        raise Exception(f\"Batch export completed with {len(failed)}\
    \ failures\")\n    \n    return results\n```\n\n#### Filtered Export\n\nExport\
    \ multiple tables with SQL filtering:\n\n```python\ndef batch_export_with_filter(\n\
    \    table_configs: List[Dict[str, Any]],\n    output_dir: str = \"exports\"\n\
    ) -> Dict[str, str]:\n    \"\"\"\n    Export tables with individual filter configurations.\n\
    \    \n    Args:\n        table_configs: List of table configurations with filters\n\
    \        output_dir: Output directory\n    \n    Table config structure:\n   \
    \     {\n            'table_id': 'in.c-main.customers',\n            'whereColumn':\
    \ 'status',        # Column to filter on\n            'whereValues': ['active'],\
    \      # Values to match\n            'whereOperator': 'eq',          # Operator:\
    \ eq, ne, gt, lt, ge, le\n            'changedSince': '2024-01-01'   # Optional:\
    \ incremental export\n        }\n    \n    Available whereOperator values:\n \
    \       - 'eq': Equal (column = value)\n        - 'ne': Not equal (column != value)\n\
    \        - 'gt': Greater than (column > value)\n        - 'lt': Less than (column\
    \ < value)\n        - 'ge': Greater than or equal (column >= value)\n        -\
    \ 'le': Less than or equal (column <= value)\n    \n    Example:\n        configs\
    \ = [\n            {\n                'table_id': 'in.c-main.customers',\n   \
    \             'whereColumn': 'status',\n                'whereValues': ['active',\
    \ 'pending'],\n                'whereOperator': 'eq'\n            },\n       \
    \     {\n                'table_id': 'in.c-main.orders',\n                'whereColumn':\
    \ 'order_date',\n                'whereValues': ['2024-01-01'],\n            \
    \    'whereOperator': 'ge',\n                'changedSince': '2024-01-01T00:00:00Z'\n\
    \            }\n        ]\n        results = batch_export_with_filter(configs)\n\
    \    \"\"\"\n    import os\n    os.makedirs(output_dir, exist_ok=True)\n    \n\
    \    results = {}\n    \n    for config in table_configs:\n        table_id =\
    \ config['table_id']\n        print(f\"\\nExporting {table_id} with filters...\"\
    )\n        \n        # Build export parameters\n        export_params = {}\n \
    \       if 'whereColumn' in config:\n            export_params['whereColumn']\
    \ = config['whereColumn']\n        if 'whereValues' in config:\n            export_params['whereValues']\
    \ = config['whereValues']\n        if 'whereOperator' in config:\n           \
    \ export_params['whereOperator'] = config['whereOperator']\n        if 'changedSince'\
    \ in config:\n            export_params['changedSince'] = config['changedSince']\n\
    \        \n        output_file = os.path.join(\n            output_dir,\n    \
    \        f\"{table_id.replace('.', '_')}_filtered.csv\"\n        )\n        \n\
    \        results[table_id] = export_table_with_retry(\n            table_id,\n\
    \            output_file,\n            **export_params\n        )\n    \n    return\
    \ results\n```\n\n### Batch Import\n\nImport multiple CSV files into Keboola tables.\n\
    \n#### Sequential Import\n\n```python\ndef batch_import_tables(\n    import_configs:\
    \ List[Dict[str, Any]],\n    incremental: bool = False\n) -> Dict[str, str]:\n\
    \    \"\"\"\n    Import multiple CSV files to Keboola tables.\n    \n    Args:\n\
    \        import_configs: List of import configurations\n        incremental: If\
    \ True, use incremental mode for all imports\n    \n    Import config structure:\n\
    \        {\n            'table_id': 'in.c-main.customers',  # Target table\n \
    \           'file_path': 'data/customers.csv',  # Source CSV file\n          \
    \  'incremental': True,                 # Optional: override global setting\n\
    \            'primary_key': ['id']               # Required for incremental\n\
    \        }\n    \n    Returns:\n        Dictionary mapping table_id to job_id\n\
    \    \n    Example:\n        configs = [\n            {\n                'table_id':\
    \ 'in.c-main.customers',\n                'file_path': 'exports/customers.csv',\n\
    \                'incremental': True,\n                'primary_key': ['id']\n\
    \            },\n            {\n                'table_id': 'in.c-main.orders',\n\
    \                'file_path': 'exports/orders.csv'\n            }\n        ]\n\
    \        results = batch_import_tables(configs)\n    \"\"\"\n    results = {}\n\
    \    failed = {}\n    \n    print(f\"Starting batch import of {len(import_configs)}\
    \ tables...\")\n    \n    for i, config in enumerate(import_configs, 1):\n   \
    \     table_id = config['table_id']\n        file_path = config['file_path']\n\
    \        is_incremental = config.get('incremental', incremental)\n        \n \
    \       print(f\"\\n[{i}/{len(import_configs)}] Importing {table_id}\")\n    \
    \    \n        try:\n            # Verify file exists\n            if not os.path.exists(file_path):\n\
    \                raise FileNotFoundError(f\"File not found: {file_path}\")\n \
    \           \n            # Read CSV data\n            with open(file_path, 'rb')\
    \ as f:\n                csv_data = f.read()\n            \n            # Set\
    \ primary key if incremental and specified\n            if is_incremental and\
    \ 'primary_key' in config:\n                print(f\"Setting primary key: {config['primary_key']}\"\
    )\n                pk_response = requests.post(\n                    f\"https://{stack_url}/v2/storage/tables/{table_id}\"\
    ,\n                    headers={\"X-StorageApi-Token\": token},\n            \
    \        json={\"primaryKey\": config['primary_key']}\n                )\n   \
    \             pk_response.raise_for_status()\n            \n            # Start\
    \ import job\n            params = {}\n            if is_incremental:\n      \
    \          params['incremental'] = '1'\n            \n            response = requests.post(\n\
    \                f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\"\
    ,\n                headers={\n                    \"X-StorageApi-Token\": token,\n\
    \                    \"Content-Type\": \"text/csv\"\n                },\n    \
    \            params=params,\n                data=csv_data\n            )\n  \
    \          response.raise_for_status()\n            job_id = response.json()[\"\
    id\"]\n            \n            # Poll for completion using helper\n        \
    \    poll_job_until_complete(job_id, timeout=600, poll_interval=3)\n         \
    \   \n            results[table_id] = job_id\n            print(f\"✓ Imported\
    \ {table_id}\")\n            \n        except Exception as e:\n            print(f\"\
    ✗ Failed to import {table_id}: {e}\")\n            failed[table_id] = str(e)\n\
    \    \n    print(f\"\\n=== Batch Import Complete ===\")\n    print(f\"Success:\
    \ {len(results)}/{len(import_configs)} tables\")\n    if failed:\n        print(f\"\
    Failed: {len(failed)} tables\")\n        for table_id, error in failed.items():\n\
    \            print(f\"  - {table_id}: {error}\")\n    \n    if failed:\n     \
    \   raise Exception(f\"Batch import completed with {len(failed)} failures\")\n\
    \    \n    return results\n```\n\n### Performance Considerations\n\n#### Choosing\
    \ Export Strategy\n\n**Use Sequential Export when**:\n- Exporting many tables\
    \ (>20)\n- Tables are very large (>1GB each)\n- Rate limiting is a concern\n-\
    \ You need predictable resource usage\n\n**Use Parallel Export when**:\n- Exporting\
    \ few tables (<20)\n- Tables are small to medium (<500MB each)\n- Speed is critical\n\
    - You have verified rate limits allow concurrency\n\n#### Optimization Tips\n\n\
    ```python\n# 1. Use filtered exports to reduce data transfer\nexport_params =\
    \ {\n    'whereColumn': 'date',\n    'whereValues': ['2024-01-01'],\n    'whereOperator':\
    \ 'ge'  # Only export rows where date >= 2024-01-01\n}\n\n# 2. Use incremental\
    \ exports for changed data only\nexport_params = {\n    'changedSince': '2024-01-01T00:00:00Z'\
    \  # Only export modified rows\n}\n\n# 3. Adjust parallel workers based on table\
    \ sizes\nif avg_table_size_mb < 100:\n    max_workers = 10  # Small tables: more\
    \ concurrency\nelif avg_table_size_mb < 500:\n    max_workers = 5   # Medium tables:\
    \ moderate concurrency\nelse:\n    max_workers = 2   # Large tables: minimal concurrency\n\
    \n# 4. Monitor and handle rate limits\ntry:\n    results = batch_export_tables_parallel(table_ids,\
    \ max_workers=5)\nexcept Exception as e:\n    if '429' in str(e):  # Rate limited\n\
    \        print(\"Rate limit hit, retrying with sequential export...\")\n     \
    \   results = batch_export_tables_sequential(table_ids)\n    else:\n        raise\n\
    ```\n\n#### Memory Management\n\n```python\n# For very large imports, stream data\
    \ instead of loading into memory\ndef import_large_file(table_id: str, file_path:\
    \ str):\n    \"\"\"\n    Import large file using streaming upload.\n    \"\"\"\
    \n    # Upload file to Keboola Files first\n    with open(file_path, 'rb') as\
    \ f:\n        upload_response = requests.post(\n            f\"https://{stack_url}/v2/storage/files\"\
    ,\n            headers={\"X-StorageApi-Token\": token},\n            files={'file':\
    \ f}\n        )\n    upload_response.raise_for_status()\n    file_id = upload_response.json()['id']\n\
    \    \n    # Import from uploaded file\n    response = requests.post(\n      \
    \  f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\",\n     \
    \   headers={\"X-StorageApi-Token\": token},\n        json={'dataFileId': file_id}\n\
    \    )\n    response.raise_for_status()\n    \n    return poll_job_until_complete(response.json()['id'])\n\
    ```\n\n#### Complete Example\n\n```python\n# Initialize variables (see Prerequisites\
    \ section)\nimport os\nimport requests\nfrom typing import List, Dict, Any\n\n\
    stack_url = os.environ.get(\"KEBOOLA_STACK_URL\", \"connection.keboola.com\")\n\
    token = os.environ[\"KEBOOLA_TOKEN\"]\n\n# Define tables to process\ntable_ids\
    \ = [\n    'in.c-main.customers',\n    'in.c-main.orders',\n    'in.c-main.products'\n\
    ]\n\n# Step 1: Export all tables with filter\nprint(\"Step 1: Exporting tables...\"\
    )\nexport_results = batch_export_tables_sequential(\n    table_ids,\n    output_dir='exports',\n\
    \    changedSince='2024-01-01T00:00:00Z'  # Only export recent data\n)\n\n# Step\
    \ 2: Process exported files (example: data transformation)\nprint(\"\\nStep 2:\
    \ Processing data...\")\nimport pandas as pd\n\nfor table_id, file_path in export_results.items():\n\
    \    df = pd.read_csv(file_path)\n    # Perform transformations\n    df['processed_date']\
    \ = pd.Timestamp.now()\n    # Save processed file\n    df.to_csv(file_path, index=False)\n\
    \    print(f\"Processed {table_id}: {len(df)} rows\")\n\n# Step 3: Import processed\
    \ files back\nprint(\"\\nStep 3: Importing processed data...\")\nimport_configs\
    \ = [\n    {\n        'table_id': table_id,\n        'file_path': file_path,\n\
    \        'incremental': True,\n        'primary_key': ['id']\n    }\n    for table_id,\
    \ file_path in export_results.items()\n]\n\nimport_results = batch_import_tables(import_configs)\n\
    \nprint(\"\\n=== Pipeline Complete ===\")\nprint(f\"Exported: {len(export_results)}\
    \ tables\")\nprint(f\"Imported: {len(import_results)} tables\")\n```\n"
  format: markdown
- source: 03-common-pitfalls.md
  content: "# Common Pitfalls\n\n## 1. Hardcoding Stack URLs\n\n**Problem**: Using\
    \ `connection.keboola.com` for all projects\n\n**Solution**: Always use environment\
    \ variables:\n\n```python\n# ❌ WRONG\nstack_url = \"connection.keboola.com\"\n\
    \n# ✅ CORRECT\nstack_url = os.environ.get(\"KEBOOLA_STACK_URL\", \"connection.keboola.com\"\
    )\n```\n\n## 2. Not Handling Job Polling\n\n**Problem**: Polling async jobs incorrectly\
    \ - too frequently, without proper timeout/error handling, or without cleanup\n\
    \n**Common Mistakes**:\n- Polling too frequently (every 100ms) → Gets account\
    \ rate-limited/banned\n- Not handling all error states → Jobs fail silently\n\
    - Missing timeout logic → Scripts hang forever\n- Not cleaning up failed jobs\
    \ → Resource leaks\n\n**Solution**: Poll with appropriate interval, handle all\
    \ states, and clean up:\n\n```python\nimport time\nimport requests\n\ndef wait_for_job(job_id,\
    \ timeout=300, poll_interval=2):\n    \"\"\"Wait for job completion with proper\
    \ polling and error handling.\n    \n    Args:\n        job_id: Keboola job ID\n\
    \        timeout: Maximum wait time in seconds (default 5 minutes)\n        poll_interval:\
    \ Seconds between status checks (minimum 2 seconds)\n    \n    Returns:\n    \
    \    Job result dict on success\n    \n    Raises:\n        TimeoutError: Job\
    \ didn't complete within timeout\n        Exception: Job failed with error\n \
    \   \"\"\"\n    # Enforce minimum poll interval to avoid rate limiting\n    if\
    \ poll_interval < 2:\n        raise ValueError(\"poll_interval must be at least\
    \ 2 seconds to avoid rate limits\")\n    \n    start_time = time.time()\n    attempts\
    \ = 0\n    \n    while time.time() - start_time < timeout:\n        attempts +=\
    \ 1\n        \n        try:\n            response = requests.get(\n          \
    \      f\"https://{stack_url}/v2/storage/jobs/{job_id}\",\n                headers={\"\
    X-StorageApi-Token\": token},\n                timeout=10  # Request timeout\n\
    \            )\n            response.raise_for_status()\n        except requests.exceptions.RequestException\
    \ as e:\n            # Network error - retry with backoff\n            print(f\"\
    Request failed (attempt {attempts}): {e}\")\n            time.sleep(min(poll_interval\
    \ * 2, 10))\n            continue\n        \n        job = response.json()\n \
    \       status = job[\"status\"]\n        \n        # SUCCESS: Job completed successfully\n\
    \        if status == \"success\":\n            print(f\"Job {job_id} completed\
    \ successfully after {attempts} checks\")\n            return job\n        \n\
    \        # ERROR STATES: Job failed, was cancelled, or terminated\n        elif\
    \ status in [\"error\", \"cancelled\", \"terminated\"]:\n            error_msg\
    \ = job.get(\"error\", {}).get(\"message\", \"Unknown error\")\n            error_type\
    \ = job.get(\"error\", {}).get(\"type\", \"unknown\")\n            \n        \
    \    # Log full error details\n            print(f\"Job {job_id} failed:\")\n\
    \            print(f\"  Status: {status}\")\n            print(f\"  Error Type:\
    \ {error_type}\")\n            print(f\"  Message: {error_msg}\")\n          \
    \  \n            raise Exception(\n                f\"Job {job_id} failed with\
    \ status '{status}': {error_msg}\"\n            )\n        \n        # PROCESSING:\
    \ Job still running (waiting, processing)\n        elif status in [\"waiting\"\
    , \"processing\"]:\n            elapsed = time.time() - start_time\n         \
    \   print(f\"Job {job_id} status: {status} (elapsed: {elapsed:.1f}s)\")\n    \
    \        time.sleep(poll_interval)\n        \n        # UNKNOWN STATE: Unexpected\
    \ status\n        else:\n            print(f\"Warning: Unknown job status '{status}'\
    \ for job {job_id}\")\n            time.sleep(poll_interval)\n    \n    # TIMEOUT:\
    \ Job didn't complete in time\n    elapsed = time.time() - start_time\n    raise\
    \ TimeoutError(\n        f\"Job {job_id} did not complete within {timeout}s \"\
    \n        f\"(checked {attempts} times over {elapsed:.1f}s)\"\n    )\n\n\n# Example\
    \ usage with proper cleanup\ndef export_table_safe(table_id):\n    \"\"\"Export\
    \ table with proper job polling and cleanup.\"\"\"\n    job_id = None\n    \n\
    \    try:\n        # Start async export\n        response = requests.post(\n \
    \           f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n            headers={\"X-StorageApi-Token\": token}\n        )\n        response.raise_for_status()\n\
    \        job_id = response.json()[\"id\"]\n        \n        # Wait with proper\
    \ polling (2-second intervals)\n        job = wait_for_job(job_id, timeout=600,\
    \ poll_interval=2)\n        \n        # Download result\n        file_url = job[\"\
    results\"][\"file\"][\"url\"]\n        data_response = requests.get(file_url)\n\
    \        return data_response.content\n        \n    except TimeoutError as e:\n\
    \        print(f\"Export timed out: {e}\")\n        # Job might still be running\
    \ - log for manual cleanup\n        if job_id:\n            print(f\"Job ID for\
    \ cleanup: {job_id}\")\n        raise\n        \n    except Exception as e:\n\
    \        print(f\"Export failed: {e}\")\n        # Job failed or was cancelled\
    \ - already logged\n        raise\n```\n\n**Real-World Example: Rate Limiting**\n\
    \n```python\n# ❌ WRONG - Polls every 100ms, will get banned\ndef poll_too_fast(job_id):\n\
    \    while True:\n        response = requests.get(\n            f\"https://{stack_url}/v2/storage/jobs/{job_id}\"\
    ,\n            headers={\"X-StorageApi-Token\": token}\n        )\n        if\
    \ response.json()[\"status\"] == \"success\":\n            return response.json()\n\
    \        time.sleep(0.1)  # 100ms = 10 requests/second!\n\n# ✅ CORRECT - Polls\
    \ every 2 seconds (safe rate)\ndef poll_correctly(job_id):\n    return wait_for_job(job_id,\
    \ poll_interval=2)\n```\n\n**Polling Interval Guidelines**:\n\n| Job Type | Typical\
    \ Duration | Recommended Interval |\n|----------|------------------|---------------------|\n\
    | Table export (small) | 5-30 seconds | 2 seconds |\n| Table export (large) |\
    \ 1-10 minutes | 3-5 seconds |\n| Table import | 10-60 seconds | 2-3 seconds |\n\
    | Transformation | 30-300 seconds | 5 seconds |\n\n**Why 2 seconds minimum?**\n\
    - Keboola rate limits: ~30 requests/minute per token\n- Polling every 2 seconds\
    \ = 30 requests/minute (safe)\n- Polling every 100ms = 600 requests/minute (banned)\n\
    \n**Error States to Handle**:\n\n```python\nSUCCESS_STATES = [\"success\"]\nERROR_STATES\
    \ = [\"error\", \"cancelled\", \"terminated\"]\nPROCESSING_STATES = [\"waiting\"\
    , \"processing\"]\n\n# Always check for ALL error states\nif job[\"status\"] in\
    \ ERROR_STATES:\n    # Extract detailed error information\n    error_details =\
    \ job.get(\"error\", {})\n    error_message = error_details.get(\"message\", \"\
    Unknown error\")\n    error_type = error_details.get(\"type\", \"unknown\")\n\
    \    \n    # Log for debugging\n    logging.error(f\"Job {job_id} failed: {error_type}\
    \ - {error_message}\")\n    \n    raise Exception(f\"Job failed: {error_message}\"\
    )\n```\n```\n\n## 3. Ignoring Rate Limits\n\n**Problem**: Making too many API\
    \ calls too quickly\n\n**Solution**: Implement exponential backoff:\n\n```python\n\
    import time\nfrom requests.exceptions import HTTPError\n\ndef api_call_with_retry(url,\
    \ headers, max_retries=3):\n    \"\"\"Make API call with exponential backoff.\"\
    \"\"\n    for attempt in range(max_retries):\n        try:\n            response\
    \ = requests.get(url, headers=headers)\n            response.raise_for_status()\n\
    \            return response.json()\n\n        except HTTPError as e:\n      \
    \      if e.response.status_code == 429:  # Rate limited\n                wait_time\
    \ = 2 ** attempt\n                print(f\"Rate limited. Waiting {wait_time}s...\"\
    )\n                time.sleep(wait_time)\n            else:\n                raise\n\
    \n    raise Exception(\"Max retries exceeded\")\n```\n\n## 4. Not Validating Table\
    \ IDs\n\n**Problem**: Using invalid table ID format\n\n**Solution**: Validate\
    \ format before API calls:\n\n```python\nimport re\n\ndef validate_table_id(table_id):\n\
    \    \"\"\"Validate Keboola table ID format.\"\"\"\n    pattern = r'^(in|out)\\\
    .c-[a-z0-9-]+\\.[a-z0-9_-]+$'\n\n    if not re.match(pattern, table_id):\n   \
    \     raise ValueError(\n            f\"Invalid table ID: {table_id}. \"\n   \
    \         f\"Expected format: stage.c-bucket.table\"\n        )\n\n    return\
    \ True\n\n# Usage\nvalidate_table_id(\"in.c-main.customers\")  # ✓\nvalidate_table_id(\"\
    my_table\")  # ✗ Raises ValueError\n```\n\n## 5. Missing Error Handling\n\n**Problem**:\
    \ Not handling API errors gracefully\n\n**Solution**: Always check response status:\n\
    \n```python\ndef safe_api_call(url, headers):\n    \"\"\"Make API call with proper\
    \ error handling.\"\"\"\n    try:\n        response = requests.get(url, headers=headers,\
    \ timeout=30)\n        response.raise_for_status()\n\n        return response.json()\n\
    \n    except requests.exceptions.Timeout:\n        print(\"Request timed out\"\
    )\n        return None\n\n    except requests.exceptions.HTTPError as e:\n   \
    \     if e.response.status_code == 401:\n            print(\"Invalid token\")\n\
    \        elif e.response.status_code == 404:\n            print(\"Resource not\
    \ found\")\n        else:\n            print(f\"HTTP error: {e}\")\n        return\
    \ None\n\n    except Exception as e:\n        print(f\"Unexpected error: {e}\"\
    )\n        return None\n```\n\n\n## 3. Wrong HTTP Method for Async Endpoints\n\
    \n**Problem**: Using GET instead of POST for async export operations\n\n**Solution**:\
    \ Always use POST for /export-async endpoints:\n\n```python\n# ❌ WRONG - This\
    \ will return 405 Method Not Allowed\nresponse = requests.get(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n    headers={\"X-StorageApi-Token\": token}\n)\n\n# ✅ CORRECT - Use POST to\
    \ initiate async jobs\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n    headers={\"X-StorageApi-Token\": token}\n)\n```\n\n**Why**: The `/export-async`\
    \ endpoint creates a new export job, which is a write operation requiring POST.\
    \ The API will reject GET requests.\n\n\n\n## 6. Incremental Loads Without Primary\
    \ Key\n\n**Problem**: Attempting incremental load on table without primary key\n\
    \n**Solution**: Always set primary key before using incremental mode:\n\n```python\n\
    # ❌ WRONG - This will fail if table has no primary key\nresponse = requests.post(\n\
    \    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\",\n   \
    \ headers={\"X-StorageApi-Token\": token},\n    params={\n        \"incremental\"\
    : \"1\",\n        \"dataString\": csv_data\n    }\n)\n\n# ✅ CORRECT - Set primary\
    \ key first\n# Option 1: Set when creating table\nresponse = requests.post(\n\
    \    f\"https://{stack_url}/v2/storage/buckets/in.c-main/tables-async\",\n   \
    \ headers={\"X-StorageApi-Token\": token},\n    params={\n        \"name\": \"\
    my_table\",\n        \"primaryKey\": \"id\",  # Set primary key at creation\n\
    \        \"dataString\": csv_data\n    }\n)\n\n# Option 2: Set on existing table\n\
    response = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}\"\
    ,\n    headers={\"X-StorageApi-Token\": token},\n    json={\"primaryKey\": [\"\
    id\"]}\n)\nresponse.raise_for_status()\n\n# Now incremental loads will work\n\
    response = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\"\
    ,\n    headers={\"X-StorageApi-Token\": token},\n    params={\n        \"incremental\"\
    : \"1\",\n        \"dataString\": new_data\n    }\n)\n```\n\n**Why**: Incremental\
    \ mode needs primary key to identify which rows to update vs insert.\n\n## 7.\
    \ Using Wrong Endpoint for Table Import\n\n**Problem**: Confusing table creation\
    \ endpoint with table import endpoint\n\n**Solution**: Use correct endpoint based\
    \ on operation:\n\n```python\n# ❌ WRONG - Using creation endpoint for existing\
    \ table\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/buckets/in.c-main/tables-async\"\
    ,\n    headers={\"X-StorageApi-Token\": token},\n    params={\n        \"name\"\
    : \"existing_table\",  # This creates NEW table or fails\n        \"dataString\"\
    : csv_data\n    }\n)\n\n# ✅ CORRECT - Use import endpoint for existing table\n\
    response = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\"\
    ,\n    headers={\"X-StorageApi-Token\": token},\n    params={\"dataString\": csv_data}\n\
    )\n\n# ✅ CORRECT - Use creation endpoint only for NEW tables\nresponse = requests.post(\n\
    \    f\"https://{stack_url}/v2/storage/buckets/in.c-main/tables-async\",\n   \
    \ headers={\"X-StorageApi-Token\": token},\n    params={\n        \"name\": \"\
    new_table\",\n        \"dataString\": csv_data\n    }\n)\n```\n\n**Rule of thumb**:\n\
    - Creating new table: `/buckets/{bucket}/tables-async`\n- Importing to existing\
    \ table: `/tables/{table_id}/import-async`\n\n## 8. Confusing Workspace Context\
    \ with Project Context\n\n**Problem**: Using workspace IDs in Storage API calls\
    \ or Storage API table names in workspace SQL\n\n**Solution**: Understand the\
    \ context boundary:\n\n```python\n# ❌ WRONG - Using workspace-style table reference\
    \ in Storage API\nimport requests\nproject_id = os.environ['KBC_PROJECT_ID']\n\
    table_ref = f'\"{project_id}\".\"in.c-main\".\"customers\"'  # Snowflake format\n\
    \nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_ref}/export-async\"\
    ,  # FAILS\n    headers={\"X-StorageApi-Token\": token}\n)\n\n# ✅ CORRECT - Storage\
    \ API uses bucket.table format (no project ID)\ntable_id = \"in.c-main.customers\"\
    \  # Storage API format\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n    headers={\"X-StorageApi-Token\": token}\n)\n\n# ❌ WRONG - Using Storage\
    \ API format in workspace SQL\nimport streamlit as st\nconn = st.connection('snowflake',\
    \ type='snowflake')\ntable_id = \"in.c-main.customers\"  # Storage API format\n\
    \nquery = f\"SELECT * FROM {table_id}\"  # FAILS - not valid SQL\ndf = conn.query(query)\n\
    \n# ✅ CORRECT - Workspace SQL requires fully qualified names\nproject_id = os.environ['KBC_PROJECT_ID']\n\
    table_ref = f'\"{project_id}\".\"in.c-main\".\"customers\"'  # Snowflake format\n\
    \nquery = f\"SELECT * FROM {table_ref}\"\ndf = conn.query(query)\n```\n\n**Rule\
    \ of thumb**:\n- **Storage API** (REST endpoints): Use `bucket.table` format,\
    \ no project ID\n- **Workspace** (SQL queries): Use `\"PROJECT_ID\".\"bucket\"\
    .\"table\"` format\n\n**Why the difference?**\n\n- Storage API operates at **project\
    \ level** - it knows your project from the token\n- Workspace operates at **database\
    \ level** - PROJECT_ID is the database name in Snowflake\n\n### Context Detection\
    \ Pattern\n\n```python\ndef get_table_reference(bucket, table):\n    \"\"\"Get\
    \ correct table reference for current context.\"\"\"\n    if 'KBC_PROJECT_ID'\
    \ in os.environ:\n        # Workspace context - return Snowflake-qualified name\n\
    \        project_id = os.environ['KBC_PROJECT_ID']\n        return f'\"{project_id}\"\
    .\"{bucket}\".\"{table}\"'\n    else:\n        # Storage API context - return\
    \ API format\n        return f\"{bucket}.{table}\"\n\n# Usage\ntable_ref = get_table_reference('in.c-main',\
    \ 'customers')\n\nif 'KBC_PROJECT_ID' in os.environ:\n    # Workspace: Use in\
    \ SQL\n    query = f\"SELECT * FROM {table_ref}\"\n    df = conn.query(query)\n\
    else:\n    # Storage API: Use in endpoint\n    response = requests.post(\n   \
    \     f\"https://{stack_url}/v2/storage/tables/{table_ref}/export-async\",\n \
    \       headers={\"X-StorageApi-Token\": token}\n    )\n```\n\n**Common Error\
    \ Messages**:\n\n- `Table 'in.c-main.customers' does not exist` (in workspace)\
    \ → Use quoted, qualified name\n- `Invalid table ID` (in Storage API) → Remove\
    \ quotes and project ID\n- `SQL compilation error` (in workspace) → Missing quotes\
    \ or project ID\n\n\n## 9. Incorrect Pagination Usage\n\n**Problem**: Using data-preview\
    \ pagination for large table exports or not handling pagination in list endpoints\n\
    \n**Solution**: Choose the right pagination strategy for your use case:\n\n```python\n\
    # ❌ WRONG - Using data-preview for large tables (limited to 1000 rows)\ndef export_large_table_wrong(table_id):\n\
    \    offset = 0\n    limit = 1000\n    all_data = []\n    \n    while True:\n\
    \        response = requests.get(\n            f\"https://{stack_url}/v2/storage/tables/{table_id}/data-preview\"\
    ,\n            headers={\"X-StorageApi-Token\": token},\n            params={\"\
    limit\": limit, \"offset\": offset}\n        )\n        data = response.json()\n\
    \        if not data:\n            break\n        all_data.extend(data)\n    \
    \    offset += limit\n    \n    return all_data  # Will never get more than 1000\
    \ rows!\n\n# ✅ CORRECT - Use async export for complete table data\ndef export_large_table_correct(table_id):\n\
    \    response = requests.post(\n        f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n        headers={\"X-StorageApi-Token\": token}\n    )\n    response.raise_for_status()\n\
    \    job_id = response.json()[\"id\"]\n    \n    # Poll and download (see full\
    \ example in Storage API docs)\n    # Returns complete dataset regardless of size\n\
    \    return wait_for_export_job(job_id)\n\n# ❌ WRONG - Not handling pagination\
    \ in list endpoints\ndef get_all_tables_wrong():\n    response = requests.get(\n\
    \        f\"https://{stack_url}/v2/storage/tables\",\n        headers={\"X-StorageApi-Token\"\
    : token}\n    )\n    return response.json()  # Only returns first page!\n\n# ✅\
    \ CORRECT - Paginate through all results\ndef get_all_tables_correct():\n    all_tables\
    \ = []\n    offset = 0\n    limit = 100\n    \n    while True:\n        response\
    \ = requests.get(\n            f\"https://{stack_url}/v2/storage/tables\",\n \
    \           headers={\"X-StorageApi-Token\": token},\n            params={\"limit\"\
    : limit, \"offset\": offset}\n        )\n        response.raise_for_status()\n\
    \        tables = response.json()\n        \n        if not tables:\n        \
    \    break\n        \n        all_tables.extend(tables)\n        \n        if\
    \ len(tables) < limit:\n            break\n        \n        offset += limit\n\
    \    \n    return all_tables\n```\n\n**Rule of thumb**:\n- **Small preview (<100\
    \ rows)**: Use `data-preview` without pagination\n- **Browse/list resources**:\
    \ Use pagination with `limit`/`offset`\n- **Full table export**: Use `export-async`\
    \ (no manual pagination needed)\n\n**Why**: Different endpoints have different\
    \ pagination capabilities and limits. Using the wrong approach can result in incomplete\
    \ data or unnecessary complexity.\n\n\n\n## 10. Using Tokens with Insufficient\
    \ Permissions\n\n**Problem**: API calls fail with 403 Forbidden because token\
    \ lacks required permissions\n\n**Solution**: Use appropriate token scope for\
    \ your operation:\n\n```python\n# ❌ WRONG - Using read-only token for write operation\n\
    read_only_token = os.environ['KEBOOLA_READ_TOKEN']\n\nresponse = requests.post(\n\
    \    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\",\n   \
    \ headers={\"X-StorageApi-Token\": read_only_token},\n    params={\"dataString\"\
    : csv_data}\n)\n# Fails with 403 Forbidden\n\n# ✅ CORRECT - Use token with write\
    \ permissions\nwrite_token = os.environ['KEBOOLA_WRITE_TOKEN']\n\nresponse = requests.post(\n\
    \    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\",\n   \
    \ headers={\"X-StorageApi-Token\": write_token},\n    params={\"dataString\":\
    \ csv_data}\n)\nresponse.raise_for_status()\n\n# ✅ BETTER - Check token permissions\
    \ before operation\ndef verify_token_permissions(token, required_permission='write'):\n\
    \    \"\"\"Verify token has required permissions.\"\"\"\n    response = requests.get(\n\
    \        f\"https://{stack_url}/v2/storage/tokens/verify\",\n        headers={\"\
    X-StorageApi-Token\": token}\n    )\n    response.raise_for_status()\n    token_info\
    \ = response.json()\n    \n    if required_permission == 'write' and not token_info.get('canManageBuckets'):\n\
    \        raise PermissionError(\n            f\"Token '{token_info['description']}'\
    \ does not have write permissions. \"\n            \"Create a token with canManageBuckets=True.\"\
    \n        )\n    \n    return token_info\n\n# Usage\ntoken_info = verify_token_permissions(write_token,\
    \ 'write')\nprint(f\"Using token: {token_info['description']}\")\n```\n\n**Rule\
    \ of thumb**:\n- **Reading data**: Use tokens with `storage:read` or bucket-specific\
    \ read permissions\n- **Writing data**: Use tokens with `canManageBuckets=True`\
    \ and write permissions\n- **Production apps**: Use least-privilege tokens (read-only\
    \ when possible)\n- **Development**: Use separate dev tokens with appropriate\
    \ scope\n\n**Common scenarios**:\n\n```python\n# Scenario 1: Dashboard/Data App\
    \ (read-only)\nREAD_TOKEN = os.environ['KEBOOLA_READ_TOKEN']  # read permissions\
    \ only\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n    headers={\"X-StorageApi-Token\": READ_TOKEN}\n)  # ✓ Works - export only\
    \ needs read permission\n\n# Scenario 2: ETL Pipeline (read + write)\nWRITE_TOKEN\
    \ = os.environ['KEBOOLA_WRITE_TOKEN']  # full permissions\nresponse = requests.post(\n\
    \    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\",\n   \
    \ headers={\"X-StorageApi-Token\": WRITE_TOKEN},\n    params={\"dataString\":\
    \ data}\n)  # ✓ Works - import needs write permission\n\n# Scenario 3: Bucket-specific\
    \ access\nLIMITED_TOKEN = os.environ['KEBOOLA_LIMITED_TOKEN']  # only in.c-main\
    \ access\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/in.c-main.customers/export-async\"\
    ,\n    headers={\"X-StorageApi-Token\": LIMITED_TOKEN}\n)  # ✓ Works - has access\
    \ to in.c-main bucket\n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/in.c-sales.orders/export-async\"\
    ,\n    headers={\"X-StorageApi-Token\": LIMITED_TOKEN}\n)  # ✗ Fails - no access\
    \ to in.c-sales bucket\n```\n\n**Why**: Using the principle of least privilege\
    \ improves security. Read-only tokens can't accidentally modify data, and bucket-specific\
    \ tokens limit blast radius if compromised.\n\n## 6. Authentication Errors\n\n\
    ### Invalid Token (401 Unauthorized)\n\n**Problem**: Getting 401 errors when making\
    \ API calls\n\n**Common Causes**:\n\n1. **Token not set or incorrect**:\n```python\n\
    # ❌ WRONG - Token not loaded\nheaders = {\"X-StorageApi-Token\": \"\"}\n\n# ✅\
    \ CORRECT - Load from environment\nimport os\ntoken = os.environ.get(\"KEBOOLA_TOKEN\"\
    )\nif not token:\n    raise ValueError(\"KEBOOLA_TOKEN environment variable not\
    \ set\")\n\nheaders = {\"X-StorageApi-Token\": token}\n```\n\n2. **Token expired**:\n\
    ```python\ndef check_token_expiration(token, stack_url):\n    \"\"\"Check if token\
    \ is valid and when it expires.\"\"\"\n    try:\n        response = requests.get(\n\
    \            f\"https://{stack_url}/v2/storage/tokens/verify\",\n            headers={\"\
    X-StorageApi-Token\": token}\n        )\n        response.raise_for_status()\n\
    \        \n        token_info = response.json()\n        expires = token_info.get('expires')\n\
    \        \n        if expires:\n            from datetime import datetime\n  \
    \          expiry_date = datetime.fromisoformat(expires.replace('Z', '+00:00'))\n\
    \            print(f\"Token expires: {expiry_date}\")\n            \n        \
    \    if datetime.now(expiry_date.tzinfo) > expiry_date:\n                raise\
    \ Exception(\"Token has expired. Create a new token in Keboola UI.\")\n      \
    \  else:\n            print(\"Token does not expire\")\n        \n        return\
    \ token_info\n        \n    except requests.exceptions.HTTPError as e:\n     \
    \   if e.response.status_code == 401:\n            raise Exception(\n        \
    \        \"Invalid token. Check that:\\n\"\n                \"1. Token is copied\
    \ correctly (no extra spaces)\\n\"\n                \"2. Token hasn't been deleted\
    \ in Keboola UI\\n\"\n                \"3. Token hasn't expired\"\n          \
    \  )\n        raise\n\n# Usage\ncheck_token_expiration(token, stack_url)\n```\n\
    \n3. **Wrong stack URL**:\n```python\n# ❌ WRONG - Hardcoded wrong stack\nstack_url\
    \ = \"connection.keboola.com\"  # Your project might be on EU stack\n\n# ✅ CORRECT\
    \ - Use environment variable\nstack_url = os.environ.get(\"KEBOOLA_STACK_URL\"\
    , \"connection.keboola.com\")\n\n# ✅ VERIFY - Test token on correct stack\nresponse\
    \ = requests.get(\n    f\"https://{stack_url}/v2/storage/tokens/verify\",\n  \
    \  headers={\"X-StorageApi-Token\": token}\n)\nif response.status_code == 401:\n\
    \    print(f\"Token invalid for stack: {stack_url}\")\n    print(\"Check your\
    \ project's stack URL in Keboola UI\")\n```\n\n### Permission Denied (403 Forbidden)\n\
    \n**Problem**: Token is valid but operation is not allowed\n\n**Solution**: Verify\
    \ token has required permissions\n\n```python\ndef diagnose_permission_error(token,\
    \ stack_url, operation):\n    \"\"\"Diagnose why operation is forbidden.\"\"\"\
    \n    response = requests.get(\n        f\"https://{stack_url}/v2/storage/tokens/verify\"\
    ,\n        headers={\"X-StorageApi-Token\": token}\n    )\n    response.raise_for_status()\n\
    \    token_info = response.json()\n    \n    print(f\"Token: {token_info['description']}\"\
    )\n    print(f\"Can manage buckets: {token_info.get('canManageBuckets', False)}\"\
    )\n    print(f\"Bucket permissions: {token_info.get('bucketPermissions', {})}\"\
    )\n    \n    if operation == 'write' and not token_info.get('canManageBuckets'):\n\
    \        print(\"\\n❌ ERROR: Token lacks write permissions\")\n        print(\"\
    SOLUTION: Create a new token with 'Manage Buckets' enabled\")\n        return\
    \ False\n    \n    if operation == 'read' and not token_info.get('bucketPermissions'):\n\
    \        print(\"\\n❌ ERROR: Token has no bucket access\")\n        print(\"SOLUTION:\
    \ Grant bucket-specific read permissions\")\n        return False\n    \n    return\
    \ True\n\n# Usage\ntry:\n    response = requests.post(\n        f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\"\
    ,\n        headers={\"X-StorageApi-Token\": token},\n        params={\"dataString\"\
    : csv_data}\n    )\n    response.raise_for_status()\nexcept requests.exceptions.HTTPError\
    \ as e:\n    if e.response.status_code == 403:\n        diagnose_permission_error(token,\
    \ stack_url, 'write')\n    raise\n```\n\n### Token Scope Issues\n\n**Problem**:\
    \ Token has wrong scope for the operation\n\n**Solution**: Use appropriate token\
    \ type\n\n```python\n# ❌ WRONG - Using read-only token for data import\nREAD_TOKEN\
    \ = os.environ['KEBOOLA_READ_TOKEN']\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\"\
    ,\n    headers={\"X-StorageApi-Token\": READ_TOKEN},  # Will fail with 403\n \
    \   params={\"dataString\": data}\n)\n\n# ✅ CORRECT - Use write token for imports\n\
    WRITE_TOKEN = os.environ['KEBOOLA_WRITE_TOKEN']\nresponse = requests.post(\n \
    \   f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\",\n    headers={\"\
    X-StorageApi-Token\": WRITE_TOKEN},\n    params={\"dataString\": data}\n)\n\n\
    # ✅ BEST - Validate token scope before operation\ndef get_appropriate_token(operation):\n\
    \    \"\"\"Get token with appropriate permissions.\"\"\"\n    if operation in\
    \ ['write', 'import', 'create', 'delete']:\n        token = os.environ.get('KEBOOLA_WRITE_TOKEN')\n\
    \        if not token:\n            raise ValueError(\n                \"Write\
    \ operation requires KEBOOLA_WRITE_TOKEN. \"\n                \"Set environment\
    \ variable with a token that has write permissions.\"\n            )\n    else:\n\
    \        token = os.environ.get('KEBOOLA_READ_TOKEN') or os.environ.get('KEBOOLA_TOKEN')\n\
    \        if not token:\n            raise ValueError(\"No Keboola token found\
    \ in environment variables\")\n    \n    return token\n\n# Usage\ntoken = get_appropriate_token('import')\n\
    ```\n\n### Network and Proxy Issues\n\n**Problem**: Authentication fails due to\
    \ network configuration\n\n**Solution**: Configure requests properly for your\
    \ network\n\n```python\nimport requests\nfrom requests.adapters import HTTPAdapter\n\
    from requests.packages.urllib3.util.retry import Retry\n\ndef create_session_with_retry():\n\
    \    \"\"\"Create session with retry logic and proxy support.\"\"\"\n    session\
    \ = requests.Session()\n    \n    # Configure retries\n    retry_strategy = Retry(\n\
    \        total=3,\n        status_forcelist=[429, 500, 502, 503, 504],\n     \
    \   allowed_methods=[\"HEAD\", \"GET\", \"POST\", \"PUT\", \"DELETE\"],\n    \
    \    backoff_factor=1\n    )\n    adapter = HTTPAdapter(max_retries=retry_strategy)\n\
    \    session.mount(\"https://\", adapter)\n    session.mount(\"http://\", adapter)\n\
    \    \n    # Configure proxy if needed\n    proxy = os.environ.get('HTTP_PROXY')\
    \ or os.environ.get('HTTPS_PROXY')\n    if proxy:\n        session.proxies = {\n\
    \            'http': proxy,\n            'https': proxy\n        }\n    \n   \
    \ # Set timeout\n    session.timeout = 30\n    \n    return session\n\n# Usage\n\
    session = create_session_with_retry()\n\ntry:\n    response = session.get(\n \
    \       f\"https://{stack_url}/v2/storage/tokens/verify\",\n        headers={\"\
    X-StorageApi-Token\": token},\n        timeout=30\n    )\n    response.raise_for_status()\n\
    \    print(\"Authentication successful\")\nexcept requests.exceptions.Timeout:\n\
    \    print(\"Request timed out. Check network connectivity.\")\nexcept requests.exceptions.ProxyError:\n\
    \    print(\"Proxy error. Check HTTP_PROXY/HTTPS_PROXY environment variables.\"\
    )\nexcept requests.exceptions.SSLError:\n    print(\"SSL error. Check certificate\
    \ configuration.\")\nexcept requests.exceptions.ConnectionError:\n    print(f\"\
    Cannot connect to {stack_url}. Check internet connection.\")\n```\n\n### Complete\
    \ Authentication Troubleshooting Function\n\n```python\ndef troubleshoot_authentication(token,\
    \ stack_url):\n    \"\"\"Complete authentication diagnostics.\"\"\"\n    print(\"\
    === Keboola Authentication Troubleshooting ===\")\n    \n    # 1. Check token\
    \ is set\n    if not token:\n        print(\"❌ No token provided\")\n        print(\"\
    SOLUTION: Set KEBOOLA_TOKEN environment variable\")\n        return False\n  \
    \  \n    print(f\"✓ Token is set (length: {len(token)})\")\n    \n    # 2. Check\
    \ stack URL format\n    if not stack_url.startswith(('connection.', 'http')):\n\
    \        print(f\"❌ Invalid stack URL format: {stack_url}\")\n        print(\"\
    SOLUTION: Use format like 'connection.keboola.com'\")\n        return False\n\
    \    \n    print(f\"✓ Stack URL format valid: {stack_url}\")\n    \n    # 3. Test\
    \ connectivity\n    try:\n        response = requests.get(\n            f\"https://{stack_url}/v2/storage\"\
    ,\n            timeout=10\n        )\n        print(f\"✓ Can reach stack (status:\
    \ {response.status_code})\")\n    except requests.exceptions.RequestException\
    \ as e:\n        print(f\"❌ Cannot reach stack: {e}\")\n        print(\"SOLUTION:\
    \ Check internet connection and firewall settings\")\n        return False\n \
    \   \n    # 4. Verify token\n    try:\n        response = requests.get(\n    \
    \        f\"https://{stack_url}/v2/storage/tokens/verify\",\n            headers={\"\
    X-StorageApi-Token\": token},\n            timeout=10\n        )\n        response.raise_for_status()\n\
    \        \n        token_info = response.json()\n        print(f\"✓ Token is valid\"\
    )\n        print(f\"  Description: {token_info.get('description', 'N/A')}\")\n\
    \        print(f\"  Can manage buckets: {token_info.get('canManageBuckets', False)}\"\
    )\n        \n        expires = token_info.get('expires')\n        if expires:\n\
    \            print(f\"  Expires: {expires}\")\n        else:\n            print(f\"\
    \  Expires: Never\")\n        \n        return True\n        \n    except requests.exceptions.HTTPError\
    \ as e:\n        if e.response.status_code == 401:\n            print(\"❌ Token\
    \ is invalid (401 Unauthorized)\")\n            print(\"POSSIBLE CAUSES:\")\n\
    \            print(\"  1. Token was deleted in Keboola UI\")\n            print(\"\
    \  2. Token has expired\")\n            print(\"  3. Token is for different stack\"\
    )\n            print(\"  4. Token contains typos or extra whitespace\")\n    \
    \        print(\"SOLUTION: Create a new token in Keboola UI (Users & Settings\
    \ → API Tokens)\")\n        else:\n            print(f\"❌ Unexpected error: {e.response.status_code}\"\
    )\n        return False\n    except Exception as e:\n        print(f\"❌ Error\
    \ verifying token: {e}\")\n        return False\n\n# Usage\nif not troubleshoot_authentication(token,\
    \ stack_url):\n    sys.exit(1)\n```\n\n### Quick Reference: Authentication Error\
    \ Codes\n\n| Error Code | Meaning | Common Cause | Solution |\n|------------|---------|--------------|----------|\n\
    | 401 | Unauthorized | Invalid/expired token | Verify token, create new one if\
    \ expired |\n| 403 | Forbidden | Insufficient permissions | Use token with write/manage\
    \ permissions |\n| 404 | Not Found | Wrong stack URL or table doesn't exist |\
    \ Verify stack URL and table ID |\n| 429 | Too Many Requests | Rate limit exceeded\
    \ | Implement exponential backoff |\n| 500 | Server Error | Keboola platform issue\
    \ | Retry with backoff, check status page |\n\n## Storage vs Workspace Context\n"
  format: markdown
- source: 04-component-development.md
  content: "# Component Development\n\n## Overview\n\nKeboola components are Docker\
    \ containers that follow the Common Interface specification for processing data.\
    \ They communicate with Keboola exclusively through the filesystem at `/data`.\n\
    \n## Component Types\n\n- **Extractors**: Pull data from external sources\n- **Writers**:\
    \ Send data to external destinations\n- **Applications**: Process or transform\
    \ data\n\nNote: Don't include component type names ('extractor', 'writer', 'application')\
    \ in the component name itself.\n\n## Project Structure\n\n```\nmy-component/\n\
    ├── src/\n│   ├── component.py          # Main logic with run() function\n│  \
    \ └── configuration.py      # Configuration validation\n├── component_config/\n\
    │   ├── component_config.json           # Configuration schema\n│   ├── component_long_description.md\
    \   # Detailed docs\n│   └── component_short_description.md  # Brief description\n\
    ├── tests/\n│   └── test_component.py     # Unit tests\n├── data/            \
    \         # Local data folder (gitignored)\n│   ├── config.json           # Example\
    \ config for local testing\n│   ├── in/                   # Input tables and files\n\
    │   └── out/                  # Output tables and files\n├── .github/workflows/\n\
    │   └── push.yml              # CI/CD deployment\n├── Dockerfile             \
    \   # Container definition\n└── pyproject.toml            # Python dependencies\n\
    ```\n\n## Data Folder Contract\n\nComponents communicate with Keboola through\
    \ the `/data` directory:\n\n**INPUT** (read-only):\n- `config.json` - Component\
    \ configuration from UI\n- `in/tables/*.csv` - Input tables with `.manifest` files\n\
    - `in/files/*` - Input files\n- `in/state.json` - Previous run state (for incremental\
    \ processing)\n\n**OUTPUT** (write):\n- `out/tables/*.csv` - Output tables with\
    \ `.manifest` files\n- `out/files/*` - Output files\n- `out/state.json` - New\
    \ state for next run\n\n**IMPORTANT**: The Keboola platform automatically creates\
    \ all data directories (`data/in/`, `data/out/tables/`, `data/out/files/`, etc.).\
    \ You **never** need to call `mkdir()` or create these directories manually in\
    \ your component code.\n\n## Basic Component Implementation\n\n```python\nfrom\
    \ keboola.component import CommonInterface\nimport logging\nimport sys\nimport\
    \ traceback\n\nREQUIRED_PARAMETERS = ['api_key', 'endpoint']\n\nclass Component(CommonInterface):\n\
    \    def __init__(self):\n        super().__init__()\n\n    def run(self):\n \
    \       try:\n            # 1. Validate configuration\n            self.validate_configuration(REQUIRED_PARAMETERS)\n\
    \            params = self.configuration.parameters\n\n            # 2. Load state\
    \ for incremental processing\n            state = self.get_state_file()\n    \
    \        last_timestamp = state.get('last_timestamp')\n\n            # 3. Process\
    \ input tables\n            input_tables = self.get_input_tables_definitions()\n\
    \            for table in input_tables:\n                self._process_table(table)\n\
    \n            # 4. Create output tables with manifests\n            self._create_output_tables()\n\
    \n            # 5. Save state for next run\n            self.write_state_file({\n\
    \                'last_timestamp': current_timestamp\n            })\n\n     \
    \   except ValueError as err:\n            # User errors (configuration/input\
    \ issues)\n            logging.error(str(err))\n            print(err, file=sys.stderr)\n\
    \            sys.exit(1)\n        except Exception as err:\n            # System\
    \ errors (unhandled exceptions)\n            logging.exception(\"Unhandled error\
    \ occurred\")\n            traceback.print_exc(file=sys.stderr)\n            sys.exit(2)\n\
    \nif __name__ == '__main__':\n    try:\n        comp = Component()\n        comp.run()\n\
    \    except Exception as e:\n        logging.exception(\"Component execution failed\"\
    )\n        sys.exit(2)\n```\n\n## Configuration Schema\n\nDefine configuration\
    \ parameters in `component_config/component_config.json`:\n\n```json\n{\n  \"\
    type\": \"object\",\n  \"title\": \"Configuration\",\n  \"required\": [\"api_key\"\
    , \"endpoint\"],\n  \"properties\": {\n    \"#api_key\": {\n      \"type\": \"\
    string\",\n      \"title\": \"API Key\",\n      \"description\": \"Your API authentication\
    \ token\",\n      \"format\": \"password\"\n    },\n    \"endpoint\": {\n    \
    \  \"type\": \"string\",\n      \"title\": \"API Endpoint\",\n      \"description\"\
    : \"Base URL for the API\"\n    },\n    \"incremental\": {\n      \"type\": \"\
    boolean\",\n      \"title\": \"Incremental Load\",\n      \"description\": \"\
    Only fetch data since last run\",\n      \"default\": false\n    }\n  }\n}\n```\n\
    \n### Sensitive Data Handling\n\nPrefix parameter names with `#` to enable automatic\
    \ hashing:\n```json\n{\n  \"#password\": {\n    \"type\": \"string\",\n    \"\
    title\": \"Password\",\n    \"format\": \"password\"\n  }\n}\n```\n\n### UI Elements\n\
    \n**Code Editor** (ACE editor for multi-line input):\n```json\n{\n  \"query\"\
    : {\n    \"type\": \"string\",\n    \"title\": \"SQL Query\",\n    \"format\"\
    : \"textarea\",\n    \"options\": {\n      \"ace\": {\n        \"mode\": \"sql\"\
    \n      }\n    }\n  }\n}\n```\n\n**Test Connection Button**:\n```json\n{\n  \"\
    test_connection\": {\n    \"type\": \"button\",\n    \"title\": \"Test Connection\"\
    ,\n    \"options\": {\n      \"syncAction\": \"test-connection\"\n    }\n  }\n\
    }\n```\n\n## CSV Processing\n\nAlways process CSV files efficiently using generators:\n\
    \n```python\nimport csv\n\ndef process_input_table(table_def):\n    with open(table_def.full_path,\
    \ 'r', encoding='utf-8') as in_file:\n        # Handle null characters with generator\n\
    \        lazy_lines = (line.replace('\\0', '') for line in in_file)\n        reader\
    \ = csv.DictReader(lazy_lines, dialect='kbc')\n\n        for row in reader:\n\
    \            # Process row by row for memory efficiency\n            yield process_row(row)\n\
    ```\n\n## Creating Output Tables\n\nCreate output tables with proper schema definitions:\n\
    \n```python\nfrom collections import OrderedDict\nfrom keboola.component.dao import\
    \ ColumnDefinition, BaseType\n\n# Define schema\nschema = OrderedDict({\n    \"\
    id\": ColumnDefinition(\n        data_types=BaseType.integer(),\n        primary_key=True\n\
    \    ),\n    \"name\": ColumnDefinition(),\n    \"value\": ColumnDefinition(\n\
    \        data_types=BaseType.numeric(length=\"10,2\")\n    )\n})\n\n# Create table\
    \ definition\nout_table = self.create_out_table_definition(\n    name=\"results.csv\"\
    ,\n    destination=\"out.c-data.results\",\n    schema=schema,\n    incremental=True\n\
    )\n\n# Write data\nimport csv\nwith open(out_table.full_path, 'w', newline='',\
    \ encoding='utf-8') as f:\n    writer = csv.DictWriter(f, fieldnames=out_table.column_names)\n\
    \    writer.writeheader()\n    for row in data:\n        writer.writerow(row)\n\
    \n# Write manifest\nself.write_manifest(out_table)\n```\n\n## State Management\
    \ for Incremental Processing\n\nImplement proper state handling for incremental\
    \ loads:\n\n```python\ndef run_incremental(self):\n    # Load previous state\n\
    \    state = self.get_state_file()\n    last_timestamp = state.get('last_timestamp',\
    \ '1970-01-01T00:00:00Z')\n\n    # Fetch only new data since last_timestamp\n\
    \    new_data = self._fetch_data_since(last_timestamp)\n\n    # Process and save\
    \ data\n    self._process_data(new_data)\n\n    # Update state with current timestamp\n\
    \    from datetime import datetime, timezone\n    current_timestamp = datetime.now(timezone.utc).isoformat()\n\
    \    self.write_state_file({\n        'last_timestamp': current_timestamp,\n \
    \       'records_processed': len(new_data)\n    })\n```\n\n## Error Handling\n\
    \nFollow Keboola's error handling conventions:\n\n- **Exit code 1**: User errors\
    \ (configuration problems, invalid inputs)\n- **Exit code 2**: System errors (unhandled\
    \ exceptions, application errors)\n\n```python\ntry:\n    # Component logic\n\
    \    validate_inputs(params)\n    result = perform_operation()\n\nexcept ValueError\
    \ as err:\n    # User-fixable errors\n    logging.error(f\"Configuration error:\
    \ {err}\")\n    print(err, file=sys.stderr)\n    sys.exit(1)\n\nexcept requests.HTTPError\
    \ as err:\n    # API errors\n    logging.error(f\"API request failed: {err}\"\
    )\n    print(f\"Failed to connect to API: {err.response.status_code}\", file=sys.stderr)\n\
    \    sys.exit(1)\n\nexcept Exception as err:\n    # Unhandled exceptions\n   \
    \ logging.exception(\"Unhandled error in component execution\")\n    traceback.print_exc(file=sys.stderr)\n\
    \    sys.exit(2)\n```\n\n## Local Development\n\n### Running Locally\n\n```bash\n\
    # Set up virtual environment\npython -m venv .venv\nsource .venv/bin/activate\n\
    pip install -e .\n\n# Set data directory environment variable\nexport KBC_DATADIR=./data\n\
    \n# Run component\npython src/component.py\n```\n\n### Using Docker\n\n```bash\n\
    # Build image\ndocker build -t my-component:latest .\n\n# Run with mounted data\
    \ folder\ndocker run --rm \\\n  -v $(pwd)/data:/data \\\n  -e KBC_DATADIR=/data\
    \ \\\n  my-component:latest\n```\n\n### Prepare Test Data\n\nCreate `data/config.json`\
    \ with example parameters:\n\n```json\n{\n  \"parameters\": {\n    \"api_key\"\
    : \"your_key_here\",\n    \"#password\": \"test_password\",\n    \"from_date\"\
    : \"2024-01-01\",\n    \"incremental\": false\n  }\n}\n```\n\nCreate sample input\
    \ tables:\n\n```bash\nmkdir -p data/in/tables\ncat > data/in/tables/input.csv\
    \ <<EOF\nid,name,email\n1,John Doe,john@example.com\n2,Jane Smith,jane@example.com\n\
    EOF\n```\n\n## Best Practices\n\n### DO:\n\n- Use `CommonInterface` class for\
    \ all Keboola interactions\n- Validate configuration early with `validate_configuration()`\n\
    - Process CSV files with generators for memory efficiency\n- Always specify `encoding='utf-8'`\
    \ for file operations\n- Use proper exit codes (1 for user errors, 2 for system\
    \ errors)\n- Define explicit schemas for output tables\n- Implement state management\
    \ for incremental processing\n- Write comprehensive tests\n- Quote all SQL identifiers\
    \ (`\"column_name\"`, not `column_name`)\n\n### DON'T:\n\n- Load entire CSV files\
    \ into memory\n- Hard-code configuration values\n- Skip configuration validation\n\
    - Forget to write manifests for output tables\n- Skip state file management for\
    \ incremental loads\n- Forget to handle null characters in CSV files\n- Call `mkdir()`\
    \ for platform-managed directories (in/, out/, tables/, files/)\n\n## Dockerfile\n\
    \n```dockerfile\nFROM python:3.11-alpine\n\n# Install dependencies\nWORKDIR /code\n\
    COPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n\
    # Copy component code\nCOPY src/ /code/src/\n\n# Set entrypoint with unbuffered\
    \ output\nENTRYPOINT [\"python\", \"-u\", \"/code/src/component.py\"]\n```\n\n\
    ## CI/CD Deployment\n\n### GitHub Actions Workflow\n\n```yaml\n# .github/workflows/push.yml\n\
    name: Build and Deploy\n\non:\n  push:\n    tags:\n      - 'v*'\n\njobs:\n  build:\n\
    \    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n\
    \      - name: Build Docker image\n        run: docker build -t my-component:${{\
    \ github.ref_name }} .\n\n      - name: Run tests\n        run: docker-compose\
    \ run --rm test\n\n      - name: Deploy to Keboola\n        env:\n          KBC_DEVELOPERPORTAL_USERNAME:\
    \ ${{ secrets.KBC_USERNAME }}\n          KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_PASSWORD\
    \ }}\n        run: ./deploy.sh\n```\n\n### Version Management\n\nFollow semantic\
    \ versioning:\n\n- **v1.0.0** - Major release (breaking changes)\n- **v1.1.0**\
    \ - Minor release (new features)\n- **v1.0.1** - Patch release (bug fixes)\n\n\
    ```bash\n# Tag and push\ngit tag -a v1.0.0 -m \"Release version 1.0.0\"\ngit push\
    \ origin v1.0.0\n```\n\n## Testing\n\n### Unit Tests\n\n```python\nimport unittest\n\
    from src.component import Component\n\nclass TestComponent(unittest.TestCase):\n\
    \    def test_configuration_validation(self):\n        \"\"\"Test that required\
    \ parameters are validated.\"\"\"\n        # Test implementation\n\n    def test_csv_processing(self):\n\
    \        \"\"\"Test CSV reading and writing with proper encoding.\"\"\"\n    \
    \    # Test implementation\n\n    def test_state_management(self):\n        \"\
    \"\"Test state file persistence.\"\"\"\n        # Test implementation\n```\n\n\
    Run tests:\n\n```bash\n# Using unittest\npython -m unittest discover -s tests\n\
    \n# Using pytest\npytest tests/ -v --cov=src\n```\n\n## Code Quality\n\nUse Ruff\
    \ for code formatting and linting:\n\n```bash\n# Format code\nruff format .\n\n\
    # Lint and auto-fix issues\nruff check --fix .\n```\n\n## Resources\n\n- [Keboola\
    \ Developer Docs](https://developers.keboola.com/)\n- [Python Component Library](https://github.com/keboola/python-component)\n\
    - [Component Tutorial](https://developers.keboola.com/extend/component/tutorial/)\n\
    - [Cookiecutter Template](https://github.com/keboola/cookiecutter-python-component)\n"
  format: markdown
- source: 05-dataapp-development.md
  content: "# Data App Development\n\n## Overview\n\nKeboola Data Apps are Streamlit\
    \ applications that run directly in the Keboola platform, providing interactive\
    \ dashboards and analytics tools. They connect to Keboola Storage and can query\
    \ data from workspace tables.\n\n## Key Concepts\n\n### What are Data Apps?\n\n\
    Data Apps are containerized Streamlit applications that:\n- Run inside Keboola's\
    \ infrastructure\n- Have direct access to project data via workspace\n- Support\
    \ interactive filtering, visualization, and exploration\n- Can be shared with\
    \ team members\n- Auto-scale based on usage\n\n### Architecture Pattern: SQL-First\n\
    \n**Core Principle**: Push computation to the database, never load large datasets\
    \ into Python.\n\nWhy?\n- Keboola workspaces (Snowflake, Redshift, BigQuery) are\
    \ optimized for queries\n- Loading data into Streamlit doesn't scale\n- SQL aggregation\
    \ is 10-100x faster than pandas\n\n## Project Structure\n\n```\nmy-dataapp/\n\
    ├── streamlit_app.py          # Main app entry point with sidebar\n├── pages/\n\
    │   ├── 01_Overview.py        # First page\n│   ├── 02_Analytics.py       # Second\
    \ page\n│   └── 03_Details.py         # Third page\n├── utils/\n│   ├── data_loader.py\
    \        # Centralized data access\n│   └── config.py             # Environment\
    \ configuration\n├── requirements.txt          # Python dependencies\n└── README.md\
    \                 # Documentation\n```\n\n## Environment Setup\n\n### Local Development\n\
    \nData apps must work in two environments with **different contexts**:\n\n1. **Local\
    \ Development (Storage API / Project Context)**: \n   - Uses Storage API token\
    \ for authentication\n   - References tables as `in.c-bucket.table`\n   - Exports\
    \ data via REST API\n   - No workspace ID involved\n\n2. **Production (Workspace\
    \ Context)**: \n   - Uses workspace database connection\n   - References tables\
    \ as `\"PROJECT_ID\".\"in.c-bucket\".\"table\"`\n   - Queries data via SQL\n \
    \  - Requires workspace environment variables\n\n**Why Two Contexts?**\n\nIn production,\
    \ Data Apps run inside a **Keboola workspace** (Snowflake/Redshift instance) where\
    \ your project data is mirrored. This provides:\n- Direct SQL access (fast queries)\n\
    - No API rate limits\n- Native database features\n\nDuring local development,\
    \ you don't have workspace access, so you use the **Storage API** (REST) to export\
    \ data.\n\n**Environment Variables by Context**:\n\n```python\n# WORKSPACE CONTEXT\
    \ (Production)\n# Automatically set by Keboola platform:\nKBC_PROJECT_ID=6789\
    \           # Your project ID (used in table references)\nKBC_BUCKET_ID=in.c-main\
    \       # Default bucket for app\nKBC_TABLE_NAME=customers      # Default table\
    \ for app\n\n# STORAGE API CONTEXT (Local)\n# You must set manually:\nKEBOOLA_TOKEN=your-token\
    \                        # Storage API token\nKEBOOLA_STACK_URL=connection.keboola.com\
    \       # Your stack URL\n```\n\n```python\n# utils/config.py\nimport os\nimport\
    \ streamlit as st\n\ndef get_connection_mode():\n    \"\"\"Detect if running locally\
    \ or in Keboola.\"\"\"\n    return 'workspace' if 'KBC_PROJECT_ID' in os.environ\
    \ else 'local'\n\ndef get_storage_token():\n    \"\"\"Get Storage API token from\
    \ environment.\"\"\"\n    return os.environ.get('KEBOOLA_TOKEN')\n\ndef get_stack_url():\n\
    \    \"\"\"Get Keboola stack URL.\"\"\"\n    return os.environ.get('KEBOOLA_STACK_URL',\
    \ 'connection.keboola.com')\n```\n\n### Connection Setup\n\n```python\n# utils/data_loader.py\n\
    import os\nimport streamlit as st\nfrom utils.config import get_connection_mode\n\
    \n@st.cache_resource\ndef get_connection():\n    \"\"\"Get database connection\
    \ based on environment.\"\"\"\n    mode = get_connection_mode()\n\n    if mode\
    \ == 'workspace':\n        # Running in Keboola - use workspace connection\n \
    \       return st.connection('snowflake', type='snowflake')\n    else:\n     \
    \   # Local development - use Storage API\n        return None  # Implement Storage\
    \ API wrapper\n\ndef get_table_name():\n    \"\"\"Get fully qualified table name.\"\
    \"\"\n    mode = get_connection_mode()\n\n    if mode == 'workspace':\n      \
    \  # In workspace: database.schema.table\n        return f'\"{os.environ[\"KBC_PROJECT_ID\"\
    ]}\".\"{os.environ[\"KBC_BUCKET_ID\"]}\".\"{os.environ[\"KBC_TABLE_NAME\"]}\"\
    '\n    else:\n        # Local: bucket.table\n        return 'in.c-analysis.usage_data'\n\
    ```\n\n## SQL-First Design Pattern\n\n### Good Pattern: Aggregate in Database\n\
    \n```python\n@st.cache_data(ttl=300)\ndef get_summary_metrics(where_clause: str\
    \ = \"\"):\n    query = f'''\n        SELECT\n            COUNT(*) as total_count,\n\
    \            COUNT(DISTINCT \"user_id\") as unique_users,\n            AVG(\"\
    session_duration\") as avg_duration,\n            SUM(\"revenue\") as total_revenue\n\
    \        FROM {get_table_name()}\n        WHERE \"date\" >= CURRENT_DATE - INTERVAL\
    \ '90 days'\n            {f\"AND {where_clause}\" if where_clause else \"\"}\n\
    \    '''\n    return execute_query(query)\n```\n\n### Bad Pattern: Load All Data\n\
    \n```python\n# DON'T DO THIS\ndf = execute_query(f\"SELECT * FROM {get_table_name()}\"\
    )\nresult = df.groupby('category').agg({'value': 'mean'})\n```\n\n## Global Filter\
    \ Pattern\n\nGlobal filters allow users to filter all pages from one control in\
    \ the sidebar.\n\n### Step 1: Create Filter Function\n\n```python\n# utils/data_loader.py\n\
    import streamlit as st\n\ndef get_user_type_filter_clause():\n    \"\"\"Get SQL\
    \ WHERE clause for user type filter.\"\"\"\n    # Initialize session state with\
    \ default\n    if 'user_filter' not in st.session_state:\n        st.session_state.user_filter\
    \ = 'external'\n\n    # Return appropriate SQL condition\n    if st.session_state.user_filter\
    \ == 'external':\n        return '\"user_type\" = \\'External User\\''\n    elif\
    \ st.session_state.user_filter == 'internal':\n        return '\"user_type\" =\
    \ \\'Keboola User\\''\n    return ''  # 'all' - no filter\n```\n\n### Step 2:\
    \ Add UI Control\n\n```python\n# streamlit_app.py (sidebar)\nimport streamlit\
    \ as st\nfrom utils.data_loader import get_user_type_filter_clause\n\nst.set_page_config(page_title=\"\
    My Dashboard\", layout=\"wide\")\n\n# Initialize session state\nif 'user_filter'\
    \ not in st.session_state:\n    st.session_state.user_filter = 'external'\n\n\
    # Sidebar filter\nst.sidebar.header(\"Filters\")\nuser_option = st.sidebar.radio(\n\
    \    \"User Type:\",\n    options=['external', 'internal', 'all'],\n    index=['external',\
    \ 'internal', 'all'].index(st.session_state.user_filter)\n)\n\n# Update session\
    \ state and trigger rerun if changed\nif user_option != st.session_state.user_filter:\n\
    \    st.session_state.user_filter = user_option\n    st.rerun()\n```\n\n### Step\
    \ 3: Use Filter in Pages\n\n```python\n# pages/01_Overview.py\nimport streamlit\
    \ as st\nfrom utils.data_loader import get_user_type_filter_clause, execute_query\n\
    \n# Build WHERE clause\nwhere_parts = ['\"status\" = \\'active\\'']  # Base filter\n\
    user_filter = get_user_type_filter_clause()\nif user_filter:\n    where_parts.append(user_filter)\n\
    where_clause = ' AND '.join(where_parts)\n\n# Use in query\n@st.cache_data(ttl=300)\n\
    def get_page_data():\n    query = f'''\n        SELECT \"date\", COUNT(*) as count\n\
    \        FROM {get_table_name()}\n        WHERE {where_clause}\n        GROUP\
    \ BY \"date\"\n        ORDER BY \"date\"\n    '''\n    return execute_query(query)\n\
    \ndf = get_page_data()\nst.line_chart(df, x='date', y='count')\n```\n\n## Query\
    \ Execution\n\n### Basic Query Function\n\n```python\n# utils/data_loader.py\n\
    import streamlit as st\nimport pandas as pd\n\n@st.cache_data(ttl=300)\ndef execute_query(sql:\
    \ str) -> pd.DataFrame:\n    \"\"\"Execute SQL query and return DataFrame.\"\"\
    \"\n    conn = get_connection()\n\n    try:\n        df = conn.query(sql)\n  \
    \      return df\n    except Exception as e:\n        st.error(f\"Query failed:\
    \ {e}\")\n        return pd.DataFrame()\n```\n\n### SQL Best Practices\n\n**Always\
    \ quote identifiers**:\n```sql\n-- CORRECT\nSELECT \"user_id\", \"revenue\" FROM\
    \ \"my_table\"\n\n-- WRONG (fails with reserved keywords or mixed case)\nSELECT\
    \ user_id, revenue FROM my_table\n```\n\n**Use parameterized WHERE clauses**:\n\
    ```python\ndef get_date_filter_clause(start_date, end_date):\n    \"\"\"Generate\
    \ date range filter.\"\"\"\n    return f'\"date\" BETWEEN \\'{start_date}\\' AND\
    \ \\'{end_date}\\''\n```\n\n## Caching Strategy\n\n### Cache Database Connections\n\
    \n```python\n@st.cache_resource\ndef get_connection():\n    \"\"\"Cache connection\
    \ object (doesn't change).\"\"\"\n    return st.connection('snowflake', type='snowflake')\n\
    ```\n\n### Cache Query Results\n\n```python\n@st.cache_data(ttl=300)  # Cache\
    \ for 5 minutes\ndef get_metrics(where_clause: str):\n    \"\"\"Cache query results\
    \ (data can change).\"\"\"\n    query = f\"SELECT COUNT(*) FROM {get_table_name()}\
    \ WHERE {where_clause}\"\n    return execute_query(query)\n```\n\n### TTL Guidelines\n\
    \n- **Static reference data**: `ttl=3600` (1 hour)\n- **Dashboard metrics**: `ttl=300`\
    \ (5 minutes)\n- **Real-time data**: `ttl=60` (1 minute)\n- **User-specific data**:\
    \ No cache or very short TTL\n\n## Session State Management\n\nStreamlit reruns\
    \ the entire script on every interaction. Use session state to persist values.\n\
    \n### Initialize Before Use\n\n```python\n# Always initialize session state before\
    \ creating widgets\nif 'selected_category' not in st.session_state:\n    st.session_state.selected_category\
    \ = 'all'\n\n# Now create widget\ncategory = st.selectbox(\n    \"Category\",\n\
    \    options=['all', 'sales', 'marketing'],\n    index=['all', 'sales', 'marketing'].index(st.session_state.selected_category)\n\
    )\n\n# Update session state if changed\nif category != st.session_state.selected_category:\n\
    \    st.session_state.selected_category = category\n    st.rerun()\n```\n\n##\
    \ Error Handling\n\n### Handle Empty Results\n\n```python\ndf = get_page_data()\n\
    \nif df.empty:\n    st.warning(\"No data available for the selected filters.\"\
    )\nelse:\n    st.line_chart(df, x='date', y='count')\n```\n\n### Catch Query Errors\n\
    \n```python\n@st.cache_data(ttl=300)\ndef execute_query(sql: str):\n    try:\n\
    \        conn = get_connection()\n        return conn.query(sql)\n    except Exception\
    \ as e:\n        st.error(f\"Database query failed: {e}\")\n        return pd.DataFrame()\n\
    ```\n\n## Common Patterns\n\n### Metric Cards\n\n```python\n@st.cache_data(ttl=300)\n\
    def get_kpi_metrics():\n    query = f'''\n        SELECT\n            COUNT(*)\
    \ as total_users,\n            SUM(\"revenue\") as total_revenue,\n          \
    \  AVG(\"session_duration\") as avg_duration\n        FROM {get_table_name()}\n\
    \        WHERE \"date\" >= CURRENT_DATE - INTERVAL '30 days'\n    '''\n    return\
    \ execute_query(query).iloc[0]\n\nmetrics = get_kpi_metrics()\n\ncol1, col2, col3\
    \ = st.columns(3)\ncol1.metric(\"Total Users\", f\"{metrics['total_users']:,}\"\
    )\ncol2.metric(\"Revenue\", f\"${metrics['total_revenue']:,.2f}\")\ncol3.metric(\"\
    Avg Duration\", f\"{metrics['avg_duration']:.1f}s\")\n```\n\n### Date Range Filter\n\
    \n```python\nimport datetime\n\ncol1, col2 = st.columns(2)\nstart_date = col1.date_input(\"\
    Start Date\", datetime.date.today() - datetime.timedelta(days=30))\nend_date =\
    \ col2.date_input(\"End Date\", datetime.date.today())\n\nwhere_clause = f'\"\
    date\" BETWEEN \\'{start_date}\\' AND \\'{end_date}\\''\n```\n\n### Dynamic Dropdown\n\
    \n```python\n@st.cache_data(ttl=3600)\ndef get_categories():\n    query = f'SELECT\
    \ DISTINCT \"category\" FROM {get_table_name()} ORDER BY \"category\"'\n    return\
    \ execute_query(query)['category'].tolist()\n\ncategories = get_categories()\n\
    selected = st.selectbox(\"Category\", options=['All'] + categories)\n```\n\n##\
    \ Variable Naming Conventions\n\n### Avoid Naming Conflicts\n\n**Problem**: Using\
    \ same variable name for SQL clause and UI widget\n```python\n# DON'T DO THIS\n\
    user_filter = get_user_filter_clause()  # SQL string\nuser_filter = st.radio(\"\
    User Type\", ...)  # UI widget - overwrites SQL!\n```\n\n**Solution**: Use descriptive,\
    \ unique names\n```python\n# DO THIS\nuser_filter_sql = get_user_filter_clause()\
    \  # SQL string\nuser_filter_option = st.radio(\"User Type\", ...)  # UI widget\n\
    ```\n\n### Session State Keys\n\nUse consistent, descriptive keys:\n```python\n\
    # Good\nst.session_state.user_type_filter = 'external'\nst.session_state.selected_date_range\
    \ = (start, end)\nst.session_state.page_number = 1\n\n# Bad (ambiguous)\nst.session_state.filter\
    \ = 'external'\nst.session_state.data = (start, end)\nst.session_state.page =\
    \ 1\n```\n\n## Deployment\n\n### Requirements File\n\n```txt\n# requirements.txt\n\
    streamlit>=1.28.0\npandas>=2.0.0\nsnowflake-connector-python>=3.0.0\nplotly>=5.17.0\n\
    ```\n\n### Environment Variables\n\nRequired in Keboola deployment:\n- `KBC_PROJECT_ID`\
    \ - Automatically set by platform\n- `KBC_BUCKET_ID` - Automatically set by platform\n\
    - `KEBOOLA_TOKEN` - Set in Data App configuration\n- `KEBOOLA_STACK_URL` - Set\
    \ in Data App configuration\n\n### Testing Before Deployment\n\n```bash\n# Local\
    \ testing\nexport KEBOOLA_TOKEN=your_token\nexport KEBOOLA_STACK_URL=connection.keboola.com\n\
    streamlit run streamlit_app.py\n```\n\n## Best Practices\n\n### DO:\n\n- Always\
    \ validate data schemas before writing code\n- Push computation to database -\
    \ aggregate in SQL, not Python\n- Use fully qualified table names from `get_table_name()`\n\
    - Quote all identifiers in SQL (`\"column_name\"`)\n- Cache all queries with `@st.cache_data(ttl=300)`\n\
    - Centralize data access in `utils/data_loader.py`\n- Initialize session state\
    \ with defaults before UI controls\n- Use unique, descriptive variable names\n\
    - Test visually before deploying\n- Handle empty DataFrames gracefully\n- Support\
    \ both local and production environments\n\n### DON'T:\n\n- Skip data validation\
    \ - always check schemas first\n- Load large datasets into Python - aggregate\
    \ in database\n- Hardcode table names - use `get_table_name()` function\n- Use\
    \ same variable name twice (SQL clause and UI widget)\n- Forget session state\
    \ initialization before creating widgets\n- Assume columns exist - validate first\n\
    - Use unquoted SQL identifiers\n- Skip error handling for empty query results\n\
    - Deploy without local testing\n\n## Visual Verification Workflow\n\nBefore deploying,\
    \ test your app:\n\n1. **Start local server**: `streamlit run streamlit_app.py`\n\
    2. **Open in browser**: `http://localhost:8501`\n3. **Test all interactions**:\n\
    \   - Click through all pages\n   - Try all filter combinations\n   - Verify metrics\
    \ update correctly\n   - Check for error messages\n4. **Capture screenshots**\
    \ of working features\n5. **Deploy with confidence**\n\n## Common Issues\n\n###\
    \ \"KeyError: 'column_name'\"\n\n**Cause**: Column doesn't exist or wrong name\n\
    **Solution**: Validate schema before querying:\n```python\n# Check available columns\
    \ first\nquery = f'SELECT * FROM {get_table_name()} LIMIT 1'\ndf = execute_query(query)\n\
    print(df.columns)  # See actual column names\n```\n\n### Filter Not Working\n\n\
    **Cause**: Filter SQL not included in WHERE clause\n**Solution**: Always build\
    \ WHERE clause systematically:\n```python\nwhere_parts = []\nif base_filter :=\
    \ get_base_filter():\n    where_parts.append(base_filter)\nif user_filter := get_user_filter_clause():\n\
    \    where_parts.append(user_filter)\nwhere_clause = ' AND '.join(where_parts)\
    \ if where_parts else '1=1'\n```\n\n### Session State Not Persisting\n\n**Cause**:\
    \ Not initializing before widget creation\n**Solution**: Initialize before use:\n\
    ```python\nif 'my_value' not in st.session_state:\n    st.session_state.my_value\
    \ = default_value\n\nwidget = st.text_input(\"Label\", value=st.session_state.my_value)\n\
    ```\n\n## Resources\n\n- [Streamlit Documentation](https://docs.streamlit.io)\n\
    - [Keboola Data Apps Guide](https://developers.keboola.com/extend/data-apps/)\n\
    - [Snowflake SQL Reference](https://docs.snowflake.com/en/sql-reference.html)\n\
    \ndef get_table_name():\n    \"\"\"Get fully qualified table name for current\
    \ context.\n    \n    Returns:\n        Workspace context: '\"PROJECT_ID\".\"\
    BUCKET_ID\".\"TABLE_NAME\"' (quoted, SQL-safe)\n        Storage API context: 'BUCKET_ID.TABLE_NAME'\
    \ (for API endpoints)\n    \n    Context difference:\n    - Workspace uses PROJECT_ID\
    \ as database name (Snowflake schema)\n    - Storage API uses bucket.table format\
    \ (no project ID)\n    \"\"\"\n    mode = get_connection_mode()\n\n    if mode\
    \ == 'workspace':\n        # WORKSPACE CONTEXT: Running in Keboola (has workspace\
    \ access)\n        # Use PROJECT_ID as database qualifier for Snowflake queries\n\
    \        project_id = os.environ['KBC_PROJECT_ID']  # e.g., \"6789\"\n       \
    \ bucket = os.environ.get('KBC_BUCKET_ID', 'in.c-analysis')  # e.g., \"in.c-main\"\
    \n        table = os.environ.get('KBC_TABLE_NAME', 'usage_data')  # e.g., \"customers\"\
    \n        \n        # Return: \"6789\".\"in.c-main\".\"customers\"\n        return\
    \ f'\"{project_id}\".\"{bucket}\".\"{table}\"'\n    else:\n        # STORAGE API\
    \ CONTEXT: Running locally (no workspace)\n        # Use bucket.table format for\
    \ Storage API endpoints\n        bucket = 'in.c-analysis'\n        table = 'usage_data'\n\
    \        \n        # Return: in.c-analysis.usage_data\n        return f'{bucket}.{table}'\n"
  format: markdown
