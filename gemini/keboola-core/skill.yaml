name: keboola-core
version: 1.0.0
description: Keboola platform knowledge for Gemini
metadata:
  generated_at: '2025-12-16T16:33:18.036536'
  source_path: docs/keboola
  generator: gemini_generator.py v1.0
  poc_notice: This is a POC. Not production-ready.
knowledge_base:
- source: 01-core-concepts.md
  content: "# Core Concepts\n\n## Overview\n\nKeboola is a cloud-based data platform\
    \ that enables you to extract, transform, and load data from various sources.\n\
    \n## Key Concepts\n\n### Project\nA project is the top-level container in Keboola.\
    \ All your configurations, data, and orchestrations belong to a project.\n\n###\
    \ Storage\nKeboola Storage is where your data lives. It consists of:\n- **Buckets**:\
    \ Logical containers for tables\n- **Tables**: The actual data\n- **Files**: Temporary\
    \ file storage\n\n### Components\nComponents are the building blocks:\n- **Extractors**:\
    \ Pull data from external sources\n- **Transformations**: Process and modify data\n\
    - **Writers**: Send data to external destinations\n\n## Authentication\n\nUse\
    \ Storage API tokens for authentication:\n\n```python\nimport os\nimport requests\n\
    \nSTORAGE_TOKEN = os.environ[\"KEBOOLA_TOKEN\"]\nSTACK_URL = os.environ.get(\"\
    KEBOOLA_STACK_URL\", \"connection.keboola.com\")\n\nheaders = {\n    \"X-StorageApi-Token\"\
    : STORAGE_TOKEN,\n    \"Content-Type\": \"application/json\"\n}\n\nresponse =\
    \ requests.get(\n    f\"https://{STACK_URL}/v2/storage/tables\",\n    headers=headers\n\
    )\n```\n\n## Regional Stacks\n\nKeboola operates multiple regional stacks:\n-\
    \ **US**: connection.keboola.com\n- **EU**: connection.eu-central-1.keboola.com\n\
    - **Azure**: connection.north-europe.azure.keboola.com\n\nAlways use your project's\
    \ stack URL, not a hardcoded one.\n\n### Workspaces\n\nWorkspaces are temporary\
    \ database environments (Snowflake, Redshift, or BigQuery) created for:\n- **Data\
    \ Apps**: Direct database access for analytics\n- **Transformations**: SQL/Python\
    \ data processing\n- **Sandboxes**: Ad-hoc data exploration\n\n**Key Concepts**:\n\
    \n- **Workspace ID**: Identifies a specific workspace instance (e.g., `12345`)\n\
    - **Project ID**: Identifies your Keboola project (e.g., `6789`)\n- **Context**:\
    \ Determines which API/connection to use\n\n**Workspace vs Storage**:\n\n| Aspect\
    \ | Workspace | Storage |\n|--------|-----------|--------|\n| **Technology** |\
    \ Snowflake/Redshift/BigQuery | Keboola Storage API |\n| **Access Method** | Database\
    \ connection (SQL) | REST API (HTTP) |\n| **Use Case** | SQL queries, Data Apps\
    \ | Data management, orchestration |\n| **Persistence** | Temporary (auto-deleted)\
    \ | Permanent |\n| **Table Names** | `database.schema.table` | `bucket.table`\
    \ |\n\n**When to Use What**:\n\n```python\n# Use WORKSPACE when:\n# - Running\
    \ inside Data App (production)\n# - Running transformation\n# - Direct SQL queries\
    \ needed\nif 'KBC_PROJECT_ID' in os.environ:\n    conn = st.connection('snowflake',\
    \ type='snowflake')\n    query = f'SELECT * FROM \"{os.environ[\"KBC_PROJECT_ID\"\
    ]}\".\"in.c-main\".\"customers\"'\n    df = conn.query(query)\n\n# Use STORAGE\
    \ API when:\n# - Running outside Keboola (local development)\n# - Managing tables/buckets\n\
    # - Orchestrating data flows\nelse:\n    import requests\n    response = requests.post(\n\
    \        f\"https://{stack_url}/v2/storage/tables/in.c-main.customers/export-async\"\
    ,\n        headers={\"X-StorageApi-Token\": token}\n    )\n```\n"
  format: markdown
- source: 02-storage-api.md
  content: "# Storage API\n\n## Reading Tables\n\n### List All Tables\n\n```python\n\
    import requests\nimport os\n\nstack_url = os.environ.get(\"KEBOOLA_STACK_URL\"\
    , \"connection.keboola.com\")\ntoken = os.environ[\"KEBOOLA_TOKEN\"]\n\nresponse\
    \ = requests.get(\n    f\"https://{stack_url}/v2/storage/tables\",\n    headers={\"\
    X-StorageApi-Token\": token}\n)\n\ntables = response.json()\nfor table in tables:\n\
    \    print(f\"{table['id']}: {table['rowsCount']} rows\")\n```\n\n### Export Table\
    \ Data\n\n```python\nimport time\n\n# Start async export job (NOTE: POST method\
    \ required)\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n    headers={\"X-StorageApi-Token\": token}\n)\nresponse.raise_for_status()\n\
    \njob_id = response.json()[\"id\"]\n\n# Poll for completion with timeout\ntimeout\
    \ = 300  # 5 minutes\nstart_time = time.time()\n\nwhile time.time() - start_time\
    \ < timeout:\n    job_response = requests.get(\n        f\"https://{stack_url}/v2/storage/jobs/{job_id}\"\
    ,\n        headers={\"X-StorageApi-Token\": token}\n    )\n    job_response.raise_for_status()\n\
    \n    job = job_response.json()\n    \n    if job[\"status\"] == \"success\":\n\
    \        # Download and save data to file\n        file_url = job[\"results\"\
    ][\"file\"][\"url\"]\n        data_response = requests.get(file_url)\n       \
    \ \n        with open(\"table_data.csv\", \"wb\") as f:\n            f.write(data_response.content)\n\
    \        \n        print(f\"Table exported to table_data.csv\")\n        break\n\
    \    \n    elif job[\"status\"] in [\"error\", \"cancelled\", \"terminated\"]:\n\
    \        error_msg = job.get(\"error\", {}).get(\"message\", \"Unknown error\"\
    )\n        raise Exception(f\"Export job failed with status {job['status']}: {error_msg}\"\
    )\n    \n    time.sleep(2)\nelse:\n    raise TimeoutError(f\"Export job {job_id}\
    \ did not complete within {timeout} seconds\")\n\n# Optional: Load data into memory\
    \ if needed\nimport csv\nwith open(\"table_data.csv\", \"r\") as f:\n    reader\
    \ = csv.DictReader(f)\n    data = list(reader)\n```\n\n## Writing Tables\n\n###\
    \ Create Table from CSV\n\n```python\n### Create New Table from CSV\n\n```python\n\
    # Upload CSV file to create a NEW table\ncsv_data = \"id,name,value\\n1,foo,100\\\
    n2,bar,200\"\n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/buckets/in.c-main/tables-async\"\
    ,\n    headers={\n        \"X-StorageApi-Token\": token,\n        \"Content-Type\"\
    : \"text/csv\"\n    },\n    params={\n        \"name\": \"my_table\",\n      \
    \  \"primaryKey\": \"id\",  # Optional: set primary key\n        \"dataString\"\
    : csv_data\n    }\n)\nresponse.raise_for_status()\n\njob_id = response.json()[\"\
    id\"]\n# Poll job until completion (see export example above)\n```\n\n### Import\
    \ Data to Existing Table\n\n```python\n# Import data to an EXISTING table\ntable_id\
    \ = \"in.c-main.my_table\"\ncsv_data = \"id,name,value\\n3,baz,300\\n4,qux,400\"\
    \n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\"\
    ,\n    headers={\n        \"X-StorageApi-Token\": token,\n        \"Content-Type\"\
    : \"text/csv\"\n    },\n    params={\n        \"dataString\": csv_data\n    }\n\
    )\nresponse.raise_for_status()\n\njob_id = response.json()[\"id\"]\n# Poll job\
    \ until completion\n```\n\n### Incremental Loads (Append/Update Data)\n\n**Important**:\
    \ Incremental loads require the table to have a primary key defined.\n\n```python\n\
    # Incremental load: append new rows or update existing rows by primary key\ntable_id\
    \ = \"in.c-main.my_table\"\ncsv_data = \"id,name,value\\n1,foo_updated,150\\n5,new_row,500\"\
    \n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\"\
    ,\n    headers={\n        \"X-StorageApi-Token\": token,\n        \"Content-Type\"\
    : \"text/csv\"\n    },\n    params={\n        \"incremental\": \"1\",  # Enable\
    \ incremental mode\n        \"dataString\": csv_data\n    }\n)\nresponse.raise_for_status()\n\
    \njob_id = response.json()[\"id\"]\n# Poll job until completion\n```\n\n**How\
    \ incremental mode works**:\n- If a row with the same primary key exists, it gets\
    \ UPDATED\n- If a row with a new primary key exists, it gets APPENDED\n- Existing\
    \ rows not in the import data are NOT deleted\n\n### Set or Change Primary Key\n\
    \n```python\n# Set primary key on existing table\ntable_id = \"in.c-main.my_table\"\
    \n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}\"\
    ,\n    headers={\n        \"X-StorageApi-Token\": token,\n        \"Content-Type\"\
    : \"application/json\"\n    },\n    json={\n        \"primaryKey\": [\"id\"] \
    \ # Can be multiple columns: [\"id\", \"date\"]\n    }\n)\nresponse.raise_for_status()\n\
    ```\n```\n\n## Common Patterns\n\n### Pagination\n\n#### Data Preview Pagination\n\
    \nFor quick data preview with small result sets, use limit/offset pagination:\n\
    \n```python\ndef export_table_paginated(table_id, chunk_size=10000):\n    \"\"\
    \"Export table preview in chunks using limit/offset.\"\"\"\n    offset = 0\n \
    \   all_data = []\n\n    while True:\n        response = requests.get(\n     \
    \       f\"https://{stack_url}/v2/storage/tables/{table_id}/data-preview\",\n\
    \            headers={\"X-StorageApi-Token\": token},\n            params={\n\
    \                \"limit\": chunk_size,\n                \"offset\": offset\n\
    \            }\n        )\n        response.raise_for_status()\n\n        chunk\
    \ = response.json()\n        if not chunk:\n            break\n\n        all_data.extend(chunk)\n\
    \        offset += chunk_size\n\n    return all_data\n```\n\n**Note**: `data-preview`\
    \ endpoint is limited to 1000 rows maximum. For larger datasets, use async export.\n\
    \n#### API Response Pagination (List Operations)\n\nMany API endpoints that return\
    \ lists support pagination parameters:\n\n```python\ndef list_all_tables_paginated():\n\
    \    \"\"\"List all tables with pagination support.\"\"\"\n    all_tables = []\n\
    \    offset = 0\n    limit = 100\n\n    while True:\n        response = requests.get(\n\
    \            f\"https://{stack_url}/v2/storage/tables\",\n            headers={\"\
    X-StorageApi-Token\": token},\n            params={\n                \"limit\"\
    : limit,\n                \"offset\": offset\n            }\n        )\n     \
    \   response.raise_for_status()\n\n        tables = response.json()\n        if\
    \ not tables:\n            break\n\n        all_tables.extend(tables)\n      \
    \  \n        # If fewer results than limit, we've reached the end\n        if\
    \ len(tables) < limit:\n            break\n            \n        offset += limit\n\
    \n    return all_tables\n```\n\n#### Pagination Parameters\n\nCommon pagination\
    \ parameters across Keboola Storage API:\n\n- **limit**: Number of records to\
    \ return (default and max vary by endpoint)\n- **offset**: Number of records to\
    \ skip\n\n```python\nparams = {\n    \"limit\": 100,    # Return up to 100 records\n\
    \    \"offset\": 200    # Skip first 200 records\n}\n```\n\n#### Full Table Export\
    \ (Recommended for Large Tables)\n\nFor exporting complete tables, especially\
    \ large ones, use async export instead of pagination:\n\n```python\ndef export_large_table(table_id):\n\
    \    \"\"\"Export large table using async job (handles pagination internally).\"\
    \"\"\n    # Start async export\n    response = requests.post(\n        f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n        headers={\"X-StorageApi-Token\": token}\n    )\n    response.raise_for_status()\n\
    \    job_id = response.json()[\"id\"]\n    \n    # Poll for completion\n    import\
    \ time\n    timeout = 600\n    start_time = time.time()\n    \n    while time.time()\
    \ - start_time < timeout:\n        job_response = requests.get(\n            f\"\
    https://{stack_url}/v2/storage/jobs/{job_id}\",\n            headers={\"X-StorageApi-Token\"\
    : token}\n        )\n        job_response.raise_for_status()\n        job = job_response.json()\n\
    \        \n        if job[\"status\"] == \"success\":\n            # Download\
    \ complete file (pagination handled by Keboola)\n            file_url = job[\"\
    results\"][\"file\"][\"url\"]\n            data_response = requests.get(file_url)\n\
    \            \n            with open(\"table_export.csv\", \"wb\") as f:\n   \
    \             f.write(data_response.content)\n            \n            return\
    \ \"table_export.csv\"\n        \n        elif job[\"status\"] in [\"error\",\
    \ \"cancelled\", \"terminated\"]:\n            error_msg = job.get(\"error\",\
    \ {}).get(\"message\", \"Unknown error\")\n            raise Exception(f\"Export\
    \ failed: {error_msg}\")\n        \n        time.sleep(2)\n    \n    raise TimeoutError(\"\
    Export job timeout\")\n```\n\n**When to use each approach**:\n\n- **data-preview\
    \ with pagination**: Quick checks, small datasets (<1000 rows)\n- **List endpoints\
    \ with pagination**: Browsing tables, buckets, configurations\n- **Async export**:\
    \ Production data export, large tables (>1000 rows)\n\n### Reading Data Incrementally\n\
    \nUse changedSince parameter to export only recently modified data:\n\n```python\n\
    from datetime import datetime, timedelta\n\n# Get data changed in last 24 hours\n\
    yesterday = (datetime.now() - timedelta(days=1)).isoformat()\n\nresponse = requests.post(\n\
    \    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\",\n   \
    \ headers={\"X-StorageApi-Token\": token},\n    params={\"changedSince\": yesterday}\n\
    )\nresponse.raise_for_status()\n\njob_id = response.json()[\"id\"]\n# Poll job\
    \ until completion\n```\n\n### Writing Data Incrementally\n\nSee the \"Incremental\
    \ Loads (Append/Update Data)\" section under \"Writing Tables\" above for how\
    \ to write data incrementally using `incremental: \"1\"` parameter.\n\n\n### Export\
    \ Table to File (Complete Example)\n\nFor a complete, production-ready example\
    \ that saves data to a file:\n\n```python\nimport requests\nimport os\nimport\
    \ time\n\nstack_url = os.environ.get(\"KEBOOLA_STACK_URL\", \"connection.keboola.com\"\
    )\ntoken = os.environ[\"KEBOOLA_TOKEN\"]\ntable_id = \"in.c-main.customers\"\n\
    output_file = \"customers.csv\"\n\ndef export_table_to_file(table_id, output_file,\
    \ timeout=300):\n    \"\"\"Export Keboola table to local CSV file.\"\"\"\n   \
    \ \n    # Start async export\n    response = requests.post(\n        f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n        headers={\"X-StorageApi-Token\": token}\n    )\n    response.raise_for_status()\n\
    \    job_id = response.json()[\"id\"]\n    \n    print(f\"Export job started:\
    \ {job_id}\")\n    \n    # Poll for completion\n    start_time = time.time()\n\
    \    while time.time() - start_time < timeout:\n        job_response = requests.get(\n\
    \            f\"https://{stack_url}/v2/storage/jobs/{job_id}\",\n            headers={\"\
    X-StorageApi-Token\": token}\n        )\n        job_response.raise_for_status()\n\
    \        job = job_response.json()\n        \n        if job[\"status\"] == \"\
    success\":\n            # Download file\n            file_url = job[\"results\"\
    ][\"file\"][\"url\"]\n            data_response = requests.get(file_url)\n   \
    \         \n            with open(output_file, \"wb\") as f:\n               \
    \ f.write(data_response.content)\n            \n            print(f\"Table exported\
    \ to {output_file}\")\n            return output_file\n        \n        elif\
    \ job[\"status\"] in [\"error\", \"cancelled\", \"terminated\"]:\n           \
    \ error_msg = job.get(\"error\", {}).get(\"message\", \"Unknown error\")\n   \
    \         raise Exception(f\"Job {job['status']}: {error_msg}\")\n        \n \
    \       time.sleep(2)\n    \n    raise TimeoutError(f\"Export did not complete\
    \ within {timeout}s\")\n\n# Usage\nexport_table_to_file(table_id, output_file)\n\
    ```\n\n\n## Storage vs Workspace Context\n\n### Understanding the Difference\n\
    \n**Storage API** operates at the **project level**:\n- Uses project-wide Storage\
    \ API token\n- Manages permanent data storage\n- Uses table IDs like `in.c-main.customers`\n\
    - Accessed via REST API endpoints\n- Used for: data ingestion, component development,\
    \ orchestration\n\n**Workspace** operates at the **workspace level**:\n- Uses\
    \ workspace-specific database credentials\n- Provides temporary SQL access to\
    \ project data\n- Uses qualified names like `\"PROJECT_ID\".\"in.c-main\".\"customers\"\
    `\n- Accessed via native database connections (JDBC/ODBC)\n- Used for: Data Apps,\
    \ transformations, SQL analysis\n\n### When to Use Storage API (Project Context)\n\
    \n✅ **Use Storage API when**:\n- Developing custom components\n- Running scripts\
    \ outside Keboola\n- Managing buckets and tables\n- Orchestrating data pipelines\n\
    - Local development/testing\n\n```python\n# Example: Local development script\n\
    import os\nimport requests\n\ntoken = os.environ['KEBOOLA_TOKEN']\nstack_url =\
    \ os.environ.get('KEBOOLA_STACK_URL', 'connection.keboola.com')\n\n# Project-level\
    \ API call\nresponse = requests.get(\n    f\"https://{stack_url}/v2/storage/tables\"\
    ,\n    headers={\"X-StorageApi-Token\": token}\n)\n\ntables = response.json()\n\
    for table in tables:\n    print(f\"Project table: {table['id']}\")\n```\n\n###\
    \ When to Use Workspace (Workspace Context)\n\n✅ **Use Workspace when**:\n- Building\
    \ Data Apps (production runtime)\n- Writing SQL transformations\n- Running queries\
    \ in Snowflake/Redshift workspace\n- Need direct database performance\n\n```python\n\
    # Example: Data App in production\nimport os\nimport streamlit as st\n\nif 'KBC_PROJECT_ID'\
    \ in os.environ:\n    # Running in workspace - use direct connection\n    conn\
    \ = st.connection('snowflake', type='snowflake')\n    \n    # Workspace-level\
    \ SQL query with qualified names\n    project_id = os.environ['KBC_PROJECT_ID']\n\
    \    query = f'''\n        SELECT * \n        FROM \"{project_id}\".\"in.c-main\"\
    .\"customers\"\n        WHERE \"status\" = 'active'\n    '''\n    df = conn.query(query)\n\
    else:\n    # Local development - use Storage API\n    # (see Storage API examples\
    \ above)\n    pass\n```\n\n### Hybrid Pattern: Support Both Contexts\n\nData Apps\
    \ should support both contexts for local development and production:\n\n```python\n\
    # utils/data_loader.py\nimport os\nimport streamlit as st\nimport requests\n\n\
    def get_connection_mode():\n    \"\"\"Detect runtime environment.\"\"\"\n    return\
    \ 'workspace' if 'KBC_PROJECT_ID' in os.environ else 'storage_api'\n\n@st.cache_resource\n\
    def get_connection():\n    \"\"\"Get appropriate connection for environment.\"\
    \"\"\n    mode = get_connection_mode()\n    \n    if mode == 'workspace':\n  \
    \      # Production: Use workspace connection\n        return st.connection('snowflake',\
    \ type='snowflake')\n    else:\n        # Local: Return Storage API client\n \
    \       return StorageAPIClient(\n            token=os.environ['KEBOOLA_TOKEN'],\n\
    \            stack_url=os.environ.get('KEBOOLA_STACK_URL')\n        )\n\ndef get_table_reference(bucket_id,\
    \ table_name):\n    \"\"\"Get correct table reference for environment.\"\"\"\n\
    \    mode = get_connection_mode()\n    \n    if mode == 'workspace':\n       \
    \ # Workspace: Fully qualified name\n        project_id = os.environ['KBC_PROJECT_ID']\n\
    \        return f'\"{project_id}\".\"{bucket_id}\".\"{table_name}\"'\n    else:\n\
    \        # Storage API: bucket.table format\n        return f\"{bucket_id}.{table_name}\"\
    \n\n# Usage in Data App\n@st.cache_data(ttl=300)\ndef load_customers():\n    conn\
    \ = get_connection()\n    table_ref = get_table_reference('in.c-main', 'customers')\n\
    \    \n    if get_connection_mode() == 'workspace':\n        query = f'SELECT\
    \ * FROM {table_ref}'\n        return conn.query(query)\n    else:\n        #\
    \ Use Storage API export\n        return conn.export_table(table_ref)\n```\n\n\
    ### Common Pitfalls\n\n❌ **Don't mix contexts**:\n```python\n# WRONG: Using Storage\
    \ API table ID in workspace SQL\nquery = f\"SELECT * FROM in.c-main.customers\"\
    \  # Fails in workspace\n\n# CORRECT: Use qualified names in workspace\nquery\
    \ = f'SELECT * FROM \"{project_id}\".\"in.c-main\".\"customers\"'\n```\n\n❌ **Don't\
    \ use workspace credentials in Storage API**:\n```python\n# WRONG: Workspace connection\
    \ for Storage API call\nconn = st.connection('snowflake')  # This is workspace,\
    \ not Storage API\n\n# CORRECT: Use Storage API token\nimport requests\nresponse\
    \ = requests.get(\n    f\"https://{stack_url}/v2/storage/tables\",\n    headers={\"\
    X-StorageApi-Token\": storage_token}\n)\n```\n"
  format: markdown
- source: 03-common-pitfalls.md
  content: "# Common Pitfalls\n\n## 1. Hardcoding Stack URLs\n\n**Problem**: Using\
    \ `connection.keboola.com` for all projects\n\n**Solution**: Always use environment\
    \ variables:\n\n```python\n# ❌ WRONG\nstack_url = \"connection.keboola.com\"\n\
    \n# ✅ CORRECT\nstack_url = os.environ.get(\"KEBOOLA_STACK_URL\", \"connection.keboola.com\"\
    )\n```\n\n## 2. Not Handling Job Polling\n\n**Problem**: Assuming async operations\
    \ complete immediately\n\n**Solution**: Always poll until job finishes:\n\n```python\n\
    def wait_for_job(job_id, timeout=300):\n    \"\"\"Wait for job completion with\
    \ timeout.\"\"\"\n    start = time.time()\n\n    while time.time() - start < timeout:\n\
    \        response = requests.get(\n            f\"https://{stack_url}/v2/storage/jobs/{job_id}\"\
    ,\n            headers={\"X-StorageApi-Token\": token}\n        )\n        response.raise_for_status()\n\
    \n        job = response.json()\n\n        if job[\"status\"] == \"success\":\n\
    \            return job\n        elif job[\"status\"] in [\"error\", \"cancelled\"\
    , \"terminated\"]:\n            error_msg = job.get(\"error\", {}).get(\"message\"\
    , \"Unknown error\")\n            raise Exception(f\"Job failed with status {job['status']}:\
    \ {error_msg}\")\n\n        time.sleep(2)\n\n    raise TimeoutError(f\"Job {job_id}\
    \ did not complete in {timeout}s\")\n```\n\n## 3. Ignoring Rate Limits\n\n**Problem**:\
    \ Making too many API calls too quickly\n\n**Solution**: Implement exponential\
    \ backoff:\n\n```python\nimport time\nfrom requests.exceptions import HTTPError\n\
    \ndef api_call_with_retry(url, headers, max_retries=3):\n    \"\"\"Make API call\
    \ with exponential backoff.\"\"\"\n    for attempt in range(max_retries):\n  \
    \      try:\n            response = requests.get(url, headers=headers)\n     \
    \       response.raise_for_status()\n            return response.json()\n\n  \
    \      except HTTPError as e:\n            if e.response.status_code == 429: \
    \ # Rate limited\n                wait_time = 2 ** attempt\n                print(f\"\
    Rate limited. Waiting {wait_time}s...\")\n                time.sleep(wait_time)\n\
    \            else:\n                raise\n\n    raise Exception(\"Max retries\
    \ exceeded\")\n```\n\n## 4. Not Validating Table IDs\n\n**Problem**: Using invalid\
    \ table ID format\n\n**Solution**: Validate format before API calls:\n\n```python\n\
    import re\n\ndef validate_table_id(table_id):\n    \"\"\"Validate Keboola table\
    \ ID format.\"\"\"\n    pattern = r'^(in|out)\\.c-[a-z0-9-]+\\.[a-z0-9_-]+$'\n\
    \n    if not re.match(pattern, table_id):\n        raise ValueError(\n       \
    \     f\"Invalid table ID: {table_id}. \"\n            f\"Expected format: stage.c-bucket.table\"\
    \n        )\n\n    return True\n\n# Usage\nvalidate_table_id(\"in.c-main.customers\"\
    )  # ✓\nvalidate_table_id(\"my_table\")  # ✗ Raises ValueError\n```\n\n## 5. Missing\
    \ Error Handling\n\n**Problem**: Not handling API errors gracefully\n\n**Solution**:\
    \ Always check response status:\n\n```python\ndef safe_api_call(url, headers):\n\
    \    \"\"\"Make API call with proper error handling.\"\"\"\n    try:\n       \
    \ response = requests.get(url, headers=headers, timeout=30)\n        response.raise_for_status()\n\
    \n        return response.json()\n\n    except requests.exceptions.Timeout:\n\
    \        print(\"Request timed out\")\n        return None\n\n    except requests.exceptions.HTTPError\
    \ as e:\n        if e.response.status_code == 401:\n            print(\"Invalid\
    \ token\")\n        elif e.response.status_code == 404:\n            print(\"\
    Resource not found\")\n        else:\n            print(f\"HTTP error: {e}\")\n\
    \        return None\n\n    except Exception as e:\n        print(f\"Unexpected\
    \ error: {e}\")\n        return None\n```\n\n\n## 3. Wrong HTTP Method for Async\
    \ Endpoints\n\n**Problem**: Using GET instead of POST for async export operations\n\
    \n**Solution**: Always use POST for /export-async endpoints:\n\n```python\n# ❌\
    \ WRONG - This will return 405 Method Not Allowed\nresponse = requests.get(\n\
    \    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\",\n   \
    \ headers={\"X-StorageApi-Token\": token}\n)\n\n# ✅ CORRECT - Use POST to initiate\
    \ async jobs\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n    headers={\"X-StorageApi-Token\": token}\n)\n```\n\n**Why**: The `/export-async`\
    \ endpoint creates a new export job, which is a write operation requiring POST.\
    \ The API will reject GET requests.\n\n\n\n## 6. Incremental Loads Without Primary\
    \ Key\n\n**Problem**: Attempting incremental load on table without primary key\n\
    \n**Solution**: Always set primary key before using incremental mode:\n\n```python\n\
    # ❌ WRONG - This will fail if table has no primary key\nresponse = requests.post(\n\
    \    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\",\n   \
    \ headers={\"X-StorageApi-Token\": token},\n    params={\n        \"incremental\"\
    : \"1\",\n        \"dataString\": csv_data\n    }\n)\n\n# ✅ CORRECT - Set primary\
    \ key first\n# Option 1: Set when creating table\nresponse = requests.post(\n\
    \    f\"https://{stack_url}/v2/storage/buckets/in.c-main/tables-async\",\n   \
    \ headers={\"X-StorageApi-Token\": token},\n    params={\n        \"name\": \"\
    my_table\",\n        \"primaryKey\": \"id\",  # Set primary key at creation\n\
    \        \"dataString\": csv_data\n    }\n)\n\n# Option 2: Set on existing table\n\
    response = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}\"\
    ,\n    headers={\"X-StorageApi-Token\": token},\n    json={\"primaryKey\": [\"\
    id\"]}\n)\nresponse.raise_for_status()\n\n# Now incremental loads will work\n\
    response = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\"\
    ,\n    headers={\"X-StorageApi-Token\": token},\n    params={\n        \"incremental\"\
    : \"1\",\n        \"dataString\": new_data\n    }\n)\n```\n\n**Why**: Incremental\
    \ mode needs primary key to identify which rows to update vs insert.\n\n## 7.\
    \ Using Wrong Endpoint for Table Import\n\n**Problem**: Confusing table creation\
    \ endpoint with table import endpoint\n\n**Solution**: Use correct endpoint based\
    \ on operation:\n\n```python\n# ❌ WRONG - Using creation endpoint for existing\
    \ table\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/buckets/in.c-main/tables-async\"\
    ,\n    headers={\"X-StorageApi-Token\": token},\n    params={\n        \"name\"\
    : \"existing_table\",  # This creates NEW table or fails\n        \"dataString\"\
    : csv_data\n    }\n)\n\n# ✅ CORRECT - Use import endpoint for existing table\n\
    response = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\"\
    ,\n    headers={\"X-StorageApi-Token\": token},\n    params={\"dataString\": csv_data}\n\
    )\n\n# ✅ CORRECT - Use creation endpoint only for NEW tables\nresponse = requests.post(\n\
    \    f\"https://{stack_url}/v2/storage/buckets/in.c-main/tables-async\",\n   \
    \ headers={\"X-StorageApi-Token\": token},\n    params={\n        \"name\": \"\
    new_table\",\n        \"dataString\": csv_data\n    }\n)\n```\n\n**Rule of thumb**:\n\
    - Creating new table: `/buckets/{bucket}/tables-async`\n- Importing to existing\
    \ table: `/tables/{table_id}/import-async`\n\n## 8. Confusing Workspace Context\
    \ with Project Context\n\n**Problem**: Using workspace IDs in Storage API calls\
    \ or Storage API table names in workspace SQL\n\n**Solution**: Understand the\
    \ context boundary:\n\n```python\n# ❌ WRONG - Using workspace-style table reference\
    \ in Storage API\nimport requests\nproject_id = os.environ['KBC_PROJECT_ID']\n\
    table_ref = f'\"{project_id}\".\"in.c-main\".\"customers\"'  # Snowflake format\n\
    \nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_ref}/export-async\"\
    ,  # FAILS\n    headers={\"X-StorageApi-Token\": token}\n)\n\n# ✅ CORRECT - Storage\
    \ API uses bucket.table format (no project ID)\ntable_id = \"in.c-main.customers\"\
    \  # Storage API format\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n    headers={\"X-StorageApi-Token\": token}\n)\n\n# ❌ WRONG - Using Storage\
    \ API format in workspace SQL\nimport streamlit as st\nconn = st.connection('snowflake',\
    \ type='snowflake')\ntable_id = \"in.c-main.customers\"  # Storage API format\n\
    \nquery = f\"SELECT * FROM {table_id}\"  # FAILS - not valid SQL\ndf = conn.query(query)\n\
    \n# ✅ CORRECT - Workspace SQL requires fully qualified names\nproject_id = os.environ['KBC_PROJECT_ID']\n\
    table_ref = f'\"{project_id}\".\"in.c-main\".\"customers\"'  # Snowflake format\n\
    \nquery = f\"SELECT * FROM {table_ref}\"\ndf = conn.query(query)\n```\n\n**Rule\
    \ of thumb**:\n- **Storage API** (REST endpoints): Use `bucket.table` format,\
    \ no project ID\n- **Workspace** (SQL queries): Use `\"PROJECT_ID\".\"bucket\"\
    .\"table\"` format\n\n**Why the difference?**\n\n- Storage API operates at **project\
    \ level** - it knows your project from the token\n- Workspace operates at **database\
    \ level** - PROJECT_ID is the database name in Snowflake\n\n### Context Detection\
    \ Pattern\n\n```python\ndef get_table_reference(bucket, table):\n    \"\"\"Get\
    \ correct table reference for current context.\"\"\"\n    if 'KBC_PROJECT_ID'\
    \ in os.environ:\n        # Workspace context - return Snowflake-qualified name\n\
    \        project_id = os.environ['KBC_PROJECT_ID']\n        return f'\"{project_id}\"\
    .\"{bucket}\".\"{table}\"'\n    else:\n        # Storage API context - return\
    \ API format\n        return f\"{bucket}.{table}\"\n\n# Usage\ntable_ref = get_table_reference('in.c-main',\
    \ 'customers')\n\nif 'KBC_PROJECT_ID' in os.environ:\n    # Workspace: Use in\
    \ SQL\n    query = f\"SELECT * FROM {table_ref}\"\n    df = conn.query(query)\n\
    else:\n    # Storage API: Use in endpoint\n    response = requests.post(\n   \
    \     f\"https://{stack_url}/v2/storage/tables/{table_ref}/export-async\",\n \
    \       headers={\"X-StorageApi-Token\": token}\n    )\n```\n\n**Common Error\
    \ Messages**:\n\n- `Table 'in.c-main.customers' does not exist` (in workspace)\
    \ → Use quoted, qualified name\n- `Invalid table ID` (in Storage API) → Remove\
    \ quotes and project ID\n- `SQL compilation error` (in workspace) → Missing quotes\
    \ or project ID\n\n\n## 9. Incorrect Pagination Usage\n\n**Problem**: Using data-preview\
    \ pagination for large table exports or not handling pagination in list endpoints\n\
    \n**Solution**: Choose the right pagination strategy for your use case:\n\n```python\n\
    # ❌ WRONG - Using data-preview for large tables (limited to 1000 rows)\ndef export_large_table_wrong(table_id):\n\
    \    offset = 0\n    limit = 1000\n    all_data = []\n    \n    while True:\n\
    \        response = requests.get(\n            f\"https://{stack_url}/v2/storage/tables/{table_id}/data-preview\"\
    ,\n            headers={\"X-StorageApi-Token\": token},\n            params={\"\
    limit\": limit, \"offset\": offset}\n        )\n        data = response.json()\n\
    \        if not data:\n            break\n        all_data.extend(data)\n    \
    \    offset += limit\n    \n    return all_data  # Will never get more than 1000\
    \ rows!\n\n# ✅ CORRECT - Use async export for complete table data\ndef export_large_table_correct(table_id):\n\
    \    response = requests.post(\n        f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n        headers={\"X-StorageApi-Token\": token}\n    )\n    response.raise_for_status()\n\
    \    job_id = response.json()[\"id\"]\n    \n    # Poll and download (see full\
    \ example in Storage API docs)\n    # Returns complete dataset regardless of size\n\
    \    return wait_for_export_job(job_id)\n\n# ❌ WRONG - Not handling pagination\
    \ in list endpoints\ndef get_all_tables_wrong():\n    response = requests.get(\n\
    \        f\"https://{stack_url}/v2/storage/tables\",\n        headers={\"X-StorageApi-Token\"\
    : token}\n    )\n    return response.json()  # Only returns first page!\n\n# ✅\
    \ CORRECT - Paginate through all results\ndef get_all_tables_correct():\n    all_tables\
    \ = []\n    offset = 0\n    limit = 100\n    \n    while True:\n        response\
    \ = requests.get(\n            f\"https://{stack_url}/v2/storage/tables\",\n \
    \           headers={\"X-StorageApi-Token\": token},\n            params={\"limit\"\
    : limit, \"offset\": offset}\n        )\n        response.raise_for_status()\n\
    \        tables = response.json()\n        \n        if not tables:\n        \
    \    break\n        \n        all_tables.extend(tables)\n        \n        if\
    \ len(tables) < limit:\n            break\n        \n        offset += limit\n\
    \    \n    return all_tables\n```\n\n**Rule of thumb**:\n- **Small preview (<100\
    \ rows)**: Use `data-preview` without pagination\n- **Browse/list resources**:\
    \ Use pagination with `limit`/`offset`\n- **Full table export**: Use `export-async`\
    \ (no manual pagination needed)\n\n**Why**: Different endpoints have different\
    \ pagination capabilities and limits. Using the wrong approach can result in incomplete\
    \ data or unnecessary complexity.\n"
  format: markdown
- source: 04-component-development.md
  content: "# Component Development\n\n## Overview\n\nKeboola components are Docker\
    \ containers that follow the Common Interface specification for processing data.\
    \ They communicate with Keboola exclusively through the filesystem at `/data`.\n\
    \n## Component Types\n\n- **Extractors**: Pull data from external sources\n- **Writers**:\
    \ Send data to external destinations\n- **Applications**: Process or transform\
    \ data\n\nNote: Don't include component type names ('extractor', 'writer', 'application')\
    \ in the component name itself.\n\n## Project Structure\n\n```\nmy-component/\n\
    ├── src/\n│   ├── component.py          # Main logic with run() function\n│  \
    \ └── configuration.py      # Configuration validation\n├── component_config/\n\
    │   ├── component_config.json           # Configuration schema\n│   ├── component_long_description.md\
    \   # Detailed docs\n│   └── component_short_description.md  # Brief description\n\
    ├── tests/\n│   └── test_component.py     # Unit tests\n├── data/            \
    \         # Local data folder (gitignored)\n│   ├── config.json           # Example\
    \ config for local testing\n│   ├── in/                   # Input tables and files\n\
    │   └── out/                  # Output tables and files\n├── .github/workflows/\n\
    │   └── push.yml              # CI/CD deployment\n├── Dockerfile             \
    \   # Container definition\n└── pyproject.toml            # Python dependencies\n\
    ```\n\n## Data Folder Contract\n\nComponents communicate with Keboola through\
    \ the `/data` directory:\n\n**INPUT** (read-only):\n- `config.json` - Component\
    \ configuration from UI\n- `in/tables/*.csv` - Input tables with `.manifest` files\n\
    - `in/files/*` - Input files\n- `in/state.json` - Previous run state (for incremental\
    \ processing)\n\n**OUTPUT** (write):\n- `out/tables/*.csv` - Output tables with\
    \ `.manifest` files\n- `out/files/*` - Output files\n- `out/state.json` - New\
    \ state for next run\n\n**IMPORTANT**: The Keboola platform automatically creates\
    \ all data directories (`data/in/`, `data/out/tables/`, `data/out/files/`, etc.).\
    \ You **never** need to call `mkdir()` or create these directories manually in\
    \ your component code.\n\n## Basic Component Implementation\n\n```python\nfrom\
    \ keboola.component import CommonInterface\nimport logging\nimport sys\nimport\
    \ traceback\n\nREQUIRED_PARAMETERS = ['api_key', 'endpoint']\n\nclass Component(CommonInterface):\n\
    \    def __init__(self):\n        super().__init__()\n\n    def run(self):\n \
    \       try:\n            # 1. Validate configuration\n            self.validate_configuration(REQUIRED_PARAMETERS)\n\
    \            params = self.configuration.parameters\n\n            # 2. Load state\
    \ for incremental processing\n            state = self.get_state_file()\n    \
    \        last_timestamp = state.get('last_timestamp')\n\n            # 3. Process\
    \ input tables\n            input_tables = self.get_input_tables_definitions()\n\
    \            for table in input_tables:\n                self._process_table(table)\n\
    \n            # 4. Create output tables with manifests\n            self._create_output_tables()\n\
    \n            # 5. Save state for next run\n            self.write_state_file({\n\
    \                'last_timestamp': current_timestamp\n            })\n\n     \
    \   except ValueError as err:\n            # User errors (configuration/input\
    \ issues)\n            logging.error(str(err))\n            print(err, file=sys.stderr)\n\
    \            sys.exit(1)\n        except Exception as err:\n            # System\
    \ errors (unhandled exceptions)\n            logging.exception(\"Unhandled error\
    \ occurred\")\n            traceback.print_exc(file=sys.stderr)\n            sys.exit(2)\n\
    \nif __name__ == '__main__':\n    try:\n        comp = Component()\n        comp.run()\n\
    \    except Exception as e:\n        logging.exception(\"Component execution failed\"\
    )\n        sys.exit(2)\n```\n\n## Configuration Schema\n\nDefine configuration\
    \ parameters in `component_config/component_config.json`:\n\n```json\n{\n  \"\
    type\": \"object\",\n  \"title\": \"Configuration\",\n  \"required\": [\"api_key\"\
    , \"endpoint\"],\n  \"properties\": {\n    \"#api_key\": {\n      \"type\": \"\
    string\",\n      \"title\": \"API Key\",\n      \"description\": \"Your API authentication\
    \ token\",\n      \"format\": \"password\"\n    },\n    \"endpoint\": {\n    \
    \  \"type\": \"string\",\n      \"title\": \"API Endpoint\",\n      \"description\"\
    : \"Base URL for the API\"\n    },\n    \"incremental\": {\n      \"type\": \"\
    boolean\",\n      \"title\": \"Incremental Load\",\n      \"description\": \"\
    Only fetch data since last run\",\n      \"default\": false\n    }\n  }\n}\n```\n\
    \n### Sensitive Data Handling\n\nPrefix parameter names with `#` to enable automatic\
    \ hashing:\n```json\n{\n  \"#password\": {\n    \"type\": \"string\",\n    \"\
    title\": \"Password\",\n    \"format\": \"password\"\n  }\n}\n```\n\n### UI Elements\n\
    \n**Code Editor** (ACE editor for multi-line input):\n```json\n{\n  \"query\"\
    : {\n    \"type\": \"string\",\n    \"title\": \"SQL Query\",\n    \"format\"\
    : \"textarea\",\n    \"options\": {\n      \"ace\": {\n        \"mode\": \"sql\"\
    \n      }\n    }\n  }\n}\n```\n\n**Test Connection Button**:\n```json\n{\n  \"\
    test_connection\": {\n    \"type\": \"button\",\n    \"title\": \"Test Connection\"\
    ,\n    \"options\": {\n      \"syncAction\": \"test-connection\"\n    }\n  }\n\
    }\n```\n\n## CSV Processing\n\nAlways process CSV files efficiently using generators:\n\
    \n```python\nimport csv\n\ndef process_input_table(table_def):\n    with open(table_def.full_path,\
    \ 'r', encoding='utf-8') as in_file:\n        # Handle null characters with generator\n\
    \        lazy_lines = (line.replace('\\0', '') for line in in_file)\n        reader\
    \ = csv.DictReader(lazy_lines, dialect='kbc')\n\n        for row in reader:\n\
    \            # Process row by row for memory efficiency\n            yield process_row(row)\n\
    ```\n\n## Creating Output Tables\n\nCreate output tables with proper schema definitions:\n\
    \n```python\nfrom collections import OrderedDict\nfrom keboola.component.dao import\
    \ ColumnDefinition, BaseType\n\n# Define schema\nschema = OrderedDict({\n    \"\
    id\": ColumnDefinition(\n        data_types=BaseType.integer(),\n        primary_key=True\n\
    \    ),\n    \"name\": ColumnDefinition(),\n    \"value\": ColumnDefinition(\n\
    \        data_types=BaseType.numeric(length=\"10,2\")\n    )\n})\n\n# Create table\
    \ definition\nout_table = self.create_out_table_definition(\n    name=\"results.csv\"\
    ,\n    destination=\"out.c-data.results\",\n    schema=schema,\n    incremental=True\n\
    )\n\n# Write data\nimport csv\nwith open(out_table.full_path, 'w', newline='',\
    \ encoding='utf-8') as f:\n    writer = csv.DictWriter(f, fieldnames=out_table.column_names)\n\
    \    writer.writeheader()\n    for row in data:\n        writer.writerow(row)\n\
    \n# Write manifest\nself.write_manifest(out_table)\n```\n\n## State Management\
    \ for Incremental Processing\n\nImplement proper state handling for incremental\
    \ loads:\n\n```python\ndef run_incremental(self):\n    # Load previous state\n\
    \    state = self.get_state_file()\n    last_timestamp = state.get('last_timestamp',\
    \ '1970-01-01T00:00:00Z')\n\n    # Fetch only new data since last_timestamp\n\
    \    new_data = self._fetch_data_since(last_timestamp)\n\n    # Process and save\
    \ data\n    self._process_data(new_data)\n\n    # Update state with current timestamp\n\
    \    from datetime import datetime, timezone\n    current_timestamp = datetime.now(timezone.utc).isoformat()\n\
    \    self.write_state_file({\n        'last_timestamp': current_timestamp,\n \
    \       'records_processed': len(new_data)\n    })\n```\n\n## Error Handling\n\
    \nFollow Keboola's error handling conventions:\n\n- **Exit code 1**: User errors\
    \ (configuration problems, invalid inputs)\n- **Exit code 2**: System errors (unhandled\
    \ exceptions, application errors)\n\n```python\ntry:\n    # Component logic\n\
    \    validate_inputs(params)\n    result = perform_operation()\n\nexcept ValueError\
    \ as err:\n    # User-fixable errors\n    logging.error(f\"Configuration error:\
    \ {err}\")\n    print(err, file=sys.stderr)\n    sys.exit(1)\n\nexcept requests.HTTPError\
    \ as err:\n    # API errors\n    logging.error(f\"API request failed: {err}\"\
    )\n    print(f\"Failed to connect to API: {err.response.status_code}\", file=sys.stderr)\n\
    \    sys.exit(1)\n\nexcept Exception as err:\n    # Unhandled exceptions\n   \
    \ logging.exception(\"Unhandled error in component execution\")\n    traceback.print_exc(file=sys.stderr)\n\
    \    sys.exit(2)\n```\n\n## Local Development\n\n### Running Locally\n\n```bash\n\
    # Set up virtual environment\npython -m venv .venv\nsource .venv/bin/activate\n\
    pip install -e .\n\n# Set data directory environment variable\nexport KBC_DATADIR=./data\n\
    \n# Run component\npython src/component.py\n```\n\n### Using Docker\n\n```bash\n\
    # Build image\ndocker build -t my-component:latest .\n\n# Run with mounted data\
    \ folder\ndocker run --rm \\\n  -v $(pwd)/data:/data \\\n  -e KBC_DATADIR=/data\
    \ \\\n  my-component:latest\n```\n\n### Prepare Test Data\n\nCreate `data/config.json`\
    \ with example parameters:\n\n```json\n{\n  \"parameters\": {\n    \"api_key\"\
    : \"your_key_here\",\n    \"#password\": \"test_password\",\n    \"from_date\"\
    : \"2024-01-01\",\n    \"incremental\": false\n  }\n}\n```\n\nCreate sample input\
    \ tables:\n\n```bash\nmkdir -p data/in/tables\ncat > data/in/tables/input.csv\
    \ <<EOF\nid,name,email\n1,John Doe,john@example.com\n2,Jane Smith,jane@example.com\n\
    EOF\n```\n\n## Best Practices\n\n### DO:\n\n- Use `CommonInterface` class for\
    \ all Keboola interactions\n- Validate configuration early with `validate_configuration()`\n\
    - Process CSV files with generators for memory efficiency\n- Always specify `encoding='utf-8'`\
    \ for file operations\n- Use proper exit codes (1 for user errors, 2 for system\
    \ errors)\n- Define explicit schemas for output tables\n- Implement state management\
    \ for incremental processing\n- Write comprehensive tests\n- Quote all SQL identifiers\
    \ (`\"column_name\"`, not `column_name`)\n\n### DON'T:\n\n- Load entire CSV files\
    \ into memory\n- Hard-code configuration values\n- Skip configuration validation\n\
    - Forget to write manifests for output tables\n- Skip state file management for\
    \ incremental loads\n- Forget to handle null characters in CSV files\n- Call `mkdir()`\
    \ for platform-managed directories (in/, out/, tables/, files/)\n\n## Dockerfile\n\
    \n```dockerfile\nFROM python:3.11-alpine\n\n# Install dependencies\nWORKDIR /code\n\
    COPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n\
    # Copy component code\nCOPY src/ /code/src/\n\n# Set entrypoint with unbuffered\
    \ output\nENTRYPOINT [\"python\", \"-u\", \"/code/src/component.py\"]\n```\n\n\
    ## CI/CD Deployment\n\n### GitHub Actions Workflow\n\n```yaml\n# .github/workflows/push.yml\n\
    name: Build and Deploy\n\non:\n  push:\n    tags:\n      - 'v*'\n\njobs:\n  build:\n\
    \    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n\
    \      - name: Build Docker image\n        run: docker build -t my-component:${{\
    \ github.ref_name }} .\n\n      - name: Run tests\n        run: docker-compose\
    \ run --rm test\n\n      - name: Deploy to Keboola\n        env:\n          KBC_DEVELOPERPORTAL_USERNAME:\
    \ ${{ secrets.KBC_USERNAME }}\n          KBC_DEVELOPERPORTAL_PASSWORD: ${{ secrets.KBC_PASSWORD\
    \ }}\n        run: ./deploy.sh\n```\n\n### Version Management\n\nFollow semantic\
    \ versioning:\n\n- **v1.0.0** - Major release (breaking changes)\n- **v1.1.0**\
    \ - Minor release (new features)\n- **v1.0.1** - Patch release (bug fixes)\n\n\
    ```bash\n# Tag and push\ngit tag -a v1.0.0 -m \"Release version 1.0.0\"\ngit push\
    \ origin v1.0.0\n```\n\n## Testing\n\n### Unit Tests\n\n```python\nimport unittest\n\
    from src.component import Component\n\nclass TestComponent(unittest.TestCase):\n\
    \    def test_configuration_validation(self):\n        \"\"\"Test that required\
    \ parameters are validated.\"\"\"\n        # Test implementation\n\n    def test_csv_processing(self):\n\
    \        \"\"\"Test CSV reading and writing with proper encoding.\"\"\"\n    \
    \    # Test implementation\n\n    def test_state_management(self):\n        \"\
    \"\"Test state file persistence.\"\"\"\n        # Test implementation\n```\n\n\
    Run tests:\n\n```bash\n# Using unittest\npython -m unittest discover -s tests\n\
    \n# Using pytest\npytest tests/ -v --cov=src\n```\n\n## Code Quality\n\nUse Ruff\
    \ for code formatting and linting:\n\n```bash\n# Format code\nruff format .\n\n\
    # Lint and auto-fix issues\nruff check --fix .\n```\n\n## Resources\n\n- [Keboola\
    \ Developer Docs](https://developers.keboola.com/)\n- [Python Component Library](https://github.com/keboola/python-component)\n\
    - [Component Tutorial](https://developers.keboola.com/extend/component/tutorial/)\n\
    - [Cookiecutter Template](https://github.com/keboola/cookiecutter-python-component)\n"
  format: markdown
- source: 05-dataapp-development.md
  content: "# Data App Development\n\n## Overview\n\nKeboola Data Apps are Streamlit\
    \ applications that run directly in the Keboola platform, providing interactive\
    \ dashboards and analytics tools. They connect to Keboola Storage and can query\
    \ data from workspace tables.\n\n## Key Concepts\n\n### What are Data Apps?\n\n\
    Data Apps are containerized Streamlit applications that:\n- Run inside Keboola's\
    \ infrastructure\n- Have direct access to project data via workspace\n- Support\
    \ interactive filtering, visualization, and exploration\n- Can be shared with\
    \ team members\n- Auto-scale based on usage\n\n### Architecture Pattern: SQL-First\n\
    \n**Core Principle**: Push computation to the database, never load large datasets\
    \ into Python.\n\nWhy?\n- Keboola workspaces (Snowflake, Redshift, BigQuery) are\
    \ optimized for queries\n- Loading data into Streamlit doesn't scale\n- SQL aggregation\
    \ is 10-100x faster than pandas\n\n## Project Structure\n\n```\nmy-dataapp/\n\
    ├── streamlit_app.py          # Main app entry point with sidebar\n├── pages/\n\
    │   ├── 01_Overview.py        # First page\n│   ├── 02_Analytics.py       # Second\
    \ page\n│   └── 03_Details.py         # Third page\n├── utils/\n│   ├── data_loader.py\
    \        # Centralized data access\n│   └── config.py             # Environment\
    \ configuration\n├── requirements.txt          # Python dependencies\n└── README.md\
    \                 # Documentation\n```\n\n## Environment Setup\n\n### Local Development\n\
    \nData apps must work in two environments with **different contexts**:\n\n1. **Local\
    \ Development (Storage API / Project Context)**: \n   - Uses Storage API token\
    \ for authentication\n   - References tables as `in.c-bucket.table`\n   - Exports\
    \ data via REST API\n   - No workspace ID involved\n\n2. **Production (Workspace\
    \ Context)**: \n   - Uses workspace database connection\n   - References tables\
    \ as `\"PROJECT_ID\".\"in.c-bucket\".\"table\"`\n   - Queries data via SQL\n \
    \  - Requires workspace environment variables\n\n**Why Two Contexts?**\n\nIn production,\
    \ Data Apps run inside a **Keboola workspace** (Snowflake/Redshift instance) where\
    \ your project data is mirrored. This provides:\n- Direct SQL access (fast queries)\n\
    - No API rate limits\n- Native database features\n\nDuring local development,\
    \ you don't have workspace access, so you use the **Storage API** (REST) to export\
    \ data.\n\n**Environment Variables by Context**:\n\n```python\n# WORKSPACE CONTEXT\
    \ (Production)\n# Automatically set by Keboola platform:\nKBC_PROJECT_ID=6789\
    \           # Your project ID (used in table references)\nKBC_BUCKET_ID=in.c-main\
    \       # Default bucket for app\nKBC_TABLE_NAME=customers      # Default table\
    \ for app\n\n# STORAGE API CONTEXT (Local)\n# You must set manually:\nKEBOOLA_TOKEN=your-token\
    \                        # Storage API token\nKEBOOLA_STACK_URL=connection.keboola.com\
    \       # Your stack URL\n```\n\n```python\n# utils/config.py\nimport os\nimport\
    \ streamlit as st\n\ndef get_connection_mode():\n    \"\"\"Detect if running locally\
    \ or in Keboola.\"\"\"\n    return 'workspace' if 'KBC_PROJECT_ID' in os.environ\
    \ else 'local'\n\ndef get_storage_token():\n    \"\"\"Get Storage API token from\
    \ environment.\"\"\"\n    return os.environ.get('KEBOOLA_TOKEN')\n\ndef get_stack_url():\n\
    \    \"\"\"Get Keboola stack URL.\"\"\"\n    return os.environ.get('KEBOOLA_STACK_URL',\
    \ 'connection.keboola.com')\n```\n\n### Connection Setup\n\n```python\n# utils/data_loader.py\n\
    import os\nimport streamlit as st\nfrom utils.config import get_connection_mode\n\
    \n@st.cache_resource\ndef get_connection():\n    \"\"\"Get database connection\
    \ based on environment.\"\"\"\n    mode = get_connection_mode()\n\n    if mode\
    \ == 'workspace':\n        # Running in Keboola - use workspace connection\n \
    \       return st.connection('snowflake', type='snowflake')\n    else:\n     \
    \   # Local development - use Storage API\n        return None  # Implement Storage\
    \ API wrapper\n\ndef get_table_name():\n    \"\"\"Get fully qualified table name.\"\
    \"\"\n    mode = get_connection_mode()\n\n    if mode == 'workspace':\n      \
    \  # In workspace: database.schema.table\n        return f'\"{os.environ[\"KBC_PROJECT_ID\"\
    ]}\".\"{os.environ[\"KBC_BUCKET_ID\"]}\".\"{os.environ[\"KBC_TABLE_NAME\"]}\"\
    '\n    else:\n        # Local: bucket.table\n        return 'in.c-analysis.usage_data'\n\
    ```\n\n## SQL-First Design Pattern\n\n### Good Pattern: Aggregate in Database\n\
    \n```python\n@st.cache_data(ttl=300)\ndef get_summary_metrics(where_clause: str\
    \ = \"\"):\n    query = f'''\n        SELECT\n            COUNT(*) as total_count,\n\
    \            COUNT(DISTINCT \"user_id\") as unique_users,\n            AVG(\"\
    session_duration\") as avg_duration,\n            SUM(\"revenue\") as total_revenue\n\
    \        FROM {get_table_name()}\n        WHERE \"date\" >= CURRENT_DATE - INTERVAL\
    \ '90 days'\n            {f\"AND {where_clause}\" if where_clause else \"\"}\n\
    \    '''\n    return execute_query(query)\n```\n\n### Bad Pattern: Load All Data\n\
    \n```python\n# DON'T DO THIS\ndf = execute_query(f\"SELECT * FROM {get_table_name()}\"\
    )\nresult = df.groupby('category').agg({'value': 'mean'})\n```\n\n## Global Filter\
    \ Pattern\n\nGlobal filters allow users to filter all pages from one control in\
    \ the sidebar.\n\n### Step 1: Create Filter Function\n\n```python\n# utils/data_loader.py\n\
    import streamlit as st\n\ndef get_user_type_filter_clause():\n    \"\"\"Get SQL\
    \ WHERE clause for user type filter.\"\"\"\n    # Initialize session state with\
    \ default\n    if 'user_filter' not in st.session_state:\n        st.session_state.user_filter\
    \ = 'external'\n\n    # Return appropriate SQL condition\n    if st.session_state.user_filter\
    \ == 'external':\n        return '\"user_type\" = \\'External User\\''\n    elif\
    \ st.session_state.user_filter == 'internal':\n        return '\"user_type\" =\
    \ \\'Keboola User\\''\n    return ''  # 'all' - no filter\n```\n\n### Step 2:\
    \ Add UI Control\n\n```python\n# streamlit_app.py (sidebar)\nimport streamlit\
    \ as st\nfrom utils.data_loader import get_user_type_filter_clause\n\nst.set_page_config(page_title=\"\
    My Dashboard\", layout=\"wide\")\n\n# Initialize session state\nif 'user_filter'\
    \ not in st.session_state:\n    st.session_state.user_filter = 'external'\n\n\
    # Sidebar filter\nst.sidebar.header(\"Filters\")\nuser_option = st.sidebar.radio(\n\
    \    \"User Type:\",\n    options=['external', 'internal', 'all'],\n    index=['external',\
    \ 'internal', 'all'].index(st.session_state.user_filter)\n)\n\n# Update session\
    \ state and trigger rerun if changed\nif user_option != st.session_state.user_filter:\n\
    \    st.session_state.user_filter = user_option\n    st.rerun()\n```\n\n### Step\
    \ 3: Use Filter in Pages\n\n```python\n# pages/01_Overview.py\nimport streamlit\
    \ as st\nfrom utils.data_loader import get_user_type_filter_clause, execute_query\n\
    \n# Build WHERE clause\nwhere_parts = ['\"status\" = \\'active\\'']  # Base filter\n\
    user_filter = get_user_type_filter_clause()\nif user_filter:\n    where_parts.append(user_filter)\n\
    where_clause = ' AND '.join(where_parts)\n\n# Use in query\n@st.cache_data(ttl=300)\n\
    def get_page_data():\n    query = f'''\n        SELECT \"date\", COUNT(*) as count\n\
    \        FROM {get_table_name()}\n        WHERE {where_clause}\n        GROUP\
    \ BY \"date\"\n        ORDER BY \"date\"\n    '''\n    return execute_query(query)\n\
    \ndf = get_page_data()\nst.line_chart(df, x='date', y='count')\n```\n\n## Query\
    \ Execution\n\n### Basic Query Function\n\n```python\n# utils/data_loader.py\n\
    import streamlit as st\nimport pandas as pd\n\n@st.cache_data(ttl=300)\ndef execute_query(sql:\
    \ str) -> pd.DataFrame:\n    \"\"\"Execute SQL query and return DataFrame.\"\"\
    \"\n    conn = get_connection()\n\n    try:\n        df = conn.query(sql)\n  \
    \      return df\n    except Exception as e:\n        st.error(f\"Query failed:\
    \ {e}\")\n        return pd.DataFrame()\n```\n\n### SQL Best Practices\n\n**Always\
    \ quote identifiers**:\n```sql\n-- CORRECT\nSELECT \"user_id\", \"revenue\" FROM\
    \ \"my_table\"\n\n-- WRONG (fails with reserved keywords or mixed case)\nSELECT\
    \ user_id, revenue FROM my_table\n```\n\n**Use parameterized WHERE clauses**:\n\
    ```python\ndef get_date_filter_clause(start_date, end_date):\n    \"\"\"Generate\
    \ date range filter.\"\"\"\n    return f'\"date\" BETWEEN \\'{start_date}\\' AND\
    \ \\'{end_date}\\''\n```\n\n## Caching Strategy\n\n### Cache Database Connections\n\
    \n```python\n@st.cache_resource\ndef get_connection():\n    \"\"\"Cache connection\
    \ object (doesn't change).\"\"\"\n    return st.connection('snowflake', type='snowflake')\n\
    ```\n\n### Cache Query Results\n\n```python\n@st.cache_data(ttl=300)  # Cache\
    \ for 5 minutes\ndef get_metrics(where_clause: str):\n    \"\"\"Cache query results\
    \ (data can change).\"\"\"\n    query = f\"SELECT COUNT(*) FROM {get_table_name()}\
    \ WHERE {where_clause}\"\n    return execute_query(query)\n```\n\n### TTL Guidelines\n\
    \n- **Static reference data**: `ttl=3600` (1 hour)\n- **Dashboard metrics**: `ttl=300`\
    \ (5 minutes)\n- **Real-time data**: `ttl=60` (1 minute)\n- **User-specific data**:\
    \ No cache or very short TTL\n\n## Session State Management\n\nStreamlit reruns\
    \ the entire script on every interaction. Use session state to persist values.\n\
    \n### Initialize Before Use\n\n```python\n# Always initialize session state before\
    \ creating widgets\nif 'selected_category' not in st.session_state:\n    st.session_state.selected_category\
    \ = 'all'\n\n# Now create widget\ncategory = st.selectbox(\n    \"Category\",\n\
    \    options=['all', 'sales', 'marketing'],\n    index=['all', 'sales', 'marketing'].index(st.session_state.selected_category)\n\
    )\n\n# Update session state if changed\nif category != st.session_state.selected_category:\n\
    \    st.session_state.selected_category = category\n    st.rerun()\n```\n\n##\
    \ Error Handling\n\n### Handle Empty Results\n\n```python\ndf = get_page_data()\n\
    \nif df.empty:\n    st.warning(\"No data available for the selected filters.\"\
    )\nelse:\n    st.line_chart(df, x='date', y='count')\n```\n\n### Catch Query Errors\n\
    \n```python\n@st.cache_data(ttl=300)\ndef execute_query(sql: str):\n    try:\n\
    \        conn = get_connection()\n        return conn.query(sql)\n    except Exception\
    \ as e:\n        st.error(f\"Database query failed: {e}\")\n        return pd.DataFrame()\n\
    ```\n\n## Common Patterns\n\n### Metric Cards\n\n```python\n@st.cache_data(ttl=300)\n\
    def get_kpi_metrics():\n    query = f'''\n        SELECT\n            COUNT(*)\
    \ as total_users,\n            SUM(\"revenue\") as total_revenue,\n          \
    \  AVG(\"session_duration\") as avg_duration\n        FROM {get_table_name()}\n\
    \        WHERE \"date\" >= CURRENT_DATE - INTERVAL '30 days'\n    '''\n    return\
    \ execute_query(query).iloc[0]\n\nmetrics = get_kpi_metrics()\n\ncol1, col2, col3\
    \ = st.columns(3)\ncol1.metric(\"Total Users\", f\"{metrics['total_users']:,}\"\
    )\ncol2.metric(\"Revenue\", f\"${metrics['total_revenue']:,.2f}\")\ncol3.metric(\"\
    Avg Duration\", f\"{metrics['avg_duration']:.1f}s\")\n```\n\n### Date Range Filter\n\
    \n```python\nimport datetime\n\ncol1, col2 = st.columns(2)\nstart_date = col1.date_input(\"\
    Start Date\", datetime.date.today() - datetime.timedelta(days=30))\nend_date =\
    \ col2.date_input(\"End Date\", datetime.date.today())\n\nwhere_clause = f'\"\
    date\" BETWEEN \\'{start_date}\\' AND \\'{end_date}\\''\n```\n\n### Dynamic Dropdown\n\
    \n```python\n@st.cache_data(ttl=3600)\ndef get_categories():\n    query = f'SELECT\
    \ DISTINCT \"category\" FROM {get_table_name()} ORDER BY \"category\"'\n    return\
    \ execute_query(query)['category'].tolist()\n\ncategories = get_categories()\n\
    selected = st.selectbox(\"Category\", options=['All'] + categories)\n```\n\n##\
    \ Variable Naming Conventions\n\n### Avoid Naming Conflicts\n\n**Problem**: Using\
    \ same variable name for SQL clause and UI widget\n```python\n# DON'T DO THIS\n\
    user_filter = get_user_filter_clause()  # SQL string\nuser_filter = st.radio(\"\
    User Type\", ...)  # UI widget - overwrites SQL!\n```\n\n**Solution**: Use descriptive,\
    \ unique names\n```python\n# DO THIS\nuser_filter_sql = get_user_filter_clause()\
    \  # SQL string\nuser_filter_option = st.radio(\"User Type\", ...)  # UI widget\n\
    ```\n\n### Session State Keys\n\nUse consistent, descriptive keys:\n```python\n\
    # Good\nst.session_state.user_type_filter = 'external'\nst.session_state.selected_date_range\
    \ = (start, end)\nst.session_state.page_number = 1\n\n# Bad (ambiguous)\nst.session_state.filter\
    \ = 'external'\nst.session_state.data = (start, end)\nst.session_state.page =\
    \ 1\n```\n\n## Deployment\n\n### Requirements File\n\n```txt\n# requirements.txt\n\
    streamlit>=1.28.0\npandas>=2.0.0\nsnowflake-connector-python>=3.0.0\nplotly>=5.17.0\n\
    ```\n\n### Environment Variables\n\nRequired in Keboola deployment:\n- `KBC_PROJECT_ID`\
    \ - Automatically set by platform\n- `KBC_BUCKET_ID` - Automatically set by platform\n\
    - `KEBOOLA_TOKEN` - Set in Data App configuration\n- `KEBOOLA_STACK_URL` - Set\
    \ in Data App configuration\n\n### Testing Before Deployment\n\n```bash\n# Local\
    \ testing\nexport KEBOOLA_TOKEN=your_token\nexport KEBOOLA_STACK_URL=connection.keboola.com\n\
    streamlit run streamlit_app.py\n```\n\n## Best Practices\n\n### DO:\n\n- Always\
    \ validate data schemas before writing code\n- Push computation to database -\
    \ aggregate in SQL, not Python\n- Use fully qualified table names from `get_table_name()`\n\
    - Quote all identifiers in SQL (`\"column_name\"`)\n- Cache all queries with `@st.cache_data(ttl=300)`\n\
    - Centralize data access in `utils/data_loader.py`\n- Initialize session state\
    \ with defaults before UI controls\n- Use unique, descriptive variable names\n\
    - Test visually before deploying\n- Handle empty DataFrames gracefully\n- Support\
    \ both local and production environments\n\n### DON'T:\n\n- Skip data validation\
    \ - always check schemas first\n- Load large datasets into Python - aggregate\
    \ in database\n- Hardcode table names - use `get_table_name()` function\n- Use\
    \ same variable name twice (SQL clause and UI widget)\n- Forget session state\
    \ initialization before creating widgets\n- Assume columns exist - validate first\n\
    - Use unquoted SQL identifiers\n- Skip error handling for empty query results\n\
    - Deploy without local testing\n\n## Visual Verification Workflow\n\nBefore deploying,\
    \ test your app:\n\n1. **Start local server**: `streamlit run streamlit_app.py`\n\
    2. **Open in browser**: `http://localhost:8501`\n3. **Test all interactions**:\n\
    \   - Click through all pages\n   - Try all filter combinations\n   - Verify metrics\
    \ update correctly\n   - Check for error messages\n4. **Capture screenshots**\
    \ of working features\n5. **Deploy with confidence**\n\n## Common Issues\n\n###\
    \ \"KeyError: 'column_name'\"\n\n**Cause**: Column doesn't exist or wrong name\n\
    **Solution**: Validate schema before querying:\n```python\n# Check available columns\
    \ first\nquery = f'SELECT * FROM {get_table_name()} LIMIT 1'\ndf = execute_query(query)\n\
    print(df.columns)  # See actual column names\n```\n\n### Filter Not Working\n\n\
    **Cause**: Filter SQL not included in WHERE clause\n**Solution**: Always build\
    \ WHERE clause systematically:\n```python\nwhere_parts = []\nif base_filter :=\
    \ get_base_filter():\n    where_parts.append(base_filter)\nif user_filter := get_user_filter_clause():\n\
    \    where_parts.append(user_filter)\nwhere_clause = ' AND '.join(where_parts)\
    \ if where_parts else '1=1'\n```\n\n### Session State Not Persisting\n\n**Cause**:\
    \ Not initializing before widget creation\n**Solution**: Initialize before use:\n\
    ```python\nif 'my_value' not in st.session_state:\n    st.session_state.my_value\
    \ = default_value\n\nwidget = st.text_input(\"Label\", value=st.session_state.my_value)\n\
    ```\n\n## Resources\n\n- [Streamlit Documentation](https://docs.streamlit.io)\n\
    - [Keboola Data Apps Guide](https://developers.keboola.com/extend/data-apps/)\n\
    - [Snowflake SQL Reference](https://docs.snowflake.com/en/sql-reference.html)\n\
    \ndef get_table_name():\n    \"\"\"Get fully qualified table name for current\
    \ context.\n    \n    Returns:\n        Workspace context: '\"PROJECT_ID\".\"\
    BUCKET_ID\".\"TABLE_NAME\"' (quoted, SQL-safe)\n        Storage API context: 'BUCKET_ID.TABLE_NAME'\
    \ (for API endpoints)\n    \n    Context difference:\n    - Workspace uses PROJECT_ID\
    \ as database name (Snowflake schema)\n    - Storage API uses bucket.table format\
    \ (no project ID)\n    \"\"\"\n    mode = get_connection_mode()\n\n    if mode\
    \ == 'workspace':\n        # WORKSPACE CONTEXT: Running in Keboola (has workspace\
    \ access)\n        # Use PROJECT_ID as database qualifier for Snowflake queries\n\
    \        project_id = os.environ['KBC_PROJECT_ID']  # e.g., \"6789\"\n       \
    \ bucket = os.environ.get('KBC_BUCKET_ID', 'in.c-analysis')  # e.g., \"in.c-main\"\
    \n        table = os.environ.get('KBC_TABLE_NAME', 'usage_data')  # e.g., \"customers\"\
    \n        \n        # Return: \"6789\".\"in.c-main\".\"customers\"\n        return\
    \ f'\"{project_id}\".\"{bucket}\".\"{table}\"'\n    else:\n        # STORAGE API\
    \ CONTEXT: Running locally (no workspace)\n        # Use bucket.table format for\
    \ Storage API endpoints\n        bucket = 'in.c-analysis'\n        table = 'usage_data'\n\
    \        \n        # Return: in.c-analysis.usage_data\n        return f'{bucket}.{table}'\n"
  format: markdown
