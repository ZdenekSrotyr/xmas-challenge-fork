name: keboola-core
version: 1.0.0
description: Keboola platform knowledge for Gemini
metadata:
  generated_at: '2025-12-16T14:06:11.530217'
  source_path: docs/keboola
  generator: gemini_generator.py v1.0
  poc_notice: This is a POC. Not production-ready.
knowledge_base:
- source: 01-core-concepts.md
  content: "# Core Concepts\n\n## Overview\n\nKeboola is a cloud-based data platform\
    \ that enables you to extract, transform, and load data from various sources.\n\
    \n## Key Concepts\n\n### Project\nA project is the top-level container in Keboola.\
    \ All your configurations, data, and orchestrations belong to a project.\n\n###\
    \ Storage\nKeboola Storage is where your data lives. It consists of:\n- **Buckets**:\
    \ Logical containers for tables\n- **Tables**: The actual data\n- **Files**: Temporary\
    \ file storage\n\n### Components\nComponents are the building blocks:\n- **Extractors**:\
    \ Pull data from external sources\n- **Transformations**: Process and modify data\
    \ (see [Custom Python Components](04-custom-components.md))\n- **Writers**: Send\
    \ data to external destinations\n\n#### Two Approaches to Working with Keboola\n\
    \n1. **Direct Storage API Calls** (covered in [Storage API](02-storage-api.md))\n\
    \   - Use REST API directly with HTTP requests\n   - Good for one-off scripts,\
    \ external orchestration, quick prototypes\n   - Requires manual handling of I/O,\
    \ authentication, job polling\n\n2. **Custom Python Components** (covered in [Custom\
    \ Components](04-custom-components.md))\n   - Use `keboola.component` library\
    \ for standardized structure\n   - Recommended for production transformations\
    \ and reusable components\n   - Automatic I/O mapping, configuration UI, state\
    \ management, deployment integration\n\n## Authentication\n\nUse Storage API tokens\
    \ for authentication:\n\n```python\nimport os\nimport requests\n\nSTORAGE_TOKEN\
    \ = os.environ[\"KEBOOLA_TOKEN\"]\nSTACK_URL = os.environ.get(\"KEBOOLA_STACK_URL\"\
    , \"connection.keboola.com\")\n\nheaders = {\n    \"X-StorageApi-Token\": STORAGE_TOKEN,\n\
    \    \"Content-Type\": \"application/json\"\n}\n\nresponse = requests.get(\n \
    \   f\"https://{STACK_URL}/v2/storage/tables\",\n    headers=headers\n)\n```\n\
    \n## Regional Stacks\n\nKeboola operates multiple regional stacks:\n- **US**:\
    \ connection.keboola.com\n- **EU**: connection.eu-central-1.keboola.com\n- **Azure**:\
    \ connection.north-europe.azure.keboola.com\n\nAlways use your project's stack\
    \ URL, not a hardcoded one.\n"
  format: markdown
- source: 02-storage-api.md
  content: "# Storage API\n\n> **Note**: This guide covers direct Storage API usage\
    \ via HTTP requests. For production transformations, consider using [Custom Python\
    \ Components](04-custom-components.md) which provide a more robust framework with\
    \ automatic I/O handling, configuration management, and deployment integration.\n\
    \n## Reading Tables\n\n### List All Tables\n\n```python\nimport requests\nimport\
    \ os\n\nstack_url = os.environ.get(\"KEBOOLA_STACK_URL\", \"connection.keboola.com\"\
    )\ntoken = os.environ[\"KEBOOLA_TOKEN\"]\n\nresponse = requests.get(\n    f\"\
    https://{stack_url}/v2/storage/tables\",\n    headers={\"X-StorageApi-Token\"\
    : token}\n)\n\ntables = response.json()\nfor table in tables:\n    print(f\"{table['id']}:\
    \ {table['rowsCount']} rows\")\n```\n\n### Export Table Data\n\n```python\nimport\
    \ time\n\n# Start async export job (NOTE: POST method required)\nresponse = requests.post(\n\
    \    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\",\n   \
    \ headers={\"X-StorageApi-Token\": token}\n)\nresponse.raise_for_status()\n\n\
    job_id = response.json()[\"id\"]\n\n# Poll for completion with timeout\ntimeout\
    \ = 300  # 5 minutes\nstart_time = time.time()\n\nwhile time.time() - start_time\
    \ < timeout:\n    job_response = requests.get(\n        f\"https://{stack_url}/v2/storage/jobs/{job_id}\"\
    ,\n        headers={\"X-StorageApi-Token\": token}\n    )\n    job_response.raise_for_status()\n\
    \n    job = job_response.json()\n    \n    if job[\"status\"] == \"success\":\n\
    \        # Download and save data to file\n        file_url = job[\"results\"\
    ][\"file\"][\"url\"]\n        data_response = requests.get(file_url)\n       \
    \ \n        with open(\"table_data.csv\", \"wb\") as f:\n            f.write(data_response.content)\n\
    \        \n        print(f\"Table exported to table_data.csv\")\n        break\n\
    \    \n    elif job[\"status\"] in [\"error\", \"cancelled\", \"terminated\"]:\n\
    \        error_msg = job.get(\"error\", {}).get(\"message\", \"Unknown error\"\
    )\n        raise Exception(f\"Export job failed with status {job['status']}: {error_msg}\"\
    )\n    \n    time.sleep(2)\nelse:\n    raise TimeoutError(f\"Export job {job_id}\
    \ did not complete within {timeout} seconds\")\n\n# Optional: Load data into memory\
    \ if needed\nimport csv\nwith open(\"table_data.csv\", \"r\") as f:\n    reader\
    \ = csv.DictReader(f)\n    data = list(reader)\n```\n\n## Writing Tables\n\n###\
    \ Create Table from CSV\n\n```python\n# Upload CSV file\ncsv_data = \"id,name,value\\\
    n1,foo,100\\n2,bar,200\"\n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/buckets/in.c-main/tables-async\"\
    ,\n    headers={\n        \"X-StorageApi-Token\": token,\n        \"Content-Type\"\
    : \"text/csv\"\n    },\n    params={\n        \"name\": \"my_table\",\n      \
    \  \"dataString\": csv_data\n    }\n)\n\njob_id = response.json()[\"id\"]\n# Poll\
    \ job until completion (same as above)\n```\n\n## Common Patterns\n\n### Pagination\n\
    \nLarge tables should be exported in chunks:\n\n```python\ndef export_table_paginated(table_id,\
    \ chunk_size=10000):\n    \"\"\"Export table in chunks.\"\"\"\n    offset = 0\n\
    \    all_data = []\n\n    while True:\n        response = requests.get(\n    \
    \        f\"https://{stack_url}/v2/storage/tables/{table_id}/data-preview\",\n\
    \            headers={\"X-StorageApi-Token\": token},\n            params={\n\
    \                \"limit\": chunk_size,\n                \"offset\": offset\n\
    \            }\n        )\n\n        chunk = response.json()\n        if not chunk:\n\
    \            break\n\n        all_data.extend(chunk)\n        offset += chunk_size\n\
    \n    return all_data\n```\n\n### Incremental Loads\n\nUse changed_since parameter\
    \ for incremental updates:\n\n```python\nfrom datetime import datetime, timedelta\n\
    \n# Get data changed in last 24 hours\nyesterday = (datetime.now() - timedelta(days=1)).isoformat()\n\
    \nresponse = requests.get(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n    headers={\"X-StorageApi-Token\": token},\n    params={\"changedSince\"\
    : yesterday}\n)\n```\n\n\n### Export Table to File (Complete Example)\n\nFor a\
    \ complete, production-ready example that saves data to a file:\n\n```python\n\
    import requests\nimport os\nimport time\n\nstack_url = os.environ.get(\"KEBOOLA_STACK_URL\"\
    , \"connection.keboola.com\")\ntoken = os.environ[\"KEBOOLA_TOKEN\"]\ntable_id\
    \ = \"in.c-main.customers\"\noutput_file = \"customers.csv\"\n\ndef export_table_to_file(table_id,\
    \ output_file, timeout=300):\n    \"\"\"Export Keboola table to local CSV file.\"\
    \"\"\n    \n    # Start async export\n    response = requests.post(\n        f\"\
    https://{stack_url}/v2/storage/tables/{table_id}/export-async\",\n        headers={\"\
    X-StorageApi-Token\": token}\n    )\n    response.raise_for_status()\n    job_id\
    \ = response.json()[\"id\"]\n    \n    print(f\"Export job started: {job_id}\"\
    )\n    \n    # Poll for completion\n    start_time = time.time()\n    while time.time()\
    \ - start_time < timeout:\n        job_response = requests.get(\n            f\"\
    https://{stack_url}/v2/storage/jobs/{job_id}\",\n            headers={\"X-StorageApi-Token\"\
    : token}\n        )\n        job_response.raise_for_status()\n        job = job_response.json()\n\
    \        \n        if job[\"status\"] == \"success\":\n            # Download\
    \ file\n            file_url = job[\"results\"][\"file\"][\"url\"]\n         \
    \   data_response = requests.get(file_url)\n            \n            with open(output_file,\
    \ \"wb\") as f:\n                f.write(data_response.content)\n            \n\
    \            print(f\"Table exported to {output_file}\")\n            return output_file\n\
    \        \n        elif job[\"status\"] in [\"error\", \"cancelled\", \"terminated\"\
    ]:\n            error_msg = job.get(\"error\", {}).get(\"message\", \"Unknown\
    \ error\")\n            raise Exception(f\"Job {job['status']}: {error_msg}\"\
    )\n        \n        time.sleep(2)\n    \n    raise TimeoutError(f\"Export did\
    \ not complete within {timeout}s\")\n\n# Usage\nexport_table_to_file(table_id,\
    \ output_file)\n```\n\n"
  format: markdown
- source: 03-common-pitfalls.md
  content: "# Common Pitfalls\n\n## 1. Hardcoding Stack URLs\n\n**Problem**: Using\
    \ `connection.keboola.com` for all projects\n\n**Solution**: Always use environment\
    \ variables:\n\n```python\n# ❌ WRONG\nstack_url = \"connection.keboola.com\"\n\
    \n# ✅ CORRECT\nstack_url = os.environ.get(\"KEBOOLA_STACK_URL\", \"connection.keboola.com\"\
    )\n```\n\n## 2. Not Handling Job Polling\n\n**Problem**: Assuming async operations\
    \ complete immediately\n\n**Solution**: Always poll until job finishes:\n\n```python\n\
    def wait_for_job(job_id, timeout=300):\n    \"\"\"Wait for job completion with\
    \ timeout.\"\"\"\n    start = time.time()\n\n    while time.time() - start < timeout:\n\
    \        response = requests.get(\n            f\"https://{stack_url}/v2/storage/jobs/{job_id}\"\
    ,\n            headers={\"X-StorageApi-Token\": token}\n        )\n        response.raise_for_status()\n\
    \n        job = response.json()\n\n        if job[\"status\"] == \"success\":\n\
    \            return job\n        elif job[\"status\"] in [\"error\", \"cancelled\"\
    , \"terminated\"]:\n            error_msg = job.get(\"error\", {}).get(\"message\"\
    , \"Unknown error\")\n            raise Exception(f\"Job failed with status {job['status']}:\
    \ {error_msg}\")\n\n        time.sleep(2)\n\n    raise TimeoutError(f\"Job {job_id}\
    \ did not complete in {timeout}s\")\n```\n\n## 3. Ignoring Rate Limits\n\n**Problem**:\
    \ Making too many API calls too quickly\n\n**Solution**: Implement exponential\
    \ backoff:\n\n```python\nimport time\nfrom requests.exceptions import HTTPError\n\
    \ndef api_call_with_retry(url, headers, max_retries=3):\n    \"\"\"Make API call\
    \ with exponential backoff.\"\"\"\n    for attempt in range(max_retries):\n  \
    \      try:\n            response = requests.get(url, headers=headers)\n     \
    \       response.raise_for_status()\n            return response.json()\n\n  \
    \      except HTTPError as e:\n            if e.response.status_code == 429: \
    \ # Rate limited\n                wait_time = 2 ** attempt\n                print(f\"\
    Rate limited. Waiting {wait_time}s...\")\n                time.sleep(wait_time)\n\
    \            else:\n                raise\n\n    raise Exception(\"Max retries\
    \ exceeded\")\n```\n\n## 4. Not Validating Table IDs\n\n**Problem**: Using invalid\
    \ table ID format\n\n**Solution**: Validate format before API calls:\n\n```python\n\
    import re\n\ndef validate_table_id(table_id):\n    \"\"\"Validate Keboola table\
    \ ID format.\"\"\"\n    pattern = r'^(in|out)\\.c-[a-z0-9-]+\\.[a-z0-9_-]+$'\n\
    \n    if not re.match(pattern, table_id):\n        raise ValueError(\n       \
    \     f\"Invalid table ID: {table_id}. \"\n            f\"Expected format: stage.c-bucket.table\"\
    \n        )\n\n    return True\n\n# Usage\nvalidate_table_id(\"in.c-main.customers\"\
    )  # ✓\nvalidate_table_id(\"my_table\")  # ✗ Raises ValueError\n```\n\n## 5. Missing\
    \ Error Handling\n\n**Problem**: Not handling API errors gracefully\n\n**Solution**:\
    \ Always check response status:\n\n```python\ndef safe_api_call(url, headers):\n\
    \    \"\"\"Make API call with proper error handling.\"\"\"\n    try:\n       \
    \ response = requests.get(url, headers=headers, timeout=30)\n        response.raise_for_status()\n\
    \n        return response.json()\n\n    except requests.exceptions.Timeout:\n\
    \        print(\"Request timed out\")\n        return None\n\n    except requests.exceptions.HTTPError\
    \ as e:\n        if e.response.status_code == 401:\n            print(\"Invalid\
    \ token\")\n        elif e.response.status_code == 404:\n            print(\"\
    Resource not found\")\n        else:\n            print(f\"HTTP error: {e}\")\n\
    \        return None\n\n    except Exception as e:\n        print(f\"Unexpected\
    \ error: {e}\")\n        return None\n```\n\n\n## 3. Wrong HTTP Method for Async\
    \ Endpoints\n\n**Problem**: Using GET instead of POST for async export operations\n\
    \n**Solution**: Always use POST for /export-async endpoints:\n\n```python\n# ❌\
    \ WRONG - This will return 405 Method Not Allowed\nresponse = requests.get(\n\
    \    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\",\n   \
    \ headers={\"X-StorageApi-Token\": token}\n)\n\n# ✅ CORRECT - Use POST to initiate\
    \ async jobs\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\"\
    ,\n    headers={\"X-StorageApi-Token\": token}\n)\n```\n\n**Why**: The `/export-async`\
    \ endpoint creates a new export job, which is a write operation requiring POST.\
    \ The API will reject GET requests.\n\n"
  format: markdown
