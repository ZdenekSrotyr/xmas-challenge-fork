{
  "analysis": "The SKILL.md file has critical gaps in Storage API write documentation. The 'Incremental Loads' section only covers reading data incrementally (changedSince parameter), not writing incrementally. It's missing documentation on: (1) the incremental parameter for write operations, (2) primary key specification which is required for incremental writes, and (3) the distinction between table creation (/buckets/{bucket}/tables-async) and table import (/tables/{table_id}/import-async) endpoints.",
  "changes": [
    {
      "file": "claude/keboola-core/SKILL.md",
      "section": "Writing Tables section (after line 166)",
      "current": "job_id = response.json()[\"id\"]\n# Poll job until completion (same as above)\n```",
      "proposed": "job_id = response.json()[\"id\"]\n# Poll job until completion (same as above)\n```\n\n### Import Data to Existing Table\n\n```python\n# Import data to existing table\ncsv_data = \"id,name,value\\n3,baz,300\\n4,qux,400\"\n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\",\n    headers={\n        \"X-StorageApi-Token\": token,\n        \"Content-Type\": \"text/csv\"\n    },\n    params={\n        \"dataString\": csv_data\n    }\n)\n\njob_id = response.json()[\"id\"]\n# Poll job until completion\n```\n\n### Incremental Write with Primary Keys\n\n```python\n# Incremental load requires primary key specification\ncsv_data = \"id,name,value\\n1,updated_foo,150\\n5,new_item,500\"\n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\",\n    headers={\n        \"X-StorageApi-Token\": token,\n        \"Content-Type\": \"text/csv\"\n    },\n    params={\n        \"incremental\": \"1\",  # Enable incremental mode\n        \"primaryKey\": \"id\",  # Required for incremental loads\n        \"dataString\": csv_data\n    }\n)\n\njob_id = response.json()[\"id\"]\n# Poll job until completion\n\n# Note: Incremental mode updates existing rows (by primary key) \n# and appends new rows. Without incremental mode, the table \n# is completely replaced.\n```",
      "reasoning": "Adds critical missing documentation for table import endpoint and incremental writes with primary keys. Shows the difference between table creation and table import operations."
    },
    {
      "file": "claude/keboola-core/SKILL.md",
      "section": "Incremental Loads section (lines 200-215)",
      "current": "### Incremental Loads\n\nUse changed_since parameter for incremental updates:\n\n```python\nfrom datetime import datetime, timedelta\n\n# Get data changed in last 24 hours\nyesterday = (datetime.now() - timedelta(days=1)).isoformat()\n\nresponse = requests.get(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\",\n    headers={\"X-StorageApi-Token\": token},\n    params={\"changedSince\": yesterday}\n)\n```",
      "proposed": "### Incremental Loads\n\n**Reading Incrementally:**\n\nUse changedSince parameter to export only recently changed data:\n\n```python\nfrom datetime import datetime, timedelta\n\n# Get data changed in last 24 hours\nyesterday = (datetime.now() - timedelta(days=1)).isoformat()\n\nresponse = requests.get(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\",\n    headers={\"X-StorageApi-Token\": token},\n    params={\"changedSince\": yesterday}\n)\n```\n\n**Writing Incrementally:**\n\nUse incremental parameter to update/append data instead of replacing:\n\n```python\n# Incremental write updates existing rows and appends new ones\ncsv_data = \"id,name,value\\n1,updated_name,999\\n6,new_row,600\"\n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\",\n    headers={\n        \"X-StorageApi-Token\": token,\n        \"Content-Type\": \"text/csv\"\n    },\n    params={\n        \"incremental\": \"1\",\n        \"primaryKey\": \"id\",  # REQUIRED for incremental mode\n        \"dataString\": csv_data\n    }\n)\n\n# Without incremental mode, the entire table is replaced\n# With incremental mode: rows with matching primary key are updated,\n# rows with new primary keys are appended\n```",
      "reasoning": "Clarifies the distinction between reading incrementally (changedSince) and writing incrementally (incremental parameter). Emphasizes that primary keys are required for incremental writes."
    },
    {
      "file": "claude/keboola-core/SKILL.md",
      "section": "Common Pitfalls section (after line 353)",
      "current": "    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return None\n```",
      "proposed": "    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return None\n```\n\n## 6. Missing Primary Keys in Incremental Loads\n\n**Problem**: Attempting incremental load without specifying primary key\n\n**Solution**: Always specify primaryKey parameter for incremental writes:\n\n```python\n# \u274c WRONG - Will fail with error\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\",\n    headers={\"X-StorageApi-Token\": token},\n    params={\n        \"incremental\": \"1\",\n        \"dataString\": csv_data\n    }\n)\n\n# \u2705 CORRECT - Includes primary key\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\",\n    headers={\"X-StorageApi-Token\": token},\n    params={\n        \"incremental\": \"1\",\n        \"primaryKey\": \"id\",  # Can be comma-separated for composite keys\n        \"dataString\": csv_data\n    }\n)\n```\n\n## 7. Using Wrong Endpoint for Table Operations\n\n**Problem**: Confusing table creation vs. table import endpoints\n\n**Solution**: Use correct endpoint for your use case:\n\n```python\n# \u274c WRONG - Using create endpoint for existing table\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/buckets/in.c-main/tables-async\",\n    params={\"name\": \"existing_table\"}  # Will create duplicate or fail\n)\n\n# \u2705 CORRECT - Create NEW table\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/buckets/in.c-main/tables-async\",\n    params={\n        \"name\": \"new_table\",\n        \"dataString\": csv_data\n    }\n)\n\n# \u2705 CORRECT - Import to EXISTING table\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/in.c-main.existing_table/import-async\",\n    params={\n        \"dataString\": csv_data\n    }\n)\n```",
      "reasoning": "Adds two new critical pitfalls that directly address the gaps identified in the test scenario. These are common mistakes developers make when working with Storage API write operations."
    }
  ],
  "pr_title": "Fix TS-002: Add missing incremental write and primary key documentation to Storage API",
  "pr_description": "## Problem\n\nThe Storage API documentation in `claude/keboola-core/SKILL.md` had critical gaps identified by test scenario TS-002:\n\n1. **Incremental Loads section misleading**: Only showed reading data incrementally (`changedSince` parameter), not writing incrementally\n2. **Primary keys completely missing**: No mention of `primaryKey` parameter which is required for incremental writes\n3. **Endpoint confusion**: No clear distinction between table creation (`/buckets/{bucket}/tables-async`) and table import (`/tables/{table_id}/import-async`)\n\n## Impact\n\nWithout these fixes, Claude would generate code that:\n- Fails when attempting incremental writes (missing `primaryKey` parameter)\n- Uses wrong endpoints for table operations\n- Replaces entire tables instead of updating them incrementally\n\n## Changes\n\n### 1. Enhanced Writing Tables Section\n- Added \"Import Data to Existing Table\" subsection with `/import-async` endpoint\n- Added \"Incremental Write with Primary Keys\" subsection showing proper incremental configuration\n- Included clear notes about incremental behavior (update existing rows by primary key, append new rows)\n\n### 2. Clarified Incremental Loads Section\n- Split into \"Reading Incrementally\" and \"Writing Incrementally\" subsections\n- Emphasized that `primaryKey` is REQUIRED for incremental mode\n- Added explanation of incremental behavior vs. full table replacement\n\n### 3. Added New Pitfalls\n- **Pitfall #6**: Missing Primary Keys in Incremental Loads - shows wrong vs. correct approach\n- **Pitfall #7**: Using Wrong Endpoint for Table Operations - clarifies create vs. import endpoints\n\n## Testing\n\nThese changes directly address the test scenario TS-002 requirements:\n- \u2705 Shows file upload/table import\n- \u2705 Shows table creation with proper parameters\n- \u2705 Shows incremental load settings (`incremental: true`)\n- \u2705 Shows primary key handling (`primaryKey` parameter)\n- \u2705 Includes job monitoring patterns (already present)\n- \u2705 Includes error handling patterns (already present)\n\n## Backward Compatibility\n\nAll changes are additive - no existing documentation was removed, only enhanced with missing information."
}