{
  "analysis": "The Storage API Write documentation in SKILL.md is critically incomplete for incremental loading. It shows how to READ incrementally using 'changedSince' but doesn't explain how to WRITE incrementally using the 'incremental' parameter. It also lacks any mention of primary keys, which are required for incremental writes. The example shows table creation but not importing to existing tables. These gaps would cause Claude to write non-functional code for incremental upload scenarios.",
  "changes": [
    {
      "file": "claude/keboola-core/SKILL.md",
      "section": "Writing Tables section (after line 166)",
      "current": "# Upload CSV file\ncsv_data = \"id,name,value\\n1,foo,100\\n2,bar,200\"\n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/buckets/in.c-main/tables-async\",\n    headers={\n        \"X-StorageApi-Token\": token,\n        \"Content-Type\": \"text/csv\"\n    },\n    params={\n        \"name\": \"my_table\",\n        \"dataString\": csv_data\n    }\n)\n\njob_id = response.json()[\"id\"]\n# Poll job until completion (same as above)",
      "proposed": "# Upload CSV file\ncsv_data = \"id,name,value\\n1,foo,100\\n2,bar,200\"\n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/buckets/in.c-main/tables-async\",\n    headers={\n        \"X-StorageApi-Token\": token,\n        \"Content-Type\": \"text/csv\"\n    },\n    params={\n        \"name\": \"my_table\",\n        \"dataString\": csv_data\n    }\n)\n\njob_id = response.json()[\"id\"]\n# Poll job until completion (same as above)\n\n### Import Data to Existing Table\n\n```python\n# Import data into existing table\ntable_id = \"in.c-main.my_table\"\n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\",\n    headers={\n        \"X-StorageApi-Token\": token,\n        \"Content-Type\": \"text/csv\"\n    },\n    params={\n        \"dataString\": csv_data\n    }\n)\n\njob_id = response.json()[\"id\"]\n# Poll job until completion\n```\n\n### Incremental Writes with Primary Keys\n\n```python\n# Incremental load requires primary key\ntable_id = \"in.c-main.my_table\"\ncsv_data = \"id,name,value\\n1,foo,150\\n3,baz,300\"  # Updates id=1, adds id=3\n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\",\n    headers={\n        \"X-StorageApi-Token\": token,\n        \"Content-Type\": \"text/csv\"\n    },\n    params={\n        \"incremental\": \"1\",  # Enable incremental mode\n        \"primaryKey\": \"id\",  # REQUIRED for incremental\n        \"dataString\": csv_data\n    }\n)\n\njob_id = response.json()[\"id\"]\n# Poll job until completion\n```\n\n**Important**: Incremental writes require:\n1. Table must already exist\n2. Primary key(s) must be specified\n3. Matching rows (by primary key) are updated, new rows are added\n4. For multiple primary keys, use comma-separated: `\"primaryKey\": \"id,date\"`",
      "reasoning": "Adds critical missing information about incremental writes and primary keys. Shows the correct endpoint for importing to existing tables and explains the incremental parameter usage."
    },
    {
      "file": "claude/keboola-core/SKILL.md",
      "section": "Incremental Loads section (lines 200-215)",
      "current": "### Incremental Loads\n\nUse changed_since parameter for incremental updates:\n\n```python\nfrom datetime import datetime, timedelta\n\n# Get data changed in last 24 hours\nyesterday = (datetime.now() - timedelta(days=1)).isoformat()\n\nresponse = requests.get(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\",\n    headers={\"X-StorageApi-Token\": token},\n    params={\"changedSince\": yesterday}\n)\n```",
      "proposed": "### Incremental Reads\n\nUse changedSince parameter to read only recently changed data:\n\n```python\nfrom datetime import datetime, timedelta\n\n# Get data changed in last 24 hours\nyesterday = (datetime.now() - timedelta(days=1)).isoformat()\n\nresponse = requests.get(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/export-async\",\n    headers={\"X-StorageApi-Token\": token},\n    params={\"changedSince\": yesterday}\n)\n```\n\n### Incremental Writes\n\nUse incremental parameter to add/update data instead of replacing:\n\n```python\n# Incremental write (merge data based on primary key)\ntable_id = \"in.c-main.customers\"\ncsv_data = \"id,name,updated\\n100,John Doe,2024-01-15\\n101,Jane Smith,2024-01-15\"\n\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\",\n    headers={\n        \"X-StorageApi-Token\": token,\n        \"Content-Type\": \"text/csv\"\n    },\n    params={\n        \"incremental\": \"1\",       # Enable incremental mode\n        \"primaryKey\": \"id\",       # Match records by this column\n        \"dataString\": csv_data\n    }\n)\n\njob_id = response.json()[\"id\"]\n# Poll job until completion\n```\n\n**How Incremental Writes Work**:\n- Rows with matching primary key values are **updated**\n- Rows with new primary key values are **added**\n- Existing rows not in the upload remain unchanged\n- Without `incremental: 1`, the entire table is **replaced**",
      "reasoning": "Clarifies the difference between incremental reads and writes, and adds comprehensive explanation of how incremental mode works with primary keys."
    },
    {
      "file": "claude/keboola-core/SKILL.md",
      "section": "Common Pitfalls section (after line 353)",
      "current": "",
      "proposed": "\n## 6. Missing Primary Keys for Incremental Writes\n\n**Problem**: Using `incremental: true` without specifying primary keys\n\n**Solution**: Always set primary keys when using incremental mode:\n\n```python\n# \u274c WRONG - Will fail\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\",\n    headers={\"X-StorageApi-Token\": token},\n    params={\n        \"incremental\": \"1\",\n        \"dataString\": csv_data\n    }\n)\n# Error: Primary key required for incremental load\n\n# \u2705 CORRECT\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/{table_id}/import-async\",\n    headers={\"X-StorageApi-Token\": token},\n    params={\n        \"incremental\": \"1\",\n        \"primaryKey\": \"id\",  # Or \"id,date\" for composite key\n        \"dataString\": csv_data\n    }\n)\n```\n\n## 7. Confusing Table Creation vs Import\n\n**Problem**: Using wrong endpoint for table operations\n\n**Solution**: Use correct endpoint for your use case:\n\n```python\n# Create NEW table (first time)\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/buckets/in.c-main/tables-async\",\n    headers={\"X-StorageApi-Token\": token},\n    params={\n        \"name\": \"my_table\",\n        \"primaryKey\": \"id\",  # Set on creation\n        \"dataString\": csv_data\n    }\n)\n\n# Import to EXISTING table (subsequent loads)\nresponse = requests.post(\n    f\"https://{stack_url}/v2/storage/tables/in.c-main.my_table/import-async\",\n    headers={\"X-StorageApi-Token\": token},\n    params={\n        \"incremental\": \"1\",\n        \"dataString\": csv_data\n    }\n)\n```",
      "reasoning": "Adds two critical pitfalls that would prevent Claude from writing working incremental upload code. Clearly shows the incorrect vs correct approaches."
    }
  ],
  "pr_title": "Fix TS-002: Add Incremental Write and Primary Key Documentation to Storage API",
  "pr_description": "## Problem\n\nThe Storage API documentation in `claude/keboola-core/SKILL.md` was incomplete for write operations:\n\n1. **Missing Incremental Writes**: The \"Incremental Loads\" section only showed how to READ incrementally using `changedSince`, but didn't explain how to WRITE incrementally using the `incremental` parameter.\n\n2. **No Primary Key Documentation**: Primary keys were not mentioned anywhere, despite being required for incremental writes.\n\n3. **Unclear Endpoints**: The difference between creating tables (`/buckets/{bucket}/tables-async`) and importing to existing tables (`/tables/{table_id}/import-async`) was not explained.\n\n## Impact\n\nWithout this documentation, Claude would:\n- Write code that replaces entire tables instead of incrementally updating them\n- Generate API calls that fail due to missing primary keys\n- Use the wrong endpoint for table operations\n\n## Changes\n\n### 1. Enhanced Writing Tables Section\n- Added \"Import Data to Existing Table\" example showing the correct `/import-async` endpoint\n- Added \"Incremental Writes with Primary Keys\" section with complete working example\n- Documented the `incremental` and `primaryKey` parameters\n- Explained requirements and behavior of incremental mode\n\n### 2. Split Incremental Loads Section\n- Renamed to \"Incremental Reads\" for clarity\n- Added new \"Incremental Writes\" section with detailed example\n- Added explanation of how incremental mode works (update vs add behavior)\n\n### 3. Added Common Pitfalls\n- **Pitfall #6**: Missing primary keys for incremental writes with correct/incorrect examples\n- **Pitfall #7**: Confusing table creation vs import with endpoint comparison\n\n## Testing\n\nThis fixes test scenario TS-002 which specifically tested Claude's ability to write incremental upload code.\n\n## Related Issues\n\n- Closes TS-002: Missing Incremental Loads and Primary Keys in Storage API Write Documentation"
}